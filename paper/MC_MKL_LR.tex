%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,amsfonts}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\argsup}{\operatornamewithlimits{arg\,sup}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.

\usepackage[nohyperref]{icml2018}
%\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multi-Class Kernel Learning: Fast Rate and Algorithms}

\begin{document}

\twocolumn[
\icmltitle{Multi-Class Kernel Learning: Fast Rate and Algorithms}

\begin{icmlauthorlist}
\icmlauthor{Jian Li}{iie,ucas}
\icmlauthor{Yong Liu}{iie}
\icmlauthor{Hailun Lin}{iie}
\icmlauthor{Yinliang Yue}{iie}
\icmlauthor{Weiping Wang}{iie}
\end{icmlauthorlist}

\icmlaffiliation{iie}{Institute of Information Engineering, Chinese Academy of Sciences}
\icmlaffiliation{ucas}{School of Cyber Security, University of Chinese Academy of Sciences}

\icmlcorrespondingauthor{Yong Liu}{liuyong@iie.ac.cn}

\icmlkeywords{Multi-Class Classification, Kernel Methods, Local Rademacher Complexity}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\icmlauthor{Your Name}{email@yourdomain.edu}
%\icmladdress{Your Fantastic Institute,
%            314159 Pi St., Palo Alto, CA 94306 USA}
%\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
%\icmladdress{Their Fantastic Institute,
%            27182 Exp St., Toronto, ON M6H 2T1 CANADA}
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

    The study on generalization performance of multi-class
    classification algorithms is one of the fundamental issues in statistical learning theory.
    %Several generalization bounds have been proposed based on different measures,
%    but the convergence rates of the existing bounds are usually at most $\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$,
%    where $K$ and $n$ are the number of classes and size of the sample, respectively.
    In this paper, we study the generalization performance of multi-class classification
    based on the notion of local Rademacher complexity and obtain a shaper data-dependent
    generalization error bound with fast convergence rate,
    substantially improving the state-of-art bounds in the existing data-dependent generalization analysis.
    The theoretical analysis motivates us to
    devise two effective multi-class kernel learning algorithms
    with statistical guarantee and fast convergence rate.
    Experimental results show that our proposed methods can significantly outperform
    the existing multi-class classification methods.
    Theoretical analysis and empirical results demonstrate that our
    multi-class kernel learning methods based local Rademacher complexity are sound and effective.
    %a convex optimization way based on any existing MC-MKL algorithms, and another one that add local Rademancher complexity in optimization formulation for which we give a stochastic sub-gradient descent method in detail
\end{abstract}

\section{Introduction}
\label{submission}
Multi-class classification is an important problem in various applications,
such as natural language processing \cite{Zhang2005cs}, information retrieval \cite{hofmann2003learning}, computer vision \cite{deng2009imagenet},
web advertising \cite{beygelzimer2009conditional}, etc.
The statistical learning theory of binary classification is by now relatively well developed \cite{vapnik1998naturestatistical,mohri2012foundations},
but there are still numerous statistical challenges to its multi-class extension \cite{maximov2016tight}.

To understand the existing multi-class classification algorithms and guide the development of new ones,
people have to develop the generalization ability of multi-class classification algorithms.
A sharper generalization bound usually implies more consistent
performance on the training set and the test set.
In recent years, some generalization bounds have been proposed to
estimate the ability of multi-class classification algorithms based on different measures,
such as
VC-dimension \cite{allwein2000reducing},
Natarajan dimension \cite{daniely2014optimal},
covering Number \cite{guermeur2002combining,zhang2004statistical,Hill2007},
Rademacher Complexity \cite{koltchinskii2002empirical,mohri2012foundations,cortes2013multi}, etc.
Although there have been several recent advances in the studying of
generalization bounds of multi-class classification algorithms,
convergence rates of the existing generalization bounds are usually %at least%
 $\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$,
where $K$ and $n$ are the number of classes and size of the sample, respectively.



In this paper, we derive a novel data-dependent generalization bound for multi-class classification
via the notion of local Rademacher complexity and
further devise two effective multi-class kernel learning algorithms based on the above theoretical analysis.
The rate of this bound is $\mathcal{O}\big(\frac{(\log K)^{2+{1}/{\log K}}}{n}\big)$,
which substantially improves on  the existing data-dependent generalization bounds.
Moreover, the proposed multi-class kernel learning algorithms have statistic guarantee and fast convergence rate.
Experimental results on benchmark datasets show that our proposed methods can significantly
outperform the existing multi-class classification methods.
The major contributions of this paper include:
 1) A new local Rademacher complexity-based bound of fast convergence rate for multi-class classification is established;
 2) Two novel multi-class classification algorithms are proposed with convergence guarantee: a) \texttt{Conv-MKL}.
Using precomputed kernel matrices regularized by local Rademancher complexity,
this method can be implemented by any $\ell_p$-norm multi-class MKL solvers;
b) \texttt{SMSD-MKL}.
This method puts local Rademancher complexity
in penalized ERM with $\ell_{2,p}$-norm regularizer and multi-class hinge loss function,
implemented by stochastic sub-gradient descent by updating dual weights.


%The rest of the paper is organized as follows.
%In Section 2, we introduce the related work.
%In Section 3, we give
%some notations and preliminaries.
%In Section 4,
%we propose the upper bound of the local Rademacher complexity for multi-class classification,
%and further derive a generalization error bound of fast convergence rate.
%%We empirically analyze the performance of our proposed bounds in Section 5.
%We end in Section 5 with conclusion.
%Due to limited space,
%some proofs are given in the supplementary material.

\section{Related Work}
In this section,
we  introduce related works:
multi-class classification bounds,
local Rademacher complexity and multi-class kernel learning classification.

\subsection{Multi-Class Classification Bounds}
 Rademacher complexity, VC-dimension, and covering number are three
 popular tools to derive generalization bounds for multi-class classification:

\textbf{Rademacher Complexities Bounds.}
   Koltchinskii and Panchenko \yrcite{koltchinskii2002empirical}
    and Koltchinskii, Panchenko, and Lozano \yrcite{koltchinskii2001some}
    first introduced a margin-based bound for multi-class classification in terms of Rademacher complexity.
    This bound was slightly improved in \cite{mohri2012foundations,cortes2013multi}.
    Maximov and Reshetova \yrcite{maximov2016tight} gave a new Rademacher complexity based bound
    that is linear in the number of classes.
    % give a new Rademacher complexity based
    %bound depends linearly in the number of classes of $\mathcal{O}\big(\frac{k}{\sqrt{n}}\big)$.
    %The bound involves the marginal distribution
    %of the classifier and the Rademacher complexity of the hypothesis class
    Based on the $\ell_p$-norm regularization,
    Lei, Binder, and Klof \yrcite{lei2015multi} introduced a bound
    with a logarithmic dependence on the number of class size.

    Instead of global Rademacher complexity,
    in this paper, we use local Rademacher complexity
    to obtain a sharper bound, which  substantially improves
    generalization performance upon existing global Rademancher complexity methods.

\textbf{VC-dimension Bounds.}
    Allwein, Schapire, and Singer \yrcite{allwein2000reducing}  used the notion of VC-dimension
    for multi-class learning problems,
    and derived a VC-dimension based bound.
    Natarajan dimension was introduced in \cite{Natarajan1989} in
    order to characterize multi-class PAC learnability,
    which exactly matches the notion of Vapnik-Chervonenkis
    dimension in the case of binary classification.
    Daniely and Shalev-Shwartz \yrcite{daniely2014optimal} derived a risk bound with Natarajan dimension for multi-class classification.
    VC dimension and Natarajan dimension are the important tools to derive generalization bounds,
    however these bounds are usually dimension
    dependent, which makes them hardly applicable to practical large
    scale problems.

\textbf{Covering Number  Bounds.}
    Based on the $\ell_\infty$-norm covering number bound of
    linear operators, Guermeur \yrcite{guermeur2002combining} obtained
    a generalization bound exhibiting a linear dependence on the class size,
    which was improved by \cite{zhang2004statistical} to a radical dependence.
    Hill and Doucet \yrcite{Hill2007} derived a class-size independent risk guarantee.
    However, their bound is based on a delicate definition of margin,
    which is not commonly used in the mainstream multi-class literature.
\subsection{Local Rademacher Complexity}
One of useful data-dependent complexity measures used in the generalization analysis
for  traditional  binary classification
is the notion of (global) Rademacher complexity \cite{bartlett2003rademacher}.
However,
it provides global estimates of the complexity of the function class,
that is, it does not reflect
the fact that the algorithm will likely pick functions that have a small error.
In recent years,
several authors have applied
 \emph{local} Rademacher complexity to obtain  better generalization error bounds
for traditional binary classification \cite{Bartlett2005lrc,Koltchinskii2006lrcoiirm}.
%The local Rademacher complexity considers Rademacher averages
%of a smaller subset of the hypothesis set,
%so it is always smaller than the corresponding global one.
%There are some work consider the use of local Rademacher complexity to
%derive tighter bounds for binary classification.
However,
numerous statistical challenges remain in multi-class case, and it is still unclear how to use
this tool to derive a tighter bound for multi-class.
In this paper,
we bridge this gap by deriving a sharper generalization bound using local Rademacher complexity.
%we show how to use the notion of local Rademacher complexity to derive a sharper generalization bound,
%which fills this gap.
%we concerns the structural relationship
%of Gaussian complexities since it is based on a comparison result
%among different Gaussian processes.
%Our result is a non-trivial extension of the local Rademacher complexity of
%binary classification for multi-class classification.
\subsection{Multi-Class Kernel Learning Algorithms}
Improvements in multi-class classification has emerged as one of success stories in Multiple Kernel Learning \cite{ZienO2007}
in which a one-stage multi-class MKL algorithm was presented as a generalization of multi-class loss function \cite{CrammerS02,TsochantaridisHJA04}.
And Orabona designed stochastic gradient methods, named
OBSCURE \cite{OrabonaJC10} and
UFO-MKL \cite{OrabonaL11}, which optimize primal versions of equivalent problems.
%Then, tighter margin-based bounds for multi-class classification in terms of Rademacher complexity are introduced \cite{mohri2012foundations,cortes2013multi}.

In this paper, we consider the use of  the local Rademacher complexity to devise the novel  multi-class classification
algorithms, which have statistical guarantee and fast convergence rate.

\section{Notations and Preliminaries}
We consider multi-class classification problems with $K\geq 2$ classes in this paper.
Let $\mathcal{X}$ be the input space and $\mathcal{Y}=\{1,2,\ldots,K\}$ the output space.
Assume that we are given a sample
$
  \mathcal{S}=\{z_1=(\mathbf  x_1,y_1),\ldots, z_n=(\mathbf  x_n,y_n)\}
$
of size $n$ drawn i.i.d. from a fixed,
but unknown probability distribution $\mu$ on $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$.
Based on the training examples $\mathcal{S}$,
we wish to learn a scoring rule $h$ from a space $\mathcal{H}$ mapping from $\mathcal{Z}$ to $\mathbb{R}$
and use the mapping $$\mathbf x\rightarrow \argmax_{y\in\mathcal{Y}} h(\mathbf x,y)$$ to predict.
For any hypothesis $h\in\mathcal{H}$,
the margin of a labeled example $z=(\mathbf x,y)$ is defined as
\begin{align*}
  \rho_h(z):= h(\mathbf x,y)-\max_{y'\not =y} h(\mathbf x,y').
\end{align*}
The $h$ misclassifies the labeled example $z=(\mathbf x,y)$
if $\rho_h(z)\leq 0$ and thus the expected risk incurred
from using $h$ for prediction is
$
  L(h):=\mathbb{E}_\mu[1_{\rho_h(z)\leq 0}],
$
where $1_{t\leq 0}$ is the 0-1 loss,
$1_{t\leq 0}=1$ if $t\leq 0$, otherwise 0.
Since 0-1 loss is hard to handle in learning machines,
one ususally considers the proxy loss:
such as the  the hinge $\ell(t)=(1-t)_+$ and the margin loss $\ell^s(t)=1_{t\leq 0}+(1-ts^{-1})1_{0<t\leq s}$, $s>0$.
In the following, we assume that: 1) $\ell(t)$ bounds the 0-1 loss: $1_{t\leq 0}\leq \ell(t)$; 2)
$\ell$ is decreasing and it has a zero point $c_\ell$, i.e., $\ell(c_\ell)=0$.
Note that both hinge loss and margin loss satisfy the above two assumptions.
%\begin{definition}[Regular Loss]
%  We call $\ell$ a $\zeta$-regular loss if it satisfies the following properties:
%  1) $\ell(t)$ bounds the 0-1 loss: $1_{t\leq 0}\leq \ell(t)$;
%  2) $\ell$ is $\zeta$-smooth, i.e., $|\ell'(t)-\ell'(s)|\leq \zeta|t-s|$;
%  3) $\ell$ is decreasing and it has a zero point $c_\ell$, i.e., $\ell(c_\ell)=0$.
%\end{definition}
%Some examples of $\zeta$-regular loss functions include the popular squared Hinge loss
%$\ell(t)=\left(\max\{0,1-t\}\right)^2$, squared margin loss $\ell(t)=\left(\max\{0,1-t+\rho\}\right)^2$.
%Note that when $\theta\rightarrow 0$, ramp loss and Huber loss converge to the  0-1 loss and
%Hinge loss $\ell(t)=\max\{0,1-t\}$, respectively.

Any function $h:\mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}$ can be equivalently represented by the vector-valued function
$(h_1,\ldots,h_K)$ with $h_j(\mathbf x)=h(\mathbf x,j)$, $\forall j=1,\ldots,K$.
Let $\kappa:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}$ be a Mercer kernel with $\phi$ being the associated feature map,
i.e., $\kappa(\mathbf x,\mathbf x')=\langle \phi(\mathbf x),\phi(\mathbf x')\rangle$.
The $\ell_p$-norm hypothesis space associated with the kernel $\kappa$ is denoted by:
 \begin{align}
 \label{hypothspapce}
   \begin{aligned}
   \mathcal{H}_{p,\kappa}=&\Big\{h_\mathbf{w}=\left(\langle \mathbf w_1,\phi(\mathbf x)\rangle,\ldots, \langle\mathbf w_K,\phi(\mathbf x)\rangle\right):\\
   &~\left\|\mathbf  w \right\|_{2,p}\leq 1, 1\leq p\leq 2
  \Big\},
   \end{aligned}
 \end{align}
 where $\mathbf w=(\mathbf w_1,\ldots,\mathbf w_K)$ and $\|\mathbf w\|_{2,p}=\left[\sum_{i=1}^K\|\mathbf w_i\|_2^p\right]^{\frac{1}{p}}$
 is the $\ell_{2,p}$-norm.
 For any $p\geq 1$, let $q$ be the dual exponent of $p$ satisfying $\frac{1}{p}+\frac{1}{q}=1$.
% Note that,  $\forall 1\leq p\leq 2$, we have $q\geq 2$.

The space of loss function associated with $\mathcal{H}_{p,\kappa}$  is denoted by
\begin{align}
\label{eq-sapce-loss-functions}
  \mathcal{L}=\left\{\ell_h:=\ell(\rho_h(z)):h\in\mathcal{H}_{p,\kappa}\right\}.
\end{align}
Let $L(\ell_h)$ and  $\hat{L}(\ell_h)$ be expected generalization error and
 empirical error  with respect to $\ell_h$:
\begin{align*}
  L(\ell_h):=\mathbb{E}_\mu[\ell(\rho_h(z))] \text{ and } \hat{L}(\ell_h)=\frac{1}{n}\sum_{i=1}^n\ell(\rho_h(z_i)).
\end{align*}




%Note that if $\ell$ is a $\zeta$-regular loss,
%then
%\begin{align*}
%  L(h)\leq  L(\ell_h).
%\end{align*}
%Thus, we can use $L(\ell_h)$ to bound $L(h)$.

%In order to estimate $L(\ell_h)$, we introduce the Rademacher complexity.
%To this end,
%we have to study the behavior of the difference between the
%risk and the empirical risk.
%To this end, we introduce the notion of uniform deviation of $\mathcal{L}$,
%denoted as
%\begin{align}
%\label{eq-Uniform-deviation-definition}
%  \hat{U}_n(\mathcal{L})
%    =\sup_{\ell_h\in\mathcal{L}}\left\{L(\ell_h)-\hat{L}(\ell_h)\right\}.
%\end{align}
%Note that
%$
%  \left\{L(\ell_h)-\hat{L}(\ell_h)\right\}_{\ell_h\in\mathcal{L}}
%  \leq \hat{U}_n(\mathcal{L}),
%$
%so we have
%\begin{align*}
%  L(\ell_h)\leq \hat{L}(\ell_h)+
%  \hat{U}_n(\mathcal{L}),
%  \forall \ell_h\in\mathcal{L}.
%\end{align*}
%$\hat{U}_n(\mathcal{L})$
%is not computable,
%but we can bound its value via  the
% Rademacher complexity $\mathcal{L}$:
\begin{definition}[Rademacher complexity]
Assume $\mathcal{L}$ is a space of loss functions as defined in Equation \eqref{eq-sapce-loss-functions}.
Then the empirical Rademacher complexity of $\mathcal{L}$ is:
%Then the empirical \textbf{ranking Rademacher complexity} of $\mathcal{L}$ is:
%    Let $\mathcal{H}$ be a family of real-valued functions
%    defined on $\mathcal{Z}$ and $\mathcal{S}=(z_1,\ldots,z_n)$ a fixed sample of size $n$ with elements in $\mathcal{Z}$.
    %Then, the empirical Rademacher and Gaussian complexities of $\mathcal{H}$ with respect to the sample $\mathcal{S}$ are defined by
    \begin{align*}
      \hat{\mathcal{R}}(\mathcal{L}):=\mathbb{E}_{\bm \sigma}
      \left[\sup_{\ell_h\in\mathcal{L}}
     \frac{1}{n}\sum_{i=1}^n\sigma_i \ell_{h}(z_i)
          \right],
    \end{align*}
     where $\sigma_1,\sigma_2, \ldots,\sigma_n$
    is an i.i.d. family of Rademacher variables taking values -1 and 1
    with equal probability independent of the sample $\mathcal{S}=(z_1,\ldots,z_n)$.
    The  Rademacher complexity of $\mathcal{L}$ is:
    \begin{align*}
      %\label{def-expect-Rademacher}
      \mathcal{R}(\mathcal{L})=\mathbb{E}_{\mu}\hat{\mathcal{R}}(\mathcal{L}).
    \end{align*}
\end{definition}
%Rademacher complexity is the most powerful tool to get generalization bounds for multi-class classification, currently.
Generalization bounds based on the notion of Rademacher complexity for multi-class classification
are standard \cite{koltchinskii2002empirical,koltchinskii2001some,mohri2012foundations}:
with probability $1-\delta$,
\begin{align*}
  L(h)\leq \inf_{0<\gamma<1}\left(\hat{L}(h_\gamma)+\mathcal{O}\left(\frac{\mathcal{R}(\mathcal{L})}{\gamma}+\frac{\log1/\delta}{\sqrt{n}}\right)\right),
\end{align*}
where $\hat{L}(h_\gamma)=\frac{1}{n}\sum_{i=1}^n\left[1_{\rho_h(z_i)\leq \gamma}\right]$.
Since $\mathcal{R}(\mathcal{L})$ is in the order of $\mathcal{O}(\frac{K^2}{\sqrt{n}})$ for various  kernel multi-class in practice,
so the standard Rademacher complexity bounds converge at rate
$\mathcal{O}\left(\frac{K^2}{\sqrt{n}}\right)$, usually.

Although Rademacher complexity is the popular tool to bound generalization,
it does not take into consideration the fact that,
typically, the hypotheses selected by a learning algorithm
have a better performance than in the worst case and
belong to a more favorable sub-family of the set of all hypotheses \cite{cortes2013learning}.
Therefore, to derive sharper generalization bound,
we consider the use of the local Rademacher complexity in this paper.
To this end,
  let $\mathcal{L}^r$ be a star-shaped space of $\mathcal{L}$ with respect to $r>0$,
  \begin{align}
  \label{def-localrademchercomplexity}
    \mathcal{L}^r=\left\{
        a\ell_h\Big|a\in[0,1],\ell_h\in\mathcal{L},
        L[(a\ell_h)^2]\leq r
    \right\},
  \end{align}
 where $L(\ell_h^2)=\mathbb{E}_\mu\left[\ell^2(\rho_h(z))\right]$.
% Note that
% \begin{align*}
%   \forall \ell_h\in\mathcal{L}^r,
%   V^2(\ell_h)=L(\ell_h^2)-[L(\ell_h)]^2
%   \leq L(\ell_h^2)\leq r,
% \end{align*}
% so the variance of each element in $\mathcal{L}^r$ is smaller than $r$.
 %Therefore,
% for all $\ell_h^r\in\mathcal{L}^r$,
% its variance  $V^2(\ell_h)=L(\ell_h^2)-[L(\ell_h)]^2$,
% so the following inequality holds
 %Since such a small class can  also have a substantially
% smaller complexity, we can obtain sharp bound.
%The local ranking Rademacher complexity is defined as follows:
%For this purpose, we consider using the  local
%ranking Rademacher complexity defined as follows:
 \begin{definition}[Local Rademacher Complexity]
 \label{def-thereee}
   For any $r>0$, the local Rademacher complexity of $\mathcal{L}$ is defined as
   \begin{align*}
     \mathcal{R}(\mathcal{L}^r):=
     \mathcal{R}\left(\left\{
        a\ell\Big|a\in[0,1],\ell\in\mathcal{L},
        L[(a\ell)^2]\leq r\right\}\right).
   \end{align*}
 \end{definition}
 The key idea to obtain sharper generalization error bound
 is to choose a much smaller class $\mathcal{L}^r\subseteq\mathcal{L}$
 with as small a variance as possible,
 while requiring that the solution is still in $\{h|h\in\mathcal{H}_{p,\kappa},\ell_h\in \mathcal{L}^r\}$.
%If the local Rademacher complexity is known,
%the risk be bounded in terms of the fixed point of the local Rademacher complexity,
%besides constants and $\mathcal{O}(\log K/n)$ terms.

In the following, we assume that
$\vartheta=\sup_{\mathbf x\in \mathcal{X}}\kappa(\mathbf x,\mathbf x)< \infty$,
and $\ell_h:\mathcal{Z}\rightarrow [0,d]$,
$d>0$ is a constant.

\section{Sharper Generalization Bounds}
In this section, we first estimate the local Rademacher complexity,
and further derive a sharper generalization bound.

\subsection{Local Rademacher Complexity}
In this subsection, we will estimate the local Rademacher complexity of multi-class classification.
\begin{theorem}
\label{rademacherlocal}
  If $\ell$ is a $\zeta$-smooth loss, i.e., $|\ell'(t)-\ell'(s)|\leq \zeta|t-s|$,
  then with probability $1-\delta$,
  we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r) \leq \frac{c_{d,\vartheta}\xi(K)\sqrt{\zeta r}\log^{\frac{3}{2}}(n)}{\sqrt{n}}+\frac{4\log(1/\delta)}{n},
  \end{align*}
  where
  \begin{align*}
  \xi(K)=
  \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } q\geq 2\log K,\\
      &(2q)^{1+\frac{1}{q}}K^{\frac{1}{q}}, &&\text{otherwise},
      \end{aligned}
      \right.
  \end{align*}
  $c_{d,\vartheta}$ is a constant depends on $d$ and $\vartheta$.
\end{theorem}
\begin{proof}
  According to the Lemma 3.6 of \cite{oneto2013improved},
  with probability $1-\delta$, we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r)\leq \hat{\mathcal{R}}(\mathcal{L}^r)+\sqrt{\frac{2\log(1/\delta)\mathcal{R}(\mathcal{L}^r)}{n}}.
  \end{align*}
  Note that $\forall a,b\geq 0, \sqrt{ab}\leq \frac{a}{2}+\frac{b}{2}$.
  Thus, we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r)\leq \hat{\mathcal{R}}(\mathcal{L}^r)+\frac{\mathcal{R}(\mathcal{L}^r)}{2}+\frac{\log(1/\delta)}{n}.
  \end{align*}
  So, we can obtain that
  \begin{align}
  \label{eqRhatRnew}
    \mathcal{R}(\mathcal{L}^r)\leq 2\hat{\mathcal{R}}(\mathcal{L}^r)+\frac{2\log(1/\delta)}{n}
  \end{align}
  From the Lemma 2.2 of \cite{Srebro2010lrc},
  we know that if $\ell$ is a $\zeta$-smooth loss function,
  \begin{align}
   \label{RlocalRnew}
    \hat{\mathcal{R}}(\mathcal{L}^r)\leq c_d\sqrt{\zeta r}\log^{\frac{3}{2}}(n)\hat{\mathcal{R}}(\mathcal{L}),
  \end{align}
  where $c_d$ is a constant depends on $d$.
  Substituting  \eqref{RlocalRnew} into \eqref{eqRhatRnew},
  we have
  \begin{align}
  \label{eq-middle-result-hatR}
    \hat{\mathcal{R}}(\mathcal{L}^r)\leq 2c_d\sqrt{\zeta r}\log^{\frac{3}{2}}(n)\hat{\mathcal{R}}(\mathcal{L})
    +\frac{2\log(1/\delta)}{n}.
  \end{align}
  From Theorem 1 in Appendix A,
  we have
  \begin{align*}
    \hat{\mathcal{R}}(\mathcal{L})\leq \frac{1}{\sqrt{n}}\times
      \left\{
      \begin{aligned}
      &\sqrt{e\vartheta}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } q\geq 2\log K,\\
      &\sqrt{\vartheta}(2q)^{1+\frac{1}{q}}c^{\frac{1}{q}}, &&\text{otherwise.}
      \end{aligned}
      \right.
  \end{align*}
  Substituting the above result into \eqref{eq-middle-result-hatR},
  the proof is over.
%  the proof is over.
\end{proof}

%\begin{proof}
%    We sketch the proof here and leave the details in the
%    supplementary material.
%    We first show that if $\ell$ is a $\zeta$-regular loss,
%    the local Rademacher complexity $\mathcal{R}(\mathcal{L}^r)$ can be bounded with the empirical Rademacher complexity $\hat{\mathcal{R}}(\mathcal{L})$:
%    \begin{align*}
%    \mathcal{R}(\mathcal{L}^r)\leq C\sqrt{\zeta r}\log^{\frac{3}{2}}(n)\hat{\mathcal{R}}(\mathcal{L})+\frac{2\log(1/\delta)}{n}.
%  \end{align*}
%  Then, based on the relationship between Rademacher complexity and Gaussian complexity,
%    \begin{align*}
%      \hat{\mathcal{R}}(\mathcal{L})\leq \sqrt{\frac{\pi}{2}}\hat{\mathcal{G}}(\mathcal{L}),
%    \end{align*}
%    where $\hat{\mathcal{G}}(\mathcal{L})$ is the Gaussian complexity on $\mathcal{L}$.
%   Finally, we concerns the structural relationship of Guassian complexities based on a comparison result among different Gaussian processes,
%   we give a upper bound of $\hat{\mathcal{G}}(\mathcal{L})$ with logarithmic dependence on the class size:
%   \begin{align*}
%     \hat{\mathcal{G}}(\mathcal{L}\leq \frac{\xi(K)}{\sqrt{n}}.
%   \end{align*}
%\end{proof}
%From \cite{cortes2013multi}, we know that,
Note that the order of the (global) Rademacher complexity over $\mathcal{L}$ is usually
$\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$ for various kernel multi-classes.
From Theorem \ref{rademacherlocal}, one can see that the order of the local Rademacher complexity is
\begin{align*}
  \mathcal{R}(\mathcal{L}^r)=\mathcal{O}\left(\frac{\sqrt{r}\xi(K)}{\sqrt{n}}+\frac{1}{n}\right).
\end{align*}
Note that $\xi(K)$ is logarithmic dependence on $K$ when $q\geq 2\log K$.
For $2\leq q < 2\log K$, $\xi(K)=\mathcal{O}(K^{\frac{2}{q}})$
which is also substantially milder than the quadratic dependence for Rademacher complexity.
If we choose a suitable value of $r$,
the order can even reach $\mathcal{O}\big(\frac{(\log K)^{2+1/\log K}}{n}\big)$ (see in the next subsection),
which substantially improves the Rademacher complexity bounds.
\subsection{A Sharper Generalization Bound using Local Rademacher Complexity}
In this subsection, we will derive a sharper bound for multi-class classification based on
the notion of local Rademacher complexity.
\begin{theorem}
\label{theorem-finally}
  If $\ell$ is a $\zeta$-smooth loss.
  Then, $\forall h\in\mathcal{H}_{p,\kappa}$ and $\forall k>\max(1,\frac{\sqrt{2}}{2d})$,  with probability
  $1-\delta$, we have $L(h)\leq $
  \begin{align*}
   \leq \max\left\{
        \frac{k}{k-1}\hat{L}(\ell_h),
       \hat{L}(\ell_h)+\frac{c_{d,\vartheta,\zeta, k}\xi^2(K) \log^3 n}{n}+\frac{c_{\delta}}{n}
     \right\},
\end{align*}
where \begin{align*}
  \xi(K)=
  \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &\text{if } q\geq 2\log K,\\
      &(2q)^{1+\frac{1}{q}}K^{\frac{1}{q}}, &\text{otherwise},
      \end{aligned}
      \right.
  \end{align*}
  $c_{d,\vartheta}$ is a constant depending on $d,\vartheta,\zeta, k$,
  and $c_{\delta}$ is a constant depending on $\delta$.
\end{theorem}
\begin{proof}
From Theorem 2 in Appendix B,
with probability $1-\delta$,
we have
\begin{align}
\label{eq-middle-leel}
   L(\ell_h)&\leq \max\left\{
        \frac{k}{k-1}\hat{L}(\ell_h),
       \hat{L}(\ell_h)+c_dr^\ast+\frac{c_\delta}{n}
     \right\},
\end{align}
where $r^\ast$ is a fixed point of $\mathcal{R}(\mathcal{L}^r)$.
%that is $r^\ast=\mathcal{R}(\mathcal{L}^{r^\ast})$.
From Lemma 5 (see  Appendix C),
we know that the $\mathcal{R}(\mathcal{L}^r)$ is a sub-root function,
so the fixed point $r^\ast$ of $\mathcal{R}(\mathcal{L}^r)$ is uniquely exists.

According to  Theorem \ref{rademacherlocal}, we know that
\begin{align*}
    \mathcal{R}(\mathcal{L}^r) \leq \frac{c_{d,\vartheta}\xi(K)\sqrt{\zeta r}\log^{\frac{3}{2}}(n)}{\sqrt{n}}+\frac{4\log(1/\delta)}{n}.
  \end{align*}
Thus, if we set $A=\frac{c_{d,\vartheta}\xi(K)\sqrt{\zeta}\log^{\frac{3}{2}}(n)}{\sqrt{n}}$, $B=\frac{4\log(1/\delta)}{n}$,
the fixed point $r^\ast$ is smaller than the solution of
  $
     A\sqrt{r}+B=r,
  $
   which is
   \begin{align*}
     r^s&=\frac{2B+A^2+\sqrt{(2B+A^2)^2-4B^2}}{2}\\
     &\leq 2B+A^2
     =\frac{c_{d,\vartheta}^2\xi^2(K)\zeta\log^3(n)}{n}+\frac{4\log(1/\delta)}{n}.
   \end{align*}
   Substituting the above inequality into \eqref{eq-middle-leel} finishes the proof.
\end{proof}
The order of the  generalization bound in Theorem \ref{theorem-finally} is
$
 \mathcal{O}\left(\frac{\xi^2(K)}{n}\right).
$
From the definition of $\xi(K)$, we can obtain that
 \begin{align*}
  \mathcal{O}\left(\frac{\xi^2(K)}{n}\right)=
  \left\{
      \begin{aligned}
      &\mathcal{O}\left(\frac{(\log K)^{2+{1}/{\log K}}}{n}\right), \text{if } q\geq 2\log K,\\
      &\mathcal{O}\left(\frac{K^{2/q}}{n}\right), \text{if } 2\leq q< 2\log K.
      \end{aligned}
      \right.
  \end{align*}
  Note that our bounds is linear dependence on the reciprocal of sample size $n$,
  while for the existing data-dependent bounds are all radical dependence.
 % which is substantially faster than the dependence on $\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)$ established in the existing
%  work for multi-class classification.
Furthermore,
  our bounds enjoy a mild dependence on the number of classes.
  The dependence is polynomial with degree $2/q$ for $2\leq q < 2\log K$ and
  becomes logarithmic if $q \geq 2\log K$,
  which is substantially milder than the quadratic dependence
  established in \cite{koltchinskii2002empirical,koltchinskii2001some,mohri2012foundations,cortes2013multi}.

%To the best of our knowledge,
%the generalization bound  of multi-class classification
%with  linear dependence on reciprocal on the size of sample and logarithmic dependence on the class size
%have never given before.

\subsection{Comparison with Related Work}
In this section, we will compare our bound with three popular bounds of multi-class classification:
 Rademacher complexity, covering number and VC-dimension Bounds.

\textbf{Rademacher Complexity Bounds}
Currently Rademacher complexity are the most powerful tools to get generalization bounds for
multi-class classification.
The important property of Rademacher complexity based bounds is that the
bounds are applicable in arbitrary Banach spaces and do not depend on the dimension of the feature
space directly.

Koltchinskii and Panchenko \yrcite{koltchinskii2002empirical}
and Koltchinskii, Panchenko, and Lozano \yrcite{koltchinskii2001some}
introduce a margin-based bound for multi-class classification in terms of Rademacher complexities:
\begin{align*}
  L(h)\leq \inf_{0<\gamma<1}\hat{L}(h_\gamma)+\mathcal{O}\left(\frac{K^2}{\gamma\sqrt{n}}+\frac{\log1/\delta}{\sqrt{n}}\right).
\end{align*}
The order is $\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$,
which is slightly improved (by a constant factor prior to the Rademacher complexity term)
by \cite{mohri2012foundations,cortes2013multi}.
Maximov and Reshetova \yrcite{maximov2016tight} give a new Rademacher complexity bound:
\begin{align*}
  L(h)\leq \inf_{0<\gamma<1}\hat{L}(h_\gamma)+\mathcal{O}
  \left(\frac{K}{\gamma\sqrt{n}}+\frac{\log1/\delta}{\sqrt{n}}\right),
\end{align*}
which has the form of $\mathcal{O}\big(\frac{K}{\sqrt{n}}\big)$.
% give a new Rademacher complexity based
%bound depends linearly in the number of classes of $\mathcal{O}\big(\frac{k}{\sqrt{n}}\big)$.
%The bound involves the marginal distribution
%of the classifier and the Rademacher complexity of the hypothesis class
Based on the $\ell_p$-norm regularization,
Lei, Binder, and Klof \yrcite{lei2015multi} derive a new bound:
\begin{align*}
   L(h)\leq  \hat{L}(\ell_h) +\mathcal{O}\left(\frac{\log^2 K}{\sqrt{n}}\right).
\end{align*}
The existing bounds based on Rademacher complexity are all radical dependence
on the reciprocal of sample size.
%of the form $\mathcal{O}\big(\frac{\log^2 K}{\sqrt{n}}\big)$
%for some value of $p$.

In this paper,
we derive a sharper bound based on the local Rademacher complexity with order
$
\mathcal{O}\Big(\frac{(\log K)^{2+\frac{1}{\log K}}}{n}\Big),
$
which is substantially sharper
than the existing bounds of  Rademacher complexity.

\textbf{Covering Number Bounds}
Based on the $\ell_\infty$-norm covering number bound of
linear operators, Guermeur \yrcite{guermeur2002combining} obtain
a generalization of form $\mathcal{O}\big(\frac{K}{\sqrt{n}}\big)$,
which is improved by \cite{zhang2004statistical} to a radical dependence:
  \begin{align*}
   L(h)\leq  \hat{L}(\ell_h) +\mathcal{O}\left(\sqrt{\frac{K}{n}}\right).
\end{align*}
Hill and Doucet \yrcite{Hill2007} derive a class-size independent risk guarantee
of form $\mathcal{O}\big(\sqrt{{1}/{n}}\big)$.
However, their bound is based on a delicate definition of margin,
which is not commonly  used in the mainstream multi-class literature.
%Related results for metric spaces with low doubling dimension were obtained by \cite{Kontorovich2014},
%who used nearest neighbors method to improve the dependence on the number
%of classes in favor of (doubling) dimension dependence.

\textbf{VC-dimension Bounds}
VC-dimension is a important tool to derive the generalization bound for binary classification.
Allwein, Schapire, and Singer \yrcite{allwein2000reducing} show how to use it
for multi-class learning problems,
and derive a VC-dimension based bounds:
\begin{align*}
   L(h)\leq  \hat{L}(h_\gamma) +\mathcal{O}\left(\frac{\sqrt{V}\log K}{\sqrt{n}}\right),
\end{align*}
where $V$ is the VC-dimension.
Natarajan dimension is introduced in \cite{Natarajan1989} in
order to characterize multi-class PAC learnability.
%It exactly matches the notion of Vapnik-Chervonenkis
%dimension in the case of two classes.
Daniely and Shalev-Shwartz \yrcite{daniely2014optimal} derive a generalization bound with Natarajan dimension:
\begin{align*}
   L(h)\leq  \hat{L}(h_\gamma) +\mathcal{O}\left(\frac{d_{Nat}}{n}\right),
\end{align*}
where $d_{Nat}$ is the Natarajan dimension.
Note that VC dimension bounds as well as Natarajan dimension bounds are usually dimension
dependent, which makes them hardly applicable for practical large
scale problems (such as typical computer vision problems).

The above theoretical analysis indicates that it is a good
choice to use the local Rademacher complexity to analyze the generalization
ability of multi-class classification.

\section{Multi-Class Multiple Kernel Learning}
Motivated by the above analysis of generalization bound and convergence rate for multi-class classification,
we will exploit properties of the local Rademacher complexity
to devise two algorithms for multi-class multiple kernel learning (MC-MKL).
%\subsection{Motivation and Anlysis}
%As we know, there is a standard way to learn binary classification problems,
%in which the prediction of $\mathbf x$ is the scalar product between an hyperplane $\mathbf w$ and the transformed sample $\phi(\mathbf x)$,
%corresponding kernel $k(\mathbf x, \mathbf x')$ defined as $\phi(\mathbf x) \cdot \phi(\mathbf x')$.

In this paper, we consider the use of multiple kernel matrix,
$$\mathbf{K}_{\bm \mu}=\sum_{m=1}^M \mu_m\mathbf{K}_m.$$
A common approach to multi-class classification is the use of joint feature
maps $\phi(\mathbf x):\mathcal X \to \mathcal H$ \cite{TsochantaridisHJA04}.
For multiple kernel learning, we have $M$ feature mappings $\phi_m$, $m=1,\ldots,M$ and
$$\mathbf{K}_m(i,j)=\langle \phi_m(\mathbf x_i), \phi_m(\mathbf x_j)\rangle$$where $
m=1,\ldots,M$ and $i,j=1, \ldots, n$.
Let $\phi_{\bm \mu}(\mathbf x)=[\phi_1(\mathbf x),\ldots,\phi_M(\mathbf x)]$.
%We know $\mathbf{K}_{\bm \mu}(i,j)=\langle \phi_{\bm \mu}(\mathbf{x}_i), \phi_{\bm \mu}(\mathbf{x}_j)\rangle$.%
%So, the $\ell_{2,p}$-norm of $\mathbf{w}$ can be defined as $\|\mathbf w\|_{2,p}=\|[\|\mathbf{w}_1\|_2,\|\mathbf{w}_2\|_2,\ldots,\|\mathbf{w}_M\|_2]\|_p$. The dual norm of $\|\cdot\|_{2,p}$ is $\|\cdot\|_{2,q}$, where $1/p+1/q=1$ \cite{kakade2009duality}.
Using Theorem \ref{theorem-finally}, the $\ell_p$ hypothesis space of multiple kernels can be written as:
 \begin{align*}
 \label{hypothspapcemkl}
   \begin{aligned}
   \mathcal{H}_{mkl}=\Big\{h_{\mathbf{w},\mathbf{K}_{\bm \mu}}&=\left(\langle \mathbf w_1,\phi_{\bm \mu}(\mathbf x)\rangle,\ldots, \langle\mathbf w_K,\phi_{\bm \mu}(\mathbf x)\rangle\right),\\
   \left\|\mathbf  w \right\|_{2,p}&\leq 1, 1 < p \leq \frac{2\log K}{2\log K-1}
  \Big\}.
   \end{aligned}
 \end{align*}
 %Note that the tail sum is the difference between the trace and
% the $\theta$ largest eigenvalues:$\sum_{j>\theta}\lambda_j(\mathbf{K})=\mathrm{Tr}(\mathbf{K})-\sum_{j=1}^\theta\lambda_j(\mathbf{K})$,
% thus the tail sum can be calculated in $O(n^2\theta)$ for each kernel.
\subsection{Conv-MKL}
%The trace of kernel matrix $\mathbf{K}_{\bm \mu}$ can be the upper-bound of the global Rademacher complexity of $\mathcal{H}_{mkl}$.
The global Rademacher complexity of $\mathcal{H}_{mkl}$ can be bounded by the trace of kernel matrix $\mathbf{K}_{\bm \mu}$.
Existing works on \cite{LanckrietCBGJ02,BachLJ04,SonnenburgRSS06} use the following constraint to $\mathcal{H}_{mkl}$:
$\mathrm{Tr}(\mathbf{K}_{\bm \mu}) \leq 1.$
According the above theoretical analysis,
local Rademancher complexity (the tail sum of the eigenvalues of the kernel) lead to
tighter generalization bounds than the global Rademancher complexity (the trace).
Thus, we add local Rademancher complexity to restrict $\mathcal{H}_{mkl}$:
\begin{align}
    \mathcal{H}_{1}=\Big\{h_{\mathbf{w}, \mathbf{K}_{\bm \mu}} \in \mathcal{H}_{mkl}:\sum_{j > \zeta} \lambda_j(\mathbf{K}_{\bm \mu}) \leq 1\Big\},
\end{align}
where $\lambda_j(\mathbf K_{\bm \mu})$ is the $j$-th eigenvalues of $\mathbf K_{\bm \mu}$,
$\zeta$ is free parameter removing the $\zeta$ largest eigenvalues to control the tail sum.
Note that the tail sum is the difference between the trace and
the $\zeta$ largest eigenvalues: $\sum_{j>\zeta}\lambda_j(\mathbf{K}_{\bm \mu})=\mathrm{Tr}(\mathbf{K}_{\bm \mu})-\sum_{j=1}^\zeta\lambda_j(\mathbf{K}_{\bm \mu})$,
thus the tail sum can be calculated in $O(n^2\zeta)$ for each kernel.

One can see that $\mathcal{H}_{1}$ is not convex, and we know that:
\begin{align*}
    \begin{aligned}
        \sum_{m=1}^M\mu_m\sum_{j>\zeta}\lambda_{j}(\mathbf{K}_m)
        & =\sum_{m=1}^M\mu_m/\|\bm \mu\|_1 \sum_{j>\zeta}\lambda_{j}(\|\bm \mu\|_1\mathbf{K}_m)\\
        & \leq \sum_{j>\zeta} \lambda_j\big(\mathbf{K}_{\bm \mu}\big).
    \end{aligned}
\end{align*}
Thus, we consider the use of the convex $\mathcal{H}_2$ replace of $\mathcal{H}_1$:
\begin{align}
    \mathcal{H}_{2}=\Big\{h_{\mathbf{w}, \mathbf{K}_{\bm \mu}} \in \mathcal{H}_{mkl}:
    \sum_{m=1}^M\mu_m\sum_{j>\zeta}\lambda_{j}(\mathbf{K}_m) \leq 1\Big\}.
\end{align}
Note that by a renormalization of the kernels $\kappa_1,\ldots,\kappa_M$,
according to $\tilde{\mathbf K}_m:=\left(\sum_{j>\zeta}\lambda_j(\mathbf K_m)\right)^{-1}\mathbf K_m$
and $\tilde{\mathbf K}_{\bm \mu}=\sum_{m=1}^M\mu_m\tilde{\mathbf K}_m$,
we can simply rewrite $\mathcal{H}_2$ as
\begin{align*}
   \mathcal{H}_2&=\Big\{h_{\mathbf{w},\tilde{\mathbf{K}}_{\bm \mu}}=\left(\langle \mathbf w_1,\tilde{\phi}_{\bm \mu}(\mathbf x)\rangle,
   \ldots, \langle\mathbf w_K,\tilde{\phi}_{\bm \mu}(\mathbf x)\rangle\right),\\
   &\left\|\mathbf  w \right\|_{2,p}\leq 1, 1 < p \leq \frac{2\log K}{2\log K-1},
   \bm \mu\succeq 0,\|\bm \mu\|_1 \leq 1\Big\}.
\end{align*}
which is the commonly studied hypothesis class in multi-class multiple kernel learning.
A simpler algorithm in which precomputed kernel matrices
are regularized by local Rademancher complexity respectively seen in Algorithm  \ref{algorithm1}:
\begin{algorithm}[h]
   \caption{Conv-MKL}
   \label{algorithm1}
    \begin{algorithmic}
       \STATE {\bfseries Input:} precomputed kernel matrices $\mathbf{K}_1,\ldots,\mathbf{K}_M$ and $\zeta$
       \FOR{$i=1$ {\bfseries to} $M$}
        \STATE Compute tail sum: $r_m=\sum_{j>\zeta}\lambda_j\left(\mathbf{K}_m\right)$
        \STATE Normalize precomputed kernel matrix: $\widetilde{\mathbf{K}}_m=\mathbf{K}_m/r_m$
       \ENDFOR
       %\STATE Compute the minimizer of ERM over $\mathcal{H}_2$:%
       \STATE Use $\widetilde{\mathbf{K}}_{\bm \mu}=\sum_{m=1}^M \mu_m \widetilde{\mathbf{K}}_m$ in any $\ell_p$-norm MKL solver
    \end{algorithmic}
\end{algorithm}



\subsection{SMSD-MKL}
%According to local Rademancher analysis in Theorem \ref{theorem-finally},
%local Rademancher complexity leads to a shaper generalization bound.
In this subsection, we consider a more challenging case.
We add local Rademancher complexity in optimization formulation:
\begin{equation}
\label{optimization-problem}
    \min_{\mathbf w,\bm \mu}
    \overbrace{\frac{1}{n}\sum_{i=1}^n\ell(\mathbf w,\phi_{\bm \mu}(\mathbf{x}_i), y_i)}^{C(\mathbf w)}
    +\underbrace{\frac{\alpha}{2}\|\mathbf w\|_{2,p}^2
    +\beta\sum_{m=1}^M\mathbf{\mu}_mr_m}_{\Omega(\mathbf w)},
\end{equation}
where
$$\ell(\mathbf w,\phi_{\bm \mu}(\mathbf{x}_i), y_i)=
\max\limits_{i \not= j}\left|1-\left(\langle\mathbf w_i,
\phi_{\bm \mu}(\mathbf{x}_i)\rangle -\langle\mathbf w_j, \phi_{\bm \mu}(\mathbf{x}_j)\rangle\right)\right|_+,$$
and
$r_m=\sum_{j>\zeta}\lambda_j(\mathbf{K}_m)$ is the tail sum of the $m$-th kernel matrix,  $m=1,\ldots, M$.

\begin{algorithm}[t]
   \caption{SMSD-MKL}
   \label{algorithm2}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $\alpha, \beta, \bm r, T$
       \STATE {\bfseries Initialize:} $\mathbf w^1=\mathbf{0}, \pmb{\theta}^1=\mathbf{0}, \bm \mu^1=\mathbf{1}, q=2\log K $
       \FOR{$t=1$ {\bfseries to} $T$}
       \STATE Sample at random $(\mathbf{x}^t, y^t)$
       \STATE Compute the dual weight: $\pmb{\theta}^{t+1}=\pmb{\theta}^t-\partial{C(\mathbf w)}$
       \STATE $\nu_m^{t+1}=\|\bm \theta_m^{t+1}\|-t\beta r_m$, $\forall m=1,\ldots, M.$
       \STATE $\mu_m^{t+1}=\frac{sgn(\nu_m^{t+1})|\nu_m^{t+1}|^{q-1}}
       {\alpha\|\bm \theta_m^{t+1}\||\nu_m^{t+1}|_q^{q-2}}$, $\forall m=1,\ldots, M.$
%       \STATE $\mathbf w_m=\frac{sgn(\nu_m^{t+1})\bm \theta_m^{t+1}|\nu_m^{t+1}|^{q-1}}
%       {\alpha\|\bm \theta_m^{t+1}\||\nu_m^{t+1}|_q^{q-2}}$, $\forall m=1,\ldots, M.$
       \ENDFOR
    \end{algorithmic}
\end{algorithm}

%We cope with the non-differentiability of the hinge loss directly by sub-gradients instead of gradients.
Based on the Stochastic Mirror Descent framework for minimization in \cite{Shalev-ShwartzT11, OrabonaL11},
we design a Stochastic Mirror and Sub-gradient Descent lgorithm, called \texttt{SMSD-MKL}, to minimize \eqref{optimization-problem},
seen in Algorithm \ref{algorithm2}.

 As in the mirror descent algorithm, the algorithm maintain two weight vectors: primal $\mathbf w$ and dual $\pmb{\theta}$.
 Meanwhile, the optimization formulation can be divided into two parts: $C(\mathbf w)$
 to update the dual $\pmb{\theta}$ vector and $\Omega(\mathbf w)$ to update the primal vector $\mathbf w$ by
 the gradient of the Fenchel dual of $\Omega$.
 Actually, the algorithm puts the kernel weight $\pmb{\mu}$ aside when updating $\pmb{\theta}$,
 but $\pmb{\mu}$ is updated together with $\mathbf w$ according to a tricky link function given in Theorem \ref{theorem-fenchel-dual}.
 %The connection between the two vectors is via a link function $\pmb{\theta}=f(\mathbf w)$, where $f:\mathcal{K}\times\mathcal{M}\times\mathcal{H} \to \mathcal{K}\times\mathcal{M}\times\mathcal{H}$. Function $\Omega$ is strictly convex, so the algorithm use $\pmb{\theta}=f(\mathbf w)=\nabla\Omega(\mathbf w)$ as link function and $\mathbf w==\nabla\Omega^\ast(\pmb{\theta})$ as the inverse function.Similar with UFO-MKL in \cite{OrabonaL11},

Initially, the algorithm sets $\mathbf{w}=\mathbf{1}$, $\pmb{\theta}=\mathbf{1}$ and $\pmb{\mu}=\mathbf{1}$.
Especially, the algorithm initializes $q=2\log K$ to make the order of generalization reach
$\mathcal{O}\big(\frac{(\log K)^{2+1/\log K}}{n}\big)$, according to Theorem \ref{theorem-finally}.
For each iteration, the algorithm randomly samples a training example from train set.

For $C(\mathbf w)$, the algorithm updates the dual vector with the gradient of $C(\mathbf w)$.
Since hinge loss used in $C(\mathbf w)$ is not differentiable,
the algorithm uses sub-gradient of $z^t=\partial\ell(\mathbf w^t,\phi_{\bm \mu}(\mathbf{x}^t), y^t)$ instead,
where $\partial\ell(\mathbf w^t,\phi_{\bm \mu}(\mathbf{x}^t), y^t)$ is the sub-gradient w.r.t $\mathbf w^t$.

For $\Omega(\mathbf w)$, as in the UFO-MKL \cite{OrabonaL11},
the algorithm uses $\mathbf w=\nabla\Omega^\ast(\pmb{\theta})$ to update the primal vector $\mathbf w$,
of which the calculation has been given in Theorem \ref{theorem-fenchel-dual}.

Actually, the algorithm updates real numbers $\|\bm \theta_m^{t+1}\|$, $\nu_m^{t+1}$ and $\mu_m^{t+1}$ in scalar products instead of
high-dimensional variables $\mathbf w^{t+1}$ and $\bm \theta_m^{t+1}$.
The $\|\bm \theta_m^{t+1}\|$ can be calculated in an efficient incremental way by scalar values as following:
\begin{align*}
\|\pmb{\theta}_m^{t+1}\|
&=\|\pmb{\theta}_m^{t}-z^t\|_2^2
=\|\pmb{\theta}_m^{t}\|_2^2-2\pmb{\theta}_m^t \cdot z^t +\|z^t\|_2^2\\
&=\|\pmb{\theta}_m^{t}\|_2^2-2\pmb{\theta}_m^t \cdot (\phi_m(\mathbf{x}_i)-\phi_m(\mathbf{x}_j))
+ 2\kappa_m(\mathbf{x}_i, \mathbf{x}_i)^2
\end{align*}
where $z^t=\partial\ell(\mathbf w^t,\phi_{\bm \mu}(\mathbf{x}_t), y_t)$.

 \begin{theorem}
\label{theorem-fenchel-dual}
Let
$
    \bm \nu=\Big[\|\pmb{\theta}_1\|-\beta r_1,\ldots,
    \|\pmb{\theta}_M\|-\beta r_M\Big],
$
then the component $m$-th of $\;\nabla\Omega^\ast(\pmb{\theta})$ is
\begin{align*}
\frac{\mathrm{sgn}(\nu_m) \pmb{\theta}_m}{\alpha\|\pmb{\theta}_m\|}
       \frac{|\nu_m|^{q-1}}{\|\bm \nu\|_q^{q-2}},
\end{align*}
where $\mathrm{sgn}(x)$ is defined as
$sgn(x)=1$, if $x>0$, $sgn(x)=-1$, if $x<0$,
$sgn(x)\in[-1,+1]$, if $x=0$.
\end{theorem}

\begin{proof}
According to standard Legendre-Fenchel duality, we can get
\begin{align*}
\nabla\Omega^*(\pmb{\theta})&=\argmax\limits_{\mathbf w} \mathbf w \cdot \pmb{\theta} - \Omega(\mathbf w)\\
&=\argmax\limits_{\mathbf w} \mathbf w \cdot \pmb{\theta}
-\frac{\alpha}{2}\|\mathbf w\|_{2,p}^2
-\beta\pmb{\mu}\cdot\pmb{r}.
\end{align*}

To reach the above argmax, the derivative of argmax should be zero,
so $\mathbf w$ must be proportional to $\pmb{\theta}$.
As in UFO-MKL \cite{OrabonaL11}, we explicitly give a tricky link function
$\mathbf w_m=\mu_m\pmb{\theta}_m$ for different kernels.
By this explicit link function, the algorithm can update both $\mathbf w$ and $\pmb \mu$ by $\nabla\Omega^\ast(\pmb{\theta})$.
Then, for the convenience of computation, we focus on $c_m=\mu_m\|\pmb{\theta}_m\|$, rewriting the argmax:
\begin{align}
    \label{lp-regularization}
    \argmin \limits_{\pmb{c}} (\beta\mathbf{r} - \mathbf{a})\cdot\pmb{c}+\frac{\alpha}{2}\|\pmb{c}\|_p^2
\end{align}
where $\mathbf{a}=\Big[\|\pmb{\theta}_1\|,\ldots, \|\pmb{\theta}_M\|\Big]$.

The optimality condition of the above minimization problem \cite{rockafellar2015convex} states that $\pmb{c}^\ast$ is an optimal solution of \eqref{lp-regularization}. And we set the derivative of above argmin being zero
\begin{align}
    \label{l2-regularization-subgradient}
        \beta\mathbf{r}-\mathbf{a}+\alpha\pmb{c}^\ast=0
\end{align}
And then we can get $\pmb{c}^\ast=\frac{1}{\alpha}\Big(\mathbf{a}-\beta\mathbf{r}\Big)$.
Following similar arguments of Lemma 6 (see in Appendix), we find that it has a closed-form solution
\begin{align*}
    c_m=&f ^{-1}(c_m^\ast)=\nabla_m\Big(\frac{1}{2}\|{c}_m^\ast\|_q^2\Big)
    =\frac{sgn({c}_m^\ast)|{c}_m^\ast|^{q-1}}{\alpha\|\pmb{c}^\ast\|_q^{q-2}}.
\end{align*}
Let
$
    \bm \nu=\Big[\|\pmb{\theta}_1\|-\beta\mathbf{r}_1,\ldots,
    \|\pmb{\theta}_M\|-\beta\mathbf{r}_M\Big],
$
and use $\mu_m=c_m/\|\pmb{\theta}_m\|$ and $\mathbf w_m=\mu_m\pmb{\theta}_m$, We can get
\begin{align*}
    &{\mu}_m=\frac{sgn(\nu_m)|\nu_m|^{q-1}}{\alpha\|\pmb{\theta}_m\|\|\bm \nu\|_q^{q-2}}\;
    &\mathbf w_m=\frac{sgn(\nu_m)\pmb{\theta}_m|\nu_m|^{q-1}}{\alpha\|\pmb{\theta}_m\|\|\bm \nu\|_q^{q-2}}.\\
\end{align*}
Similar argmax has been analysis in Sec. 7.2 of \cite{Xiao10}.
\end{proof}

%\begin{lemma}
%  \label{inverse-mapping}
%  Let $p \in (1,2]$ and $q=p/(p-1)$, and then the norms $\|\pmb{c}\|_p$ and $\|\pmb{c}^\ast\|_q$ are dual to each other. Define the mapping $f:\mathcal{M} \to \mathcal{M}$ with
%  \begin{align*}
%    \pmb{c}^\ast_i=f_i(\pmb{c})=\nabla_i\Big(\frac{1}{2}\|\pmb{c}\|_p^2\Big)=\frac{sng(\pmb{c}_i)|\pmb{c}_i|^{p-1}}{\|\pmb{c}\|_p^{p-2}}, i=1,\ldots,n,
%  \end{align*}
%and the inverse mapping $f^{-1}$ with
%  \begin{align*}
%    \pmb{c}_i=f^{-1}_i(\pmb{c}^\ast)=\nabla_i\Big(\frac{1}{2}\|\pmb{c}^\ast\|_q^2\Big)=\frac{sng(\pmb{c}^\ast_i)|\pmb{c}^\ast_i|^{q-1}}{\|\pmb{c}^\ast\|_q^{q-2}}, i=1,\ldots,n,
%  \end{align*}
%\end{lemma}
%These mapping are often called {\em link functions} in machine learning (e.g., \cite{Gentile03a}).

\subsection{Convergence Rate Guarantee}
In this subsection, we analysis convergence rate guarantee for SMSD-MKL.
%In multi-class classification, there is one hyperplane for each class
%and $\phi_m(\cdot, \cdot)$ induces the transformation in $m$-th class.
Denote by $z^t=\partial\ell(\mathbf w,\phi_{\bm \mu}(\mathbf{x}^t), y^t)$,
we now state the convergence therem for any loss function that satisfies the following hypothesis
\begin{align}
\label{loss_function_hypothesis}
    \|z_m\| \leq L\|\phi(\mathbf w_m, y')\|_2, \forall t =1,\ldots, M, y' \in \mathcal{Y}.
\end{align}
The hinge loss for multi-class $\ell(\mathbf w,\phi(\mathbf{x}_i), y_i)=
\max\limits_{i \not= j}\left|1-\left(\langle\mathbf w_i, \phi(\mathbf{x}_i)\rangle -\langle\mathbf w_j, \phi(\mathbf{x}_j)\rangle\right)\right|_+$ satisfies the hypothesis with $L=\sqrt{2}$.

\begin{theorem}
Denote by $$f(\mathbf w)=\Omega(\mathbf w)+\frac{1}{n}\sum_{i=1}^n\ell(\mathbf w,\phi(\mathbf{x}_i),y_i)$$
and by $\mathbf w^\ast$ the solution that minimize \eqref{optimization-problem}.
Suppose that $\|\phi_m(\mathbf w^t,\cdot)\|_2 \leq 1$, and the loss function $\ell$ satisfies $\eqref{loss_function_hypothesis}$.
Let $\delta \in (0,1)$, then with probability at least $1 - \delta$ over the choices of the random samples
we have that after $T$ iterations of the SMSD-MKL algorithm
\begin{align*}
    f(\mathbf w^{T+1})-f(\mathbf w^\ast) \leq \frac{eL^2(1+\log T)\log M}{\alpha\delta T},
\end{align*}
where $e$ is the Euler's number.
\end{theorem}
\begin{proof}
Using \eqref{loss_function_hypothesis} and $\|\phi_m(\mathbf w^t, y^t)\|_2 \leq 1$, we have
\begin{align*}
    &\|\partial(\mathbf w^t,\phi(\mathbf{x}^t,\cdot), y^t)\|_{2,q} \\
    &\leq LM^{1/q} \max_{j=1, \ldots, M} \|\phi_m(\mathbf{x}^t, \cdot)\|_2 \leq LF^{1/q}
\end{align*}
The function $\Omega(\mathbf w)=\frac{\alpha}{2}\|\mathbf w\|_{2,p}^2 + \beta\mathbf{\mu} \cdot \mathbf{r}$ is
 $\alpha$-strongly convex w.r.t. the norm $\|\cdot\|_{2,q}$. Hence, using according to Theorem 1 in \cite{Shalev-ShwartzSS07},
 using $\eta=1$ and $g=\Omega$, and using Markov inequality as in \cite{Shalev-ShwartzSS07} we prove the stated result.
\end{proof}


\section{Experiments}
\begin{table*}[t]
\label{tabel:accuracy}
\small
\footnotesize
%\scriptsize
%\tiny
%\renewcommand{\captionfont}{\small}
   \caption{
    \small Comparison of average test accuracies of our \texttt{Conv-MKL} and \texttt{SMSD-MKL} with the others including
    the linear multi-class SVM (LMC),
    One-against-One (One vs. One),
    One-against-the-Rest(One vs. Rest),
    Generalized Minimal Norm Problem solver (GMNP),
    the $\ell_1$-norm Multiclass MKL ($\ell_1$ MC-MKL),
    the $\ell_2$-norm Multiclass MKL ($\ell_2$ MC-MKL),
    and mixed-norm MKL solved by stochastic gradient descent (UFO-MKL).
    We bold the numbers of the best method, and underline the numbers of the other methods
    which are not significantly worse than the best one.
   %We set the numbers of our method (EPSVM) to be bold if our method outperforms all other methods (KTA, CKTA, FSM, 3-CV, 5-CV and 10-CV).
   }
   \label{tabel:accuracy}
    %\centering
    \begin{tabular*}{\linewidth}{@{\extracolsep{-0.25cm}}lccccccccc}
    \toprule
                   &\texttt{Conv-MKL}          &\texttt{SMSD-MKL}         &LMC                 & One vs. One              & One vs. Rest                                              & GMNP                      & $\ell_1$ MC-MKL          & $\ell_2$ MC-MKL    & UFO-MKL                  \\ \hline
plant              &77.14$\pm$2.25             & \textbf{78.01$\pm$2.17}  & 70.12$\pm$2.96     & 75.83$\pm$2.69           &75.17$\pm$2.68       &75.42$\pm$3.64    & \underline{77.60$\pm$2.63}&75.49$\pm$2.48            &76.77$\pm$2.42\\
psortPos           &74.41$\pm$3.35             & \textbf{76.23$\pm$3.39}  &63.85$\pm$3.94      &73.33$\pm$4.21            &71.70$\pm$4.89       & 73.55$\pm$4.22   &71.87$\pm$4.87             &70.70$\pm$4.89            &74.56$\pm$4.04\\
psortNeg           &74.07$\pm$2.16             & \textbf{74.66$\pm$1.90}  &57.85$\pm$2.49      &73.74$\pm$2.87            &71.94$\pm$2.50   &\underline{74.27$\pm$2.51}   &72.83$\pm$2.20                             &72.42$\pm$2.65      &73.80$\pm$2.26 \\
nonpl              & \textbf{79.15$\pm$1.51}   &78.69$\pm$1.58            &75.16$\pm$1.48      &77.78$\pm$1.52            &77.49$\pm$1.53 &78.35$\pm$1.46    &77.89$\pm$1.79             &77.95$\pm$1.64            &78.07$\pm$1.56\\
sector             &\underline{92.83$\pm$2.62} & \textbf{93.39$\pm$0.70}  &93.16$\pm$0.66      &90.61$\pm$0.69            &\underline{93.34$\pm$0.61}                    &\                         &\                   &92.15$\pm$2.57            &92.60$\pm$0.47\\
segment            &96.79$\pm$0.91             & \textbf{97.62$\pm$0.83}  &95.07$\pm$1.11      &97.08$\pm$0.61            &97.02$\pm$0.80       &96.87$\pm$0.80    &96.98$\pm$0.64             &\underline{97.58$\pm$0.68}&97.20$\pm$0.82\\
vehicle            &79.35$\pm$2.27             & 77.28$\pm$2.78           &75.61$\pm$3.56      &82.72$\pm$1.92            &\textbf{85.11$\pm$1.94}       &81.57$\pm$2.24    &74.96$\pm$2.93             &76.27$\pm$3.15            &76.92$\pm$2.83\\
vowel              &98.82$\pm$1.19             &\textbf{98.83$\pm$5.57}   &62.32$\pm$4.97      &98.12$\pm$1.76            &98.22$\pm$1.83       &97.04$\pm$1.85    &98.27$\pm$1.22             &97.86$\pm$1.75            &98.22$\pm$1.62\\
wine               &\textbf{99.63$\pm$0.96}    &\textbf{99.63$\pm$0.96}   &97.87$\pm$2.80      &97.24$\pm$3.05            &98.14$\pm$3.04       &97.69$\pm$2.43    &98.61$\pm$1.75             &98.52$\pm$1.89            &99.44$\pm$1.13            \\
dna                &96.08$\pm$0.83             &\textbf{96.30$\pm$0.79}   &92.02$\pm$1.50      &95.89$\pm$0.56            &95.61$\pm$0.73       &94.60$\pm$0.94    &\underline{96.27$\pm$0.68}             &95.06$\pm$0.92            &95.84$\pm$0.61\\
glass              &\textbf{75.19$\pm$5.05}    & 73.72$\pm$5.80           &63.95$\pm$6.04      &71.98$\pm$5.75            &70.00$\pm$5.75       &71.24$\pm$8.14    &69.07$\pm$8.08             &74.03$\pm$6.41            &72.46$\pm$6.12\\
iris               &\textbf{98.00$\pm$2.25}    &\underline{97.33$\pm$2.53}&89.67$\pm$8.71      &96.76$\pm$2.63            &96.11$\pm$3.04       &96.11$\pm$4.11    &96.00$\pm$3.65             &95.44$\pm$3.66            &95.56$\pm$3.07\\
svmguide2          &82.69$\pm$5.65             &\textbf{85.17$\pm$3.83}   &81.10$\pm$4.15      &\underline{84.79$\pm$3.45}&\underline{84.27$\pm$3.03}  &81.77$\pm$3.45    &83.16$\pm$3.63             &\underline{83.84$\pm$4.21}            &82.91$\pm$3.09\\
satimage           &91.64$\pm$0.88             &\underline{91.78$\pm$0.82}&84.95$\pm$1.15      &90.67$\pm$0.91            &89.29$\pm$0.96       &89.97$\pm$0.81    &\underline{91.86$\pm$0.62} &90.43$\pm$1.27            &\textbf{91.92$\pm$0.83}\\
\bottomrule
\end{tabular*}
\vspace{-0.6cm}
\end{table*}

In this section, we will  compare our proposed \texttt{Conv-MKL} (Algorithm \ref{algorithm1}) and \texttt{SMSD-MKL} (Algorithm \ref{algorithm2})
with 7 popular multi-class classification methods:
 One-against-One \cite{knerr1990single}, One-against-the-Rest \cite{bottou1994comparison},
$\ell_1$-norm linear multi-class SVM (LMC) \cite{CrammerS02},
Generalized Minimal Norm Problem solver (GMNP) \cite{franc2005optimization},
the Multiclass MKL (MC-MKL) for $\ell_1$-norm and $\ell_2$-norm \cite{ZienO2007} 
and the mixed-norm MKL solved by stochastic gradient descent (UFO-MKL) \cite{OrabonaL11}.
Actually, we complete comparison tests via implements in LIBSVM \footnote{Available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/} 
(One-against-One and One-against-the-Rest), 
the DOGMA libary \footnote{Available at http://dogma. sourceforge. net} \cite{orabona2009dogma} (LMC, GMNP, 
$\ell_1$-nomr and $\ell_2$-norm MC-MKL) and the SHOGUN-6.1.3 \footnote{Available at http://www.shogun-toolbox.org/} (UFO-MKL). 
What's more, we implement the proposed \texttt{Conv-MKL} and \texttt{SMSD-MKL} algorithms based on UFO-MKL.

We experiment on 14 publicly avaliable datasets:
four of them evaluated in \cite{ZienO2007} (plant, nonpl, psortPos, and psortNeg)
\footnote{Available at http://www.raetschlab.org/suppl/protsubloc}
and the others from LIBSVM Data. %\footnote{Available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/}.
For each data set, we use the Gaussian kernel
$K(\mathbf{x}, \mathbf{x}')=\exp\Big(-\frac{\|\mathbf{x}-\mathbf{x}'\|_2^2}{2\tau}\Big)$ as our basic kernels, 
where $\tau \in {2^i, i=-10,-9,\ldots,9,10}$. For single Kernel methods (One vs. One, One vs. Rest, GMNP), 
we choose a kernel which gives the highest performance by cross-validation. 
Meanwhile, we use all basic kernels for MKL methods (\texttt{Conv-MKL}, \texttt{SMSD-MKL}, $\ell_1$ MC-MKL, $\ell_2$ MC-MKL, UFO-MKL). 
For \texttt{Conv-MKL} and \texttt{SMSD-MKL}, we fix the parameter $\zeta=2$ to curve local Rademancher complexity.
The regularization parameterized $C \in {2^i, i=-2, \ldots, 12}$ of all algorithms and
the parameter $\gamma \in {10^i, i=-4, \ldots, 1}$ of \texttt{SMSD-MKL} are determined by 10-folds cross-validation.
Other parameters of the compared algorithms follow the same experimental setting in their papers.
For each data set, we run all methods 50 times with randomly selected 80\% for training and 20\% for testing.
The use of multiple training/testing partitions allows an estimate of the statistical significance of differences
in performance between methods.
% Let $A_i$ and $B_i$ be the test errors of methods A and B in partition $i$,
%and $d_i=B_i-A_i, i=1,\ldots,30$. Let $\bar{d}$ and $S_d$ be the mean and standard error of $d_i$.
%Under $t$-test, with confidence level 95\%, we claim that A is significantly better than B (or equivalently B significantly worse than A)
%if the $t$-statistic $\frac{\bar{d}}{S_d/\sqrt{50}} > 1.676$.
All statement of statistical significance in the remainder refer to a 95\% level of significance under $t$-test.

The average test accuracies are reported in Table \ref{tabel:accuracy}. 
The results show: 1) Our \texttt{Conv-MKL} and \texttt{SMSD-MKL}methods give best results on nearly all data sets
except \textit{vehicle} and \textit{satimage};
2) \texttt{SMSD-MKL} is significantly better than \texttt{Conv-MKL} because of wining 9 data sets and defeated on 2 data sets;
3) Compared with Multiple Kernel Learning methods ($\ell_1$ MC-MKL, $\ell_2$ MC-MKL and UFO-MKL),
our methods get better results over almost all data sets except that only UFO-MKL works slightly better than ours on \textit{satige};
4) The multiple kernel methods usually better than the compared single kernel methods (One vs. One, One vs. Rest and GMNP),
since \textit{vehicle} leads fairly good results for single kernel method but terrible results for MKL methods;
5) The kernel based methods have better performance than linear classification machine (LMC) on all data sets.

The above results show that the use of local Rademacher complexity can significantly improve
the performance of multi-class multiple kernel learning algorithms,
which conforms to our theoretical analysis.
\section{Conclusion}
In this paper, we studied the generalization performance of multi-class classification,
and derived a sharper data dependent generalization error bound using local Rademacher complexity,
which is much sharper than existing data-dependent generalization bounds of multi-class classification.
Then, we design two algorithms with statistical guarantees and fast convergence rates:
a convex optimization way based on any existing MC-MKL algorithms,
and the other that put local Rademancher complexity in optimization formulation,
for which we give a stochastic mirror and sub-gradient descent method.
Empirical results show our methods outperform the state-of-the-art multi-class classification methods.
Based on local Rademacher complexity, our analysis can be used as a solid basis for the
design of new multi-class kernel learning algorithms.


\bibliography{MC_MKL_LR}
\bibliographystyle{icml2018}

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
