%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,amsfonts}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\argsup}{\operatornamewithlimits{arg\,sup}}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multi-Class Classification: Fast Rate and Algorithms}

\begin{document}

\twocolumn[
\icmltitle{Multi-Class Kernel Learning: Fast Rate and Algorithms}
\icmlkeywords{Multi-Class Classification, Kernel Methods, Local Rademacher Complexity}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\icmlauthor{Your Name}{email@yourdomain.edu}
%\icmladdress{Your Fantastic Institute,
%            314159 Pi St., Palo Alto, CA 94306 USA}
%\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
%\icmladdress{Their Fantastic Institute,
%            27182 Exp St., Toronto, ON M6H 2T1 CANADA}
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    The study on generalization performance of multi-class
    classification algorithms is a fundamental issue in statistical learning theory.
    %Several generalization bounds have been proposed based on different measures,
%    but the convergence rates of the existing bounds are usually at most $\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$,
%    where $K$ and $n$ are the number of classes and size of the sample, respectively.
    In this paper, we study the generalization performance of multi-class classification
    based on the notion of local Rademacher complexity and obtain a data-dependent
    generalization error bound with convergence rate $\mathcal{O}\big(\frac{(\log K)^{2+{1}/{\log K}}}{n}\big)$,
    substantially improving the state-of-art linear dependence in the existing data-dependent generalization analysis,
    where $K$ and $n$ are the number of classes and size of the sample, respectively.
    The theoretical analysis motivates us to
    devise two effective multi-kernel multi-class algorithms
    with statical guarantees and fast convergence rate.
    %Experimental results on lots of benchmark data sets show that our proposed methods can significantly
%    outperform the existing multi-class classification methods.
    Theoretical analysis and empirical results demonstrate that the proposed multi-kernel multi-class
    classification algorithms outperforms the state-of-the-art multi-class kernel learning methods.
    %a convex optimization way based on any existing MC-MKL algorithms, and another one that add local Rademancher complexity in optimization formulation for which we give a stochastic sub-gradient descent method in detail
\end{abstract}

\section{Introduction}
\label{submission}
Multi-class classification is an important problem in various applications,
such as natural language processing \cite{Zhang2005cs}, information retrieval \cite{hofmann2003learning}, computer vision \cite{deng2009imagenet},
web advertising \cite{beygelzimer2009conditional}, etc.
The statistical learning theory of binary classification is by now relatively well developed \cite{vapnik1998naturestatistical,mohri2012foundations},
but there are still numerous statistical challenges in its multi-class extension \cite{maximov2016tight}.

To understand the existing multi-class classification algorithms and guide the development of new ones,
people have to investigate the generalization ability of multi-class classification algorithms.
A sharper generalization bound usually implies more consistent
performance on the training set and the test set.
In recent years, some generalization bounds have been developed to
estimate the ability of multi-class classification algorithms based on different measures,
such as
VC-dimension \cite{allwein2000reducing},
Natarajan dimension \cite{daniely2014optimal},
covering Number \cite{guermeur2002combining,zhang2004statistical,Hill2007},
Rademacher Complexity \cite{koltchinskii2002empirical,mohri2012foundations,cortes2013multi}, etc.
Although there have been several recent advances in the studying of
generalization bounds of multi-class classification algorithms,
the convergence rates of the existing generalization bounds are usually at most
 $\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$,
where $K$ and $n$ are the number of classes and size of the sample, respectively.



In this paper, we  study the generalization performance of multi-class classification,
and derive a novel data-dependent generalization bound via the notion of local Rademacher complexity.
The rate of this bound is $\mathcal{O}\big(\frac{(\log K)^{2+{1}/{\log K}}}{n}\big)$,
which substantially improves on  the existing data-dependent generalization bounds.
Moreover, we create two new multi-kernel multi-class classification algorithms
with statical guarantees and fast convergence rate.
Experimental results on lots of benchmark data sets show that our proposed methods can significantly
outperform the existing multi-class classification methods.
The major contributions of this paper include:
 1): A new local Rademacher complexity-based bound of fast convergence rate for multi-class classification is established;
 2): Two novel algorithms are proposed with convergence guarantee: a convex optimization way based on
 any existing MC-MKL algorithms,
 and another one that add local Rademancher complexity in optimization formulation
 for which we give a stochastic sub-gradient descent method;


%The rest of the paper is organized as follows.
%In Section 2, we introduce the related work.
%In Section 3, we give
%some notations and preliminaries.
%In Section 4,
%we propose the upper bound of the local Rademacher complexity for multi-class classification,
%and further derive a generalization error bound of fast convergence rate.
%%We empirically analyze the performance of our proposed bounds in Section 5.
%We end in Section 5 with conclusion.
%Due to limited space,
%some proofs are given in the supplementary material.

\section{Related Work}
In this section,
we  introduce  the related work:
multi-class classification bounds,
local Rademacher complexity and kernel-based multi-class classification.

\subsection{Multi-Class Classification Bounds}
 Rademacher complexity, VC-dimension, and covering number are  three
 popular tools to derive generalization bounds for multi-class classification:

\textbf{Rademacher Complexities Bounds.}
   Koltchinskii and Panchenko \yrcite{koltchinskii2002empirical}
    and Koltchinskii, Panchenko, and Lozano \yrcite{koltchinskii2001some}
    first introduced a margin-based bound for multi-class classification in terms of Rademacher complexity.
    This bound was slightly improved in \cite{mohri2012foundations,cortes2013multi}.
    Maximov and Reshetova \cite{maximov2016tight} gave a new Rademacher complexity based bound
    that is linear in the number of classes.
    % give a new Rademacher complexity based
    %bound depends linearly in the number of classes of $\mathcal{O}\big(\frac{k}{\sqrt{n}}\big)$.
    %The bound involves the marginal distribution
    %of the classifier and the Rademacher complexity of the hypothesis class
    Based on the $\ell_p$-norm regularization,
    Lei, Binder, and Klof \yrcite{lei2015multi} introduced a bound
    with a logarithmic dependence on the number of class size.

    Instead of using the Rademacher complexity,
    in this paper, we consider using local Rademacher complexity
    to obtain a sharper bound, which  substantially improves on
    the existing Rademacher complexity bounds.

    \textbf{VC-dimension Bounds.}
    Allwein, Schapire, and Singer \yrcite{allwein2000reducing}  used the notion of VC-dimension
    for multi-class learning problems,
    and derived a VC-dimension based bound.
    Natarajan dimension was introduced in \cite{Natarajan1989} in
    order to characterize multi-class PAC learnability,
    which exactly matches the notion of Vapnik-Chervonenkis
    dimension in the case of binary classification.
    Daniely and Shalev-Shwartz \yrcite{daniely2014optimal} derived a risk bound with Natarajan dimension for multi-class classification.
    VC dimension and Natarajan dimension are the important tools to derive generalization bounds,
    however these bounds are usually dimension
    dependent, which makes them hardly applicable for practical large
    scale problems (such as typical computer vision problems).

   \textbf{Covering Number  Bounds.}
    Based on the $\ell_\infty$-norm covering number bound of
    linear operators, Guermeur \yrcite{guermeur2002combining} obtained
    a generalization bound exhibiting a linear dependence on the class size,
    which was improved by \cite{zhang2004statistical} to a radical dependence.
    Hill and Doucet \yrcite{Hill2007} derived a class-size independent risk guarantee.
    However, their bound is based on a delicate definition of margin,
    which is not commonly used in the mainstream multi-class literature.
\subsection{Local Rademacher Complexity}
One of the useful data-dependent complexity measures used in the generalization analysis
for  traditional  binary classification
is the notion of (global) Rademacher complexity \cite{bartlett2003rademacher}.
However,
it provides global estimates of the complexity of the function class,
that is, it does not reflect
the fact that the algorithm will likely pick functions that have a small error.
In recent years,
several authors have applied
 \emph{local} Rademacher complexity to obtain  better generalization error bounds
for traditional binary classification \cite{Bartlett2005lrc,Koltchinskii2006lrcoiirm}.
%The local Rademacher complexity considers Rademacher averages
%of a smaller subset of the hypothesis set,
%so it is always smaller than the corresponding global one.
%There are some work consider the use of local Rademacher complexity to
%derive tighter bounds for binary classification.
However,
numerous statistical challenges remain in multi-class case, and it is still unclear how to use
this tool to derive a tighter bound for multi-class.
In this paper,
we bridge this gap by deriving a sharper generalization bound using local Rademacher complexity.
%we show how to use the notion of local Rademacher complexity to derive a sharper generalization bound,
%which fills this gap.
%we concerns the structural relationship
%of Gaussian complexities since it is based on a comparison result
%among different Gaussian processes.
%Our result is a non-trivial extension of the local Rademacher complexity of
%binary classification for multi-class classification.
\subsection{Multi-Class Kernel Learning Algorithms}
Improvements in multi-class classification has emerged as one of success stories in Multiple Kernel Learning \cite{ZienO2007}
in which a one-stage multi-class MKL algorithm is presented as a generalization of multi-class loss function \cite{CrammerS02,TsochantaridisHJA04}.
And Orabona designed stochastic gradient methods, named
OBSCURE \cite{OrabonaJC10} and
UFO-MKL \cite{OrabonaL11}, which optimize primal versions of equivalent problems.
Then, tighter margin-based bounds for multi-class classification in terms of Rademacher complexity are introduced \cite{mohri2012foundations,cortes2013multi}.


\section{Notations and Preliminaries}
We consider multi-class classification problems with $K\geq 2$ classes in this paper.
Let $\mathcal{X}$ be the input space and $\mathcal{Y}=\{1,2,\ldots,K\}$ the output space.
Assume that we are given a sample
$
  \mathcal{S}=\{z_1=(\mathbf  x_1,y_1),\ldots, z_n=(\mathbf  x_n,y_n)\}
$
of size $n$ drawn i.i.d. from a fixed,
but unknown probability distribution $\mu$ on $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$.
Based on the training examples $\mathcal{S}$,
we wish to learn a scoring rule $h$ from a space $\mathcal{H}$ mapping from $\mathcal{Z}$ to $\mathbb{R}$
and use the mapping $\mathbf x\rightarrow \argmax_{y\in\mathcal{Y}} h(\mathbf x,y)$ to predict.
For any hypothesis $h\in\mathcal{H}$,
the margin of a labeled example $z=(\mathbf x,y)$ is defined as
\begin{align*}
  \rho_h(z):= h(\mathbf x,y)-\max_{y'\not =y} h(\mathbf x,y').
\end{align*}
The $h$ misclassifies the labeled example $z=(\mathbf x,y)$
if $\rho_h(z)\leq 0$ and thus the expected risk incurred
from using $h$ for prediction is
$
  L(h):=\mathbb{E}_\mu[1_{\rho_h(z)\leq 0}],
$
where $1_{t\leq 0}$ is the 0-1 loss,
$1_{t\leq 0}=1$ if $t\leq 0$, otherwise 0.
Since 0-1 loss is hard to handle in learning machines,
one ususally considers the proxy loss:
\begin{definition}[Regular Loss]
  We call $\ell$ a $\zeta$-regular loss if it satisfies the following properties:
  1) $\ell(t)$ bounds the 0-1 loss: $1_{t\leq 0}\leq \ell(t)$;
  2) $\ell$ is $\zeta$-smooth, i.e., $|\ell'(t)-\ell'(s)|\leq \zeta|t-s|$;
  3) $\ell$ is decreasing and it has a zero point $c_\ell$, i.e., $\ell(c_\ell)=0$.
\end{definition}
%Some examples of $\zeta$-regular loss functions include the popular squared Hinge loss
%$\ell(t)=\left(\max\{0,1-t\}\right)^2$, squared margin loss $\ell(t)=\left(\max\{0,1-t+\rho\}\right)^2$.
%Note that when $\theta\rightarrow 0$, ramp loss and Huber loss converge to the  0-1 loss and
%Hinge loss $\ell(t)=\max\{0,1-t\}$, respectively.

Any function $h:\mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}$ can be equivalently represented by the vector-valued function
$(h_1,\ldots,h_K)$ with $h_j(\mathbf x)=h(\mathbf x,j)$, $\forall j=1,\ldots,K$.
Let $\kappa:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}$ be a Mercer kernel with $\phi$ being the associated feature map,
i.e., $\kappa(\mathbf x,\mathbf x')=\langle \phi(\mathbf x),\phi(\mathbf x')\rangle$.
In this paper, we consider the following $\ell_p$-norm hypothesis space associated with the kernel $\kappa$:
 \begin{align}
 \label{hypothspapce}
   \begin{aligned}
   \mathcal{H}_{p,\kappa}=&\Big\{h_\mathbf{w}=\left(\langle \mathbf w_1,\phi(\mathbf x)\rangle,\ldots, \langle\mathbf w_K,\phi(\mathbf x)\rangle\right):\\
  & \left\|\mathbf  w \right\|_{2,p}\leq 1, 1\leq p\leq 2
  \Big\},
   \end{aligned}
 \end{align}
 where $\mathbf w=(\mathbf w_1,\ldots,\mathbf w_K)$ and $\|\mathbf w\|_{2,p}=\left[\sum_{i=1}^K\|\mathbf x_i\|_2^p\right]^{\frac{1}{p}}$
 is the $\ell_{2,p}$-norm.
 For any $p\geq 1$, let $p^\ast$ be the dual exponent of $p$ satisfying $\frac{1}{p}+\frac{1}{p^\ast}=1$.
 Note that,  $\forall 1\leq p\leq 2$, we have $p^\ast\geq 2$.

The space of loss function associated with $\mathcal{H}_{p,\kappa}$  is denoted by
\begin{align}
\label{eq-sapce-loss-functions}
  \mathcal{L}=\left\{\ell_h:=\ell(\rho_h(z)):h\in\mathcal{H}_{p,\kappa}\right\}.
\end{align}
Let $L(\ell_h)$ and  $\hat{L}(\ell_h)$ be expected generalization error and
 empirical error  with respect to $\ell_h$:
\begin{align*}
  L(\ell_h):=\mathbb{E}_\mu[\ell(\rho_h(z))] \text{ and } \hat{L}(\ell_h)=\frac{1}{n}\sum_{i=1}^n\ell(\rho_h(z_i)).
\end{align*}




%Note that if $\ell$ is a $\zeta$-regular loss,
%then
%\begin{align*}
%  L(h)\leq  L(\ell_h).
%\end{align*}
%Thus, we can use $L(\ell_h)$ to bound $L(h)$.

%In order to estimate $L(\ell_h)$, we introduce the Rademacher complexity.
%To this end,
%we have to study the behavior of the difference between the
%risk and the empirical risk.
%To this end, we introduce the notion of uniform deviation of $\mathcal{L}$,
%denoted as
%\begin{align}
%\label{eq-Uniform-deviation-definition}
%  \hat{U}_n(\mathcal{L})
%    =\sup_{\ell_h\in\mathcal{L}}\left\{L(\ell_h)-\hat{L}(\ell_h)\right\}.
%\end{align}
%Note that
%$
%  \left\{L(\ell_h)-\hat{L}(\ell_h)\right\}_{\ell_h\in\mathcal{L}}
%  \leq \hat{U}_n(\mathcal{L}),
%$
%so we have
%\begin{align*}
%  L(\ell_h)\leq \hat{L}(\ell_h)+
%  \hat{U}_n(\mathcal{L}),
%  \forall \ell_h\in\mathcal{L}.
%\end{align*}
%$\hat{U}_n(\mathcal{L})$
%is not computable,
%but we can bound its value via  the
% Rademacher complexity $\mathcal{L}$:
\begin{definition}[Rademacher complexity]
Assume $\mathcal{L}$ is a space of loss functions as defined in Equation \eqref{eq-sapce-loss-functions}.
Then the empirical Rademacher complexity of $\mathcal{L}$ is:
%Then the empirical \textbf{ranking Rademacher complexity} of $\mathcal{L}$ is:
%    Let $\mathcal{H}$ be a family of real-valued functions
%    defined on $\mathcal{Z}$ and $\mathcal{S}=(z_1,\ldots,z_n)$ a fixed sample of size $n$ with elements in $\mathcal{Z}$.
    %Then, the empirical Rademacher and Gaussian complexities of $\mathcal{H}$ with respect to the sample $\mathcal{S}$ are defined by
    \begin{align*}
      \hat{\mathcal{R}}(\mathcal{L}):=\mathbb{E}_{\bm \sigma}
      \left[\sup_{\ell_h\in\mathcal{L}}
     \frac{1}{n}\sum_{i=1}^n\sigma_i \ell_{h}(z_i)
          \right],
    \end{align*}
     where $\sigma_1,\sigma_2, \ldots,\sigma_n$
    is an i.i.d. family of Rademacher variables taking values -1 and 1
    with equal probability independent of the sample $\mathcal{S}=(z_1,\ldots,z_n)$.
    The  Rademacher complexity of $\mathcal{L}$ is:
    \begin{align*}
      %\label{def-expect-Rademacher}
      \mathcal{R}(\mathcal{L})=\mathbb{E}_{\mu}\hat{\mathcal{R}}(\mathcal{L}).
    \end{align*}
\end{definition}
%Rademacher complexity is the most powerful tool to get generalization bounds for multi-class classification, currently.
Generalization bounds based on the Rademacher complexity for multi-class classification
are standard \cite{koltchinskii2002empirical,koltchinskii2001some,mohri2012foundations}:
with probability $1-\delta$,
\begin{align*}
  L(h)\leq \inf_{0<\gamma<1}\left(\hat{L}(h_\gamma)+\mathcal{O}\left(\frac{\mathcal{R}(\mathcal{L})}{\gamma}+\frac{\log1/\delta}{\sqrt{n}}\right)\right),
\end{align*}
where $\hat{L}(h_\gamma)=\frac{1}{n}\sum_{i=1}^n\left[1_{\rho_h(z_i)\leq \gamma}\right]$.
Since $\mathcal{R}(\mathcal{L})$ is in the order of $\mathcal{O}(\frac{K^2}{\sqrt{n}})$ for various  kernel multi-class in practice,
so the standard Rademacher complexity bounds converge at rate
$\mathcal{O}\left(\frac{K^2}{\sqrt{n}}\right)$, usually.

Although Rademacher complexity is the popular tool to bound generalization,
it does not take into consideration the fact that,
typically, the hypotheses selected by a learning algorithm
have a better performance than in the worst case and
belong to a more favorable sub-family of the set of all hypotheses \cite{cortes2013learning}.
Therefore, to derive sharper generalization bound,
we consider the use of the local Rademacher complexity in this paper.
To this end,
  let $\mathcal{L}^r$ be a star-shaped space of $\mathcal{L}$ with respect to $r>0$,
  \begin{align}
  \label{def-localrademchercomplexity}
    \mathcal{L}^r=\left\{
        a\ell_h\Big|a\in[0,1],\ell_h\in\mathcal{L},
        L[(a\ell_h)^2]\leq r
    \right\},
  \end{align}
 where $L(\ell_h^2)=\mathbb{E}_\mu\left[\ell^2(\rho_h(z))\right]$.
% Note that
% \begin{align*}
%   \forall \ell_h\in\mathcal{L}^r,
%   V^2(\ell_h)=L(\ell_h^2)-[L(\ell_h)]^2
%   \leq L(\ell_h^2)\leq r,
% \end{align*}
% so the variance of each element in $\mathcal{L}^r$ is smaller than $r$.
 %Therefore,
% for all $\ell_h^r\in\mathcal{L}^r$,
% its variance  $V^2(\ell_h)=L(\ell_h^2)-[L(\ell_h)]^2$,
% so the following inequality holds
 %Since such a small class can  also have a substantially
% smaller complexity, we can obtain sharp bound.
%The local ranking Rademacher complexity is defined as follows:
%For this purpose, we consider using the  local
%ranking Rademacher complexity defined as follows:
 \begin{definition}[Local Rademacher Complexity]
 \label{def-thereee}
   For any $r>0$, the local Rademacher complexity of $\mathcal{L}$ is defined as
   \begin{align*}
     \mathcal{R}(\mathcal{L}^r):=
     \mathcal{R}\left(\left\{
        a\ell\Big|a\in[0,1],\ell\in\mathcal{L},
        L[(a\ell)^2]\leq r\right\}\right).
   \end{align*}
 \end{definition}
 The key idea to obtain sharper generalization error bound
 is to choose a much smaller class $\mathcal{L}^r\subseteq\mathcal{L}$
 with as small a variance as possible,
 while requiring that the solution is still in $\{h|h\in\mathcal{H}_{p,\kappa},\ell_h\in \mathcal{L}^r\}$.
%If the local Rademacher complexity is known,
%the risk be bounded in terms of the fixed point of the local Rademacher complexity,
%besides constants and $\mathcal{O}(\log K/n)$ terms.

In the following, we assume that
$\vartheta=\sup_{\mathbf x\in \mathcal{X}}\kappa(\mathbf x,\mathbf x)< \infty$,
and $\ell_h:\mathcal{Z}\rightarrow [0,M]$,
$M>0$ is a constant.
\section{Sharper Generalization Bounds}
In this section, we first estimate the local Rademacher complexity,
and further derive a sharper generalization bound.

\subsection{Local Rademacher Complexity}
In this subsection, we will estimate the local Rademacher complexity of multi-class classification.
\begin{theorem}
\label{rademacherlocal}
  If $\ell$ is a $\zeta$-smooth loss, i.e., $|\ell'(t)-\ell'(s)|\leq \zeta|t-s|$,
  then with probability $1-\delta$,
  we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r) \leq \frac{c_{M,\vartheta}\xi(K)\sqrt{\zeta r}\log^{\frac{3}{2}}(n)}{\sqrt{n}}+\frac{4\log(1/\delta)}{n},
  \end{align*}
  where
  \begin{align*}
  \xi(K)=
  \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } p^\ast\geq 2\log K,\\
      &(2p^\ast)^{1+\frac{1}{p^\ast}}K^{\frac{1}{p^\ast}}, &&\text{otherwise},
      \end{aligned}
      \right.
  \end{align*}
  $c_{M,\vartheta}$ is a constant depends on $M$ and $\vartheta$.
\end{theorem}
\begin{proof}
  According to the Lemma 3.6 of \cite{oneto2013improved},
  with probability $1-\delta$, we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r)\leq \hat{\mathcal{R}}(\mathcal{L}^r)+\sqrt{\frac{2\log(1/\delta)\mathcal{R}(\mathcal{L}^r)}{n}}.
  \end{align*}
  Note that $\forall a,b\geq 0, \sqrt{ab}\leq \frac{a}{2}+\frac{b}{2}$.
  Thus, we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r)\leq \hat{\mathcal{R}}(\mathcal{L}^r)+\frac{\mathcal{R}(\mathcal{L}^r)}{2}+\frac{\log(1/\delta)}{n}.
  \end{align*}
  So, we can obtain that
  \begin{align}
  \label{eqRhatRnew}
    \mathcal{R}(\mathcal{L}^r)\leq 2\hat{\mathcal{R}}(\mathcal{L}^r)+\frac{2\log(1/\delta)}{n}
  \end{align}
  From the Lemma 2.2 of \cite{Srebro2010lrc},
  we know that if $\ell$ is a $\zeta$-smooth loss function,
  \begin{align}
   \label{RlocalRnew}
    \hat{\mathcal{R}}(\mathcal{L}^r)\leq c_M\sqrt{\zeta r}\log^{\frac{3}{2}}(n)\hat{\mathcal{R}}(\mathcal{L}),
  \end{align}
  where $c_M$ is a constant depends on $M$.
  Substituting  \eqref{RlocalRnew} into \eqref{eqRhatRnew},
  we have
  \begin{align}
  \label{eq-middle-result-hatR}
    \hat{\mathcal{R}}(\mathcal{L}^r)\leq 2c_M\sqrt{\zeta r}\log^{\frac{3}{2}}(n)\hat{\mathcal{R}}(\mathcal{L})
    +\frac{2\log(1/\delta)}{n}.
  \end{align}
  From Theorem 1 in Appendix A,
  we have
  \begin{align*}
    \hat{\mathcal{R}}(\mathcal{L})\leq \frac{1}{\sqrt{n}}\times
      \left\{
      \begin{aligned}
      &\sqrt{e\vartheta}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } p^\ast\geq 2\log K,\\
      &\sqrt{\vartheta}(2p^\ast)^{1+\frac{1}{p^\ast}}c^{\frac{1}{p^\ast}}, &&\text{otherwise.}
      \end{aligned}
      \right.
  \end{align*}
  Substituting the above result into \eqref{eq-middle-result-hatR},
  the proof is over.
%  the proof is over.
\end{proof}

%\begin{proof}
%    We sketch the proof here and leave the details in the
%    supplementary material.
%    We first show that if $\ell$ is a $\zeta$-regular loss,
%    the local Rademacher complexity $\mathcal{R}(\mathcal{L}^r)$ can be bounded with the empirical Rademacher complexity $\hat{\mathcal{R}}(\mathcal{L})$:
%    \begin{align*}
%    \mathcal{R}(\mathcal{L}^r)\leq C\sqrt{\zeta r}\log^{\frac{3}{2}}(n)\hat{\mathcal{R}}(\mathcal{L})+\frac{2\log(1/\delta)}{n}.
%  \end{align*}
%  Then, based on the relationship between Rademacher complexity and Gaussian complexity,
%    \begin{align*}
%      \hat{\mathcal{R}}(\mathcal{L})\leq \sqrt{\frac{\pi}{2}}\hat{\mathcal{G}}(\mathcal{L}),
%    \end{align*}
%    where $\hat{\mathcal{G}}(\mathcal{L})$ is the Gaussian complexity on $\mathcal{L}$.
%   Finally, we concerns the structural relationship of Guassian complexities based on a comparison result among different Gaussian processes,
%   we give a upper bound of $\hat{\mathcal{G}}(\mathcal{L})$ with logarithmic dependence on the class size:
%   \begin{align*}
%     \hat{\mathcal{G}}(\mathcal{L}\leq \frac{\xi(K)}{\sqrt{n}}.
%   \end{align*}
%\end{proof}
%From \cite{cortes2013multi}, we know that,
Note that the order of the (global) Rademacher complexity over $\mathcal{L}$ is usually
$\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$ for various kernel multi-classes.
From Theorem \ref{rademacherlocal}, one can see that the order of the local Rademacher complexity is
\begin{align*}
  \mathcal{R}(\mathcal{L}^r)=\mathcal{O}\left(\frac{\sqrt{r}\xi(K)}{\sqrt{n}}+\frac{1}{n}\right).
\end{align*}
Note that $\xi(K)$ is logarithmic dependence on $K$ when $p^\ast\geq 2\log K$.
For $2\leq p^\ast < 2\log K$, $\xi(K)=\mathcal{O}(K^{\frac{2}{p^\ast}})$
which is also substantially milder than the quadratic dependence for Rademacher complexity.
If we choose a suitable value of $r$,
the order can even reach $\mathcal{O}\big(\frac{(\log K)^{2+1/\log K}}{n}\big)$ (see in the next subsection),
which substantially improves the Rademacher complexity bounds.
\subsection{A Sharper Generalization Bound using Local Rademacher Complexity}
In this subsection, we will derive a sharper bound for multi-class classification based on
the notion of local Rademacher complexity.
\begin{theorem}
\label{theorem-finally}
  If $\ell$ is a $\zeta$-regular loss.
  Then, $\forall h\in\mathcal{H}_{p,\kappa}$ and $\forall k>\max(1,\frac{\sqrt{2}}{2M})$,  with probability
  $1-\delta$, we have $L(h)\leq $
  \begin{align*}
   \leq \max\left\{
        \frac{k}{k-1}\hat{L}(\ell_h),
       \hat{L}(\ell_h)+\frac{c_{M,\vartheta,\zeta, k}\xi^2(K) \log^3 n}{n}+\frac{c_{\delta}}{n}
     \right\},
\end{align*}
where \begin{align*}
  \xi(K)=
  \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &\text{if } p^\ast\geq 2\log K,\\
      &(2p^\ast)^{1+\frac{1}{p^\ast}}K^{\frac{1}{p^\ast}}, &\text{otherwise},
      \end{aligned}
      \right.
  \end{align*}
  $c_{M,\vartheta}$ is a constant depending on $M,\vartheta,\zeta, k$,
  and $c_{\delta}$ is a constant depending on $\delta$.
\end{theorem}
\begin{proof}
From Theorem 2 (see in Appendix B),
with probability $1-\delta$,
we have
\begin{align}
\label{eq-middle-leel}
   L(\ell_h)&\leq \max\left\{
        \frac{k}{k-1}\hat{L}(\ell_h),
       \hat{L}(\ell_h)+c_Mr^\ast+\frac{c_\delta}{n}
     \right\},
\end{align}
where $r^\ast$ is a fixed point of $\mathcal{R}(\mathcal{L}^r)$.
%that is $r^\ast=\mathcal{R}(\mathcal{L}^{r^\ast})$.
From Lemma 5 (see  Appendix C),
we know that the $\mathcal{R}(\mathcal{L}^r)$ is a sub-root function,
so the fixed point $r^\ast$ of $\mathcal{R}(\mathcal{L}^r)$ is uniquely exists.

According to  Theorem \ref{rademacherlocal}, we know that
\begin{align*}
    \mathcal{R}(\mathcal{L}^r) \leq \frac{c_{M,\vartheta}\xi(K)\sqrt{\zeta r}\log^{\frac{3}{2}}(n)}{\sqrt{n}}+\frac{4\log(1/\delta)}{n}.
  \end{align*}
Thus, if we set $A=\frac{c_{M,\vartheta}\xi(K)\sqrt{\zeta}\log^{\frac{3}{2}}(n)}{\sqrt{n}}$, $B=\frac{4\log(1/\delta)}{n}$,
the fixed point $r^\ast$ is smaller than the solution of
  $
     A\sqrt{r}+B=r,
  $
   which is
   \begin{align*}
     r^s&=\frac{2B+A^2+\sqrt{(2B+A^2)^2-4B^2}}{2}\\
     &\leq 2B+A^2
     =\frac{c_{M,\vartheta}^2\xi^2(K)\zeta\log^3(n)}{n}+\frac{4\log(1/\delta)}{n}.
   \end{align*}
   Substituting the above inequality into \eqref{eq-middle-leel} finishes the proof.
\end{proof}
The order of the  generalization bound in Theorem \ref{theorem-finally} is
$
 \mathcal{O}\left(\frac{\xi^2(K)}{n}\right).
$
From the definition of $\xi(K)$, we can obtain that
 \begin{align*}
  \mathcal{O}\left(\frac{\xi^2(K)}{n}\right)=
  \left\{
      \begin{aligned}
      &\mathcal{O}\left(\frac{(\log K)^{2+{1}/{\log K}}}{n}\right), \text{if } p^\ast\geq 2\log K,\\
      &\mathcal{O}\left(\frac{K^{2/p^\ast}}{n}\right), \text{if } 2\leq p^\ast< 2\log K.
      \end{aligned}
      \right.
  \end{align*}
  Note that our bounds is linear dependence on the reciprocal of sample size $n$,
  while for the existing data-dependent bounds are all radical dependence.
 % which is substantially faster than the dependence on $\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)$ established in the existing
%  work for multi-class classification.
Furthermore,
  our bounds enjoy a mild dependence on the number of classes.
  The dependence is polynomial with degree $2/p^\ast$ for $2\leq p^\ast\leq 2\log K$ and
  becomes logarithmic if $p^\ast\leq 2\log K$,
  which is substantially milder than the quadratic dependence
  established in \cite{koltchinskii2002empirical,koltchinskii2001some,mohri2012foundations,cortes2013multi}.

%To the best of our knowledge,
%the generalization bound  of multi-class classification
%with  linear dependence on reciprocal on the size of sample and logarithmic dependence on the class size
%have never given before.

\subsection{Comparison with Related Work}
In this section, we will compare our bound with three popular bounds of multi-class classification:
 Rademacher complexity, covering number and VC-dimension Bounds.

\textbf{Rademacher Complexity Bounds}
Currently Rademacher complexity are the most powerful tools to get generalization bounds for
multi-class classification.
The important property of Rademacher complexity based bounds is that the
bounds are applicable in arbitrary Banach spaces and do not depend on the dimension of the feature
space directly.

Koltchinskii and Panchenko \yrcite{koltchinskii2002empirical}
and Koltchinskii, Panchenko, and Lozano \yrcite{koltchinskii2001some}
introduce a margin-based bound for multi-class classification in terms of Rademacher complexities:
\begin{align*}
  L(h)\leq \inf_{0<\gamma<1}\hat{L}(h_\gamma)+\mathcal{O}\left(\frac{K^2}{\gamma\sqrt{n}}+\frac{\log1/\delta}{\sqrt{n}}\right).
\end{align*}
The order is $\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$,
which is slightly improved (by a constant factor prior to the Rademacher complexity term)
by \cite{mohri2012foundations,cortes2013multi}.
Maximov and Reshetova \yrcite{maximov2016tight} give a new Rademacher complexity bound:
\begin{align*}
  L(h)\leq \inf_{0<\gamma<1}\hat{L}(h_\gamma)+\mathcal{O}
  \left(\frac{K}{\gamma\sqrt{n}}+\frac{\log1/\delta}{\sqrt{n}}\right),
\end{align*}
which has the form of $\mathcal{O}\big(\frac{K}{\sqrt{n}}\big)$.
% give a new Rademacher complexity based
%bound depends linearly in the number of classes of $\mathcal{O}\big(\frac{k}{\sqrt{n}}\big)$.
%The bound involves the marginal distribution
%of the classifier and the Rademacher complexity of the hypothesis class
Based on the $\ell_p$-norm regularization,
Lei, Binder, and Klof \yrcite{lei2015multi} derive a new bound:
\begin{align*}
   L(h)\leq  \hat{L}(\ell_h) +\mathcal{O}\left(\frac{\log^2 K}{\sqrt{n}}\right).
\end{align*}
The existing bounds based on Rademacher complexity are all radical dependence
on the reciprocal of sample size.
%of the form $\mathcal{O}\big(\frac{\log^2 K}{\sqrt{n}}\big)$
%for some value of $p$.

In this paper,
we derive a sharper bound based on the local Rademacher complexity with order
$
\mathcal{O}\Big(\frac{(\log K)^{2+\frac{1}{\log K}}}{n}\Big),
$
which is substantially sharper
than the existing bounds of  Rademacher complexity.

\textbf{Covering Number Bounds}
Based on the $\ell_\infty$-norm covering number bound of
linear operators, Guermeur \yrcite{guermeur2002combining} obtain
a generalization of form $\mathcal{O}\big(\frac{K}{\sqrt{n}}\big)$,
which is improved by \cite{zhang2004statistical} to a radical dependence:
  \begin{align*}
   L(h)\leq  \hat{L}(\ell_h) +\mathcal{O}\left(\sqrt{\frac{K}{n}}\right).
\end{align*}
Hill and Doucet \yrcite{Hill2007} derive a class-size independent risk guarantee
of form $\mathcal{O}\big(\sqrt{{1}/{n}}\big)$.
However, their bound is based on a delicate definition of margin,
which is not commonly  used in the mainstream multi-class literature.
%Related results for metric spaces with low doubling dimension were obtained by \cite{Kontorovich2014},
%who used nearest neighbors method to improve the dependence on the number
%of classes in favor of (doubling) dimension dependence.

\textbf{VC-dimension Bounds}
VC-dimension is a important tool to derive the generalization bound for binary classification.
Allwein, Schapire, and Singer \yrcite{allwein2000reducing} show how to use it
for multi-class learning problems,
and derive a VC-dimension based bounds:
\begin{align*}
   L(h)\leq  \hat{L}(h_\gamma) +\mathcal{O}\left(\frac{\sqrt{V}\log K}{\sqrt{n}}\right),
\end{align*}
where $V$ is the VC-dimension.
Natarajan dimension is introduced in \cite{Natarajan1989} in
order to characterize multi-class PAC learnability.
%It exactly matches the notion of Vapnik-Chervonenkis
%dimension in the case of two classes.
Daniely and Shalev-Shwartz \yrcite{daniely2014optimal} derive a generalization bound with Natarajan dimension:
\begin{align*}
   L(h)\leq  \hat{L}(h_\gamma) +\mathcal{O}\left(\frac{d_{Nat}}{n}\right),
\end{align*}
where $d_{Nat}$ is the Natarajan dimension.
Note that VC dimension bounds as well as Natarajan dimension bounds are usually dimension
dependent, which makes them hardly applicable for practical large
scale problems (such as typical computer vision problems).

The above theoretical analysis indicates that it is a good
choice to use the local Rademacher complexity to analyze the generalization
ability of multi-class classification.

\section{Multi-Class Multiple Kernel Learning}
Motivated by the above analysis of generalization bound and convergence rate for multi-class classification, we will exploit properties of the local Rademacher complexity
to devise two algorithms for multi-class Multiple Kernel Learning (MC-MKL).
\subsection{Motivation and anlysis}
As we know, there is a standard way to learn binary classification problems,
in which the prediction of $\mathbf x$ is the scalar product between an hyperplane $\mathbf w$ and the transformed sample $\phi(\mathbf x)$,
corresponding kernel $k(\mathbf x, \mathbf x')$ defined as $\phi(\mathbf x) \cdot \phi(\mathbf x')$.
For multiple kernels, $k_\mu=\sum_{m=1}^M \mu_mk_m$ is a non-negative linear combination of $M$ kernels \cite{BachLJ04}.
A common approach to multiclass classification is the use of joint feature
maps $\phi(\mathbf x, y):\mathcal X \times \mathcal Y \to \mathcal H$ \cite{TsochantaridisHJA04}.
For kernels learning, we have $M$ feature mappings $\phi_m(\cdot,\cdot)$,
$m=1,\ldots,M$ and $M$ kernels
$$k_m((\mathbf x, y),(\mathbf x', y'))=\phi_m(\mathbf x, y) \cdot \phi_m(\mathbf x', y').$$
For each class, the above definition give $M$ different hyperplanes, respectively. In fact, $\phi_m(\mathbf x, y)$ can be defined as
\begin{align*}
   \phi_m(\mathbf x, y)=[\mathbf 0,\ldots, \mathbf 0,\underbrace{\phi'_m(\mathbf x)}_{y},\mathbf 0,\ldots,\mathbf 0],
\end{align*}
where $\phi_m(\cdot, \cdot)$ contains $K$ vectors in $\mathcal{H}$ space, only the $y$-th vector not being zero.
And $\phi'_m(\cdot)$ is a feature mapping that only depends on data.
According to the defined notation, $\phi(\mathbf x, y)=[\phi_1(\mathbf x, y),\ldots, \phi_M(\mathbf x, y)]$.
So, the $\ell_{2,p}$-norm of $\mathbf{w}$ can be defined as $\|\mathbf w\|_{2,p}=\|[\|\mathbf{w}_1\|_2,\|\mathbf{w}_2\|_2,\ldots,\|\mathbf{w}_M\|_2]\|_p$. The dual norm of $\|\cdot\|_{2,p}$ is $\|\cdot\|_{2,q}$, where $1/p+1/q=1$ \cite{kakade2009duality}.

Using multiple kernels, we can modify the $\mathcal{H}_{p,\kappa}$ as
 \begin{align*}
 \label{hypothspapcemkl}
   \begin{aligned}
   \mathcal{H}_{mkl}=\Big\{h_{\mathbf{w},k_\mu}&=\left(\langle \mathbf w_1,\phi(\mathbf x,1)\rangle,\ldots, \langle\mathbf w_K,\phi(\mathbf x, K)\rangle\right),\\
   \left\|\mathbf  w \right\|_{2,p}&\leq 1, 1\leq p\leq 2
  \Big\}.
   \end{aligned}
 \end{align*}
 Note that the tail sum is the difference between the trace and
 the $\theta$ largest eigenvalues:$\sum_{j>\theta}\lambda_j(\mathbf{K})=\mathrm{Tr}(\mathbf{K})-\sum_{j=1}^\theta\lambda_j(\mathbf{K})$,
 thus the tail sum can be calculated in $O(n^2\theta)$ for each kernel.
\subsection{COV-MKL}
The trace of kernel matrix $\mathbf{K}_\mu$ can be the upper-bound of the global Rademacher complexity of $\mathcal{H}_{mkl}$.
The trace of kernel matrix $\mathbf{K}_\mu$ can be the upper-bound of the global Rademacher complexity of $\mathcal{H}_{mkl}$. Existing works on learning kernels \cite{LanckrietCBGJ02,BachLJ04,SonnenburgRSS06} use the following constraint to $\mathcal{H}_{mkl}$:
$\mathrm{Tr}(\mathbf{K}_\mu) \leq 1.$
According the above theoretical analysis, the local Rademancher complexity(the tail sum of the eigenvalues of the kernel) lead to tighter generalization bounds than the global Rademancher complexity(the trace). Thus, we add the local Rademancher complexity to restrict $\mathcal{H}_{mkl}$:
\begin{align}
    \mathcal{H}_{1}=\Big\{f_{\mathbf{w}, k_\mu} \in \mathcal{H}_{mkl}:\sum_{j > \theta} \lambda(\mathbf{K}_\mu) \leq 1\Big\},
\end{align}
where $\mathbf{K}_\mu=\sum_{m=1}^M \mu_m \mathbf{K}_m$,
$\theta$ is free parameter removing the $\theta$ largest eigenvalues to control the tail sum.
Note that $\mathcal{H}_{1}$ is not convex, and we know that:
\begin{align*}
    \begin{aligned}
        \sum_{m=1}^M\mu_m\sum_{j>\theta}\lambda_{j}(\mathbf{K}_m) & =\sum_{m=1}^M\mu_m/\|\mathbf{\mu}\|_1 \sum_{j>\theta}\lambda_{j}(\|\mathbf{\mu}\|_1\mathbf{K}_m)\\
        & \leq \sum_{j>\theta} \lambda_j\big(\mathbf{K}_\mu\big).
    \end{aligned}
\end{align*}
Thus, we consider the use of the convex $\mathcal{H}_2$ replace of $\mathcal{H}_1$:
\begin{align}
    \mathcal{H}_{2}=\Big\{f_{\mathbf{w}, k_\mu} \in \mathcal{H}_{mkl}:
    \sum_{m=1}^M\mu_m\sum_{j>\theta}\lambda_{j}(\mathbf{K}_m) \leq 1\Big\}.
\end{align}
Based on the convex class $\mathcal{H}_2$, we can design a simpler algorithm in which precomputed kernel matrices
are regularized by the local Rademancher complexity respectively seen in Algorithm  \ref{algorithm1}:
\begin{algorithm}[h]
   \caption{Conv-MKL}
   \label{algorithm1}
    \begin{algorithmic}
       \STATE {\bfseries Input:} precomputed kernel matrices $\mathbf{K}_1,\ldots,\mathbf{K}_M$ and $\theta$
       \FOR{$i=1$ {\bfseries to} $M$}
        \STATE Compute tail sum: $t_m=\sum_{j>\theta}\mathbf{K}_m$
        \STATE Normalize precomputed kernel matrix: $\widetilde{\mathbf{K}}_m=\mathbf{K}_m/t_m$
       \ENDFOR
       %\STATE Compute the minimizer of ERM over $\mathcal{H}_2$:%
       \STATE Use $\widetilde{\mathbf{K}}=\sum_{m=1}^M \mu_m \widetilde{\mathbf{K}}_m$ in any $\ell_p$-norm MKL solver
    \end{algorithmic}
\end{algorithm}


\subsection{MG-MKL}
According to local Rademancher analysis in Theorem \ref{theorem-finally},
the local Rademancher complexity leads to a shaper generalization bound.
Thus, we consider the following optimization:
\begin{equation}
\label{optimization-problem}
    \min_{\mathbf{w},\bm \mu} \frac{\alpha}{2}\|\mathbf{w}\|_{2,p}^2
    +\frac{1}{n}\sum_{i=1}^n\ell(\mathbf{w},\phi(\mathbf{x}_i,\cdot), y_i)
    +\beta\sum_{m=1}^M\mathbf{\mu}_m\mathbf{r}_m
\end{equation}
where
$\mathbf{r}_m=\sum_{j>\theta}\lambda_j(\mathbf{K}_m), m=1,\ldots, M,$
$\mathbf{r}_m$ is the tail sum of the $m$-th kernel matrix,
and the loss funtion
$$\ell(\mathbf{w},\phi(\mathbf{x}_t,\cdot), y_t)=
\max\limits_{y \not= u}|1-\mathbf{w} \cdot(\phi(\mathbf{x}_t,y_t)-\phi(\mathbf{x}_t,u))|_+.$$
In the algorithm, we cope with the non-differentiability of the hinge loss directly by sub-gradients instead of gradients.

\begin{algorithm}[h]
   \caption{SG-MKL}
   \label{algorithm2}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $\alpha, \beta, \theta, \mathbf{\mu}, T$
       \STATE {\bfseries Initialize:} $\mathbf{w}_1=\mathbf{0}, \mathbf{\zeta}_1=\mathbf{0},q=2\log M $
       \FOR{$t=1$ {\bfseries to} $T$}
       \STATE Sample at random $(\mathbf{x}^t, y^t)$
       \STATE Compute the dual weight: $\mathbf{\zeta}^{t+1}=\mathbf{\zeta}^t-\partial{\ell(\xi^t)}$
       \STATE $\mathbf{v}_m=\|\mathbf{\zeta}_m^{t+1}\|-t\beta\mathbf{r}_m$, $\forall m=1,\ldots, M.$
       \STATE $\mathbf{w}^{t+1}_m=\frac{\text{sgn}(\mathbf{v}_m) \mathbf{\zeta}_m^{t+1}}{t\alpha\|\mathbf{\zeta}_m^{t+1}\|}
       \frac{|\mathbf{v}_m|^{q-1}}{\|\mathbf{v}\|_q^{q-2}}$, $\forall m=1,\ldots, M.$
       \STATE $\mu_m=\frac{\mathbf{w}^{t+1}_m}{\mathbf{\zeta}^{t+1}_m}$, $\forall m=1,\ldots, M.$
       \ENDFOR
    \end{algorithmic}
\end{algorithm}
Based on a primal-dual framework for the minimization \cite{Shalev-ShwartzK08, Xiao10,OrabonaL11},
we design a simple stochastic sub-gradient descent algorithm, called SG-MKL, to minimize \eqref{optimization-problem}.
As shown in the algorithm \ref{algorithm2}, there are two weights, a primal one $\mathbf{w}$ and a dual one $\mathbf{\zeta}$, to maintain.
Then we derive the algorithm in following:
\begin{itemize}
\item[(1)]The algorithm initializes $q=2\log M$ and $M>2$, thus $p=1/(1-1/q) \in (1,2)$.
\item[(2)] For every step, the algorithm takes a random sample from train data to update
    the dual vector $\mathbf{\zeta}^t$ in a subgradient descent step, where $\partial{\ell(\xi^t)}$ is the subgradient to $\mathbf{w}$.
    Then, the algorithm uses the regularizer and the local Rademancher complexity
    part to update primal weights $\mathbf{w}$ and kernel coefficient $\mathbf{\mu}$.
    The calculation of $\mathbf{v}_j$ can make the weight be zero when the norm is less than $\alpha t$,
    causing sparsity of the solution.
\item[(3)] Let $\Omega(\mathbf{w})=\frac{\alpha}{2}\|\mathbf{w}\|_{2,p}^2 + \beta\mathbf{\mu} \cdot \mathbf{r}$.
            Theorem \ref{theorem-fenchel-dual} gives solution of the gradient of the Fenchel dual of $\Omega$.
            Actually, the algorithm updates real numbers $\|\mathbf{\zeta}_m^{t+1}\|_2$, $\mathbf{v}_m$
            and $\mathbf{\mu}_m$ in scalar products instand of updating high-dimensional variables $\mathbf{w}$ and $\mathbf{\zeta}$.
 \item[(4)] The first real number $\|\mathbf{\zeta}_m^{t+1}\|_2$ can be calculated in an efficient incremental way as following:
            \begin{align*}
                \|\mathbf{\zeta}_m^{t+1}\|
                =\|\mathbf{\zeta}_m^{t}-z^t\|_2^2
                =\|\mathbf{\zeta}_m^{t}\|_2^2-2\mathbf{\zeta}_m^t \cdot z^t +\|z^t\|_2^2
            \end{align*}
            where $z^t=\partial\ell(\mathbf{w}^t,\phi(\mathbf{x}_t,\cdot), y_t)$.
 \end{itemize}

 The following theorem gives calculation of $\nabla\Omega^\ast(\mathbf{\zeta})$.
 \begin{theorem}
\label{theorem-fenchel-dual}
Let
$
    \mathbf{v}=\Big[\|\mathbf{\zeta}_1\|-\beta\mathbf{r}_1,\ldots,
    \|\mathbf{\zeta}_M\|-\beta\mathbf{r}_M\Big],
$
then the component $j$ of $\;\nabla\Omega^\ast(\mathbf{\zeta})$ is
\begin{align*}
\frac{\mathrm{sgn}(\mathbf{v}_m) \mathbf{\zeta}_m}{\alpha\|\mathbf{\zeta}_m\|}
       \frac{|\mathbf{v}_m|^{q-1}}{\|\mathbf{v}\|_q^{q-2}},
\end{align*}
where $\mathrm{sgn}(x)$ is defined as
$sgn(x)=1$, if $x>0$, $sgn(x)=-1$, if $x<0$,
$sgn(x)\in[-1,+1]$, if $x=0$.
\end{theorem}

\begin{proof}
According to standard Legendre-Fenchel duality, we can get
\begin{align}
\nabla\Omega^*(\mathbf{\zeta})=\argmax\limits_{\mathbf{w}} \mathbf{w} \cdot \mathbf{\zeta} - \Omega(\mathbf{w}).
\end{align}
Obviously, the solution of above argmax is the derivative being zero, thus $\mathbf{w}_m$ must be proportional to $\mathbf{\zeta}_m$, that is $\mathbf{w}_m=c_m\mathbf{\zeta}_m/\|\mathbf{\zeta}_m\|$, in which $c_m$ is real number and $\mathbf{\mu}_m=c_m/\|\mathbf{\zeta}_m\|$ can be the coefficient of multiple kernels. Then, we know $\|\mathbf{w}_m\|_2=\mathbf{c}_m/\|\mathbf{\zeta}_m\|_2$ rewrite the problem on $\mathbf{c}_m$:
\begin{align}
    \label{lp-regularization}
    \argmin \limits_{\mathbf{c}} (\beta\mathbf{r} - \mathbf{a})\cdot\mathbf{c}+\frac{\alpha}{2}\|\mathbf{c}\|_p^2
\end{align}
where $\mathbf{a}=\Big[\|\mathbf{\zeta}_1\|,\ldots, \|\mathbf{\zeta}_M\|\Big]$.

To get optimal solution of $\ell_p$-regularization in \eqref{lp-regularization}, we need to solve $\ell_2$-regularization problem firstly, according to Lemma 6 and Lemma 7 in supplementary file.
\begin{align}
 \label{l2-regularization}
    \argmin \limits_{\mathbf{c}} (\beta\mathbf{r} - \mathbf{a})\cdot\mathbf{c}+\frac{\alpha}{2}\|\mathbf{c}\|_2^2
\end{align}
The optimality condition of the above minimization problem\cite{rockafellar2015convex} states that $\mathbf{c}^\ast$ is an optimal solution of \eqref{l2-regularization} such that
\begin{align}
    \label{l2-regularization-subgradient}
        \beta\mathbf{r}-\mathbf{a}+\alpha\upsilon(\mathbf{c})=0
\end{align}
Following similar arguments as in Lemma 6 (seen in supplementary), we find that it has a closed-form solution
$\mathbf{c}=\upsilon ^{-1}(\mathbf{c}^\ast)$,
where the elements of $\mathbf{c}^\ast$ are given as
$
    \mathbf{c}_m^\ast=\frac{1}{\alpha}(\mathbf{a}_m-\beta\mathbf{r}_m)=\frac{1}{\alpha}(\|\mathbf{\zeta}_m\|-\beta\mathbf{r}_m).
$
Following Lemma 7 (seen in supplementary), the component $\mathbf{c}_m$ of $\mathbf{c}$ can be given by the inverse mapping $\upsilon ^{-1}(\mathbf{c}^\ast)$
\begin{align*}
    \mathbf{c}_m=&\upsilon ^{-1}(\mathbf{c}_m^\ast)=\nabla_m\Big(\frac{1}{2}\|\mathbf{c}_m^\ast\|_q^2\Big)
    =\frac{sgn(\mathbf{c}_m^\ast)|\mathbf{c}_m^\ast|^{q-1}}{\|\mathbf{c}^\ast\|_q^{q-2}}.
\end{align*}
Using $\mathbf{c}_m$ and $\mathbf{w}_m=\mathbf{c}_m\mathbf{\zeta}_m/\|\mathbf{\zeta}_m\|$, we have the stated result.
Similar argmax has been analysis in Sec. 7.2 of \cite{Xiao10}.
\end{proof}
\subsection{Convergence Rate Guarantee}
In this section, we analysis convergence rate guarantee for SD-MKL.
%In multi-class classification, there is one hyperplane for each class
%and $\phi_m(\cdot, \cdot)$ induces the transformation in $m$-th class.
Denote by $z^t=\partial\ell(\mathbf{w},\phi(\mathbf{x}^t,\cdot), y^t)$,
we now state the convergence therem for any loss function that satisfies the following hypothesis
\begin{align}
\label{loss_function_hypothesis}
    \|z_m\| \leq L\|\phi(\mathbf{w}_m, y')\|_2, \forall t =1,\ldots, M, y' \in \mathcal{Y}.
\end{align}
The hinge loss for multi-class $\ell(\mathbf{w}, \phi(\mathbf{w},\cdot), y)
= \max_{y' \not =y} |1-\mathbf{w}\cdot(\phi(\mathbf{w},y)-\phi(\mathbf{x},y'))|_+$ satisfies the hypothesis with $L=\sqrt{2}$.

\begin{theorem}
Denote by $$f(\mathbf{w})=\Omega(\mathbf{w})+\frac{1}{n}\sum_{i=1}^n\ell(\mathbf{w},\phi(\mathbf{x}_i,\cdot),y_i)$$
and by $\mathbf{w}^\ast$ the solution that minimize \eqref{optimization-problem}.
Suppose that $\|\phi_m(\mathbf{w}^t,\cdot)\|_2 \leq 1$, and the loss function $\ell$ satisfies $\eqref{loss_function_hypothesis}$.
Let $\delta \in (0,1)$, then with probability at least $1 - \delta$ over the choices of the random samples
we have that after $T$ iterations of the SG-MKL algorithm
\begin{align*}
    f(\mathbf{w}^{T+1})-f(\mathbf{w^\ast}) \leq \frac{eL^2(1+\log T)\log F}{\alpha\delta T},
\end{align*}
where $e$ is the Euler's number.
\end{theorem}
\begin{proof}
Using \eqref{loss_function_hypothesis} and $\|\phi_m(\mathbf{w}^t, y^t)\|_2 \leq 1$, we have
\begin{align*}
    &\|\partial(\mathbf{w}^t,\phi(\mathbf{x}^t,\cdot), y^t)\|_{2,q} \\
    &\leq LF^{1/q} \max_{j=1, \ldots, F} \|\phi_m(\mathbf{x}^t, \cdot)\|_2 \leq LF^{1/q}
\end{align*}
The function $\Omega(\mathbf{w})=\frac{\alpha}{2}\|\mathbf{w}\|_{2,p}^2 + \beta\mathbf{\mu} \cdot \mathbf{r}$ is
 $\alpha$-strongly convex w.r.t. the norm $\|\cdot\|_{2,q}$. Hence, using according to Theorem 1 in \cite{Shalev-ShwartzSS07},
 using $\eta=1$ and $g=\Omega$, and using Markov inequality as in \cite{Shalev-ShwartzSS07} we prove the stated result.
\end{proof}



\section{Experiments}

\begin{table*}[t]
\small
\footnotesize
%\scriptsize
%\tiny
%\renewcommand{\captionfont}{\small}
   \caption{
    \small Comparison of average test accuracies of our \texttt{conv} and \texttt{SG-MKL} with the others including
    the linear multi-class SVM (LSVM),
    One-against-One (One vs. One),
    One-against-the-Rest(One vs. Rest),
    Generalized Minimal Norm Problem solver (GMNP),
    the $\ell_1$-norm Multiclass MKL ($\ell_1$ MC-MKL),
    the $\ell_2$-norm Multiclass MKL ($\ell_2$ MC-MKL),
    and mixed-norm MKL solved by stochastic gradient descent (UFO-MKL).
    We bold the numbers of the best method, and underline the numbers of the other methods
    which are not significantly worse than the best one.
   %We set the numbers of our method (EPSVM) to be bold if our method outperforms all other methods (KTA, CKTA, FSM, 3-CV, 5-CV and 10-CV).
   }
   \label{tabel:accuracy}
    %\centering
    \begin{tabular*}{\linewidth}{@{\extracolsep{-0.23cm}}lccccccccc}
    \toprule
                   & SG-MKL              & Conv-MKL               & C\&S               & One vs. One              & One vs. Rest                                              & GMNP                      & $\ell_1$ MC-MKL          & $\ell_2$ MC-MKL    & UFO-MKL                  \\ \hline
plant              & \textbf{78.01$\pm$2.17}   &77.14$\pm$2.25            & 70.12$\pm$2.96     & 75.83$\pm$2.69           &75.17$\pm$2.68       &75.42$\pm$3.64    & \underline{77.60$\pm$2.63}&75.49$\pm$2.48            &76.77$\pm$2.42\\
psortPos           & \textbf{76.23$\pm$3.39}   &74.74$\pm$3.35            &63.85$\pm$39.4      &73.33$\pm$4.21            &71.70$\pm$4.89       & 73.55$\pm$4.22   &71.87$\pm$4.87             &70.70$\pm$4.89            &74.56$\pm$4.04\\
psortNeg           & \textbf{74.66$\pm$1.90}   &74.07$\pm$2.16            &57.85$\pm$2.49      &73.74$\pm$2.87            &71.94$\pm$2.50   &\underline{74.27$\pm$2.51}   &72.83$\pm$2.20                             &72.42$\pm$2.65      &73.80$\pm$2.26 \\
nonpl              &78.69$\pm$1.58             & \textbf{79.15$\pm$1.51}  &75.16$\pm$1.48      &77.78$\pm$1.52            &77.49$\pm$1.53 &78.35$\pm$1.46    &00.00$\pm$0.00             &77.95$\pm$1.64            &78.07$\pm$1.56\\
sector             & \textbf{93.39$\pm$0.70}   &\underline{92.83$\pm$2.62}&93.16$\pm$0.66      &90.61$\pm$0.69            &\underline{93.34$\pm$0.61}                    &00.00$\pm$0.00    &00.00$\pm$0.00             &92.15$\pm$2.57            &92.60$\pm$0.47\\
segment            & \textbf{97.62$\pm$0.83}   &96.80$\pm$0.91            &94.92$\pm$1.10      &97.08$\pm$0.61            &96.29$\pm$0.84       &96.87$\pm$0.80    &96.98$\pm$0.80             &\underline{97.58$\pm$0.68}&97.20$\pm$0.82\\
vehicle            & 77.28$\pm$2.78            &79.35$\pm$2.27            &75.61$\pm$3.56      &82.72$\pm$1.92            &81.24$\pm$2.21       &81.57$\pm$2.24    &74.96$\pm$2.93             &76.27$\pm$3.15            &76.92$\pm$2.83\\
vowel              &\textbf{98.83$\pm$5.57}    &98.82$\pm$1.19            &62.23$\pm$3.00      &97.05$\pm$1.72            &91.65$\pm$3.26       &88.08$\pm$3.03    &98.27$\pm$1.22             &97.86$\pm$1.75            &98.22$\pm$1.62\\
wine               &\textbf{99.52$\pm$1.08}    &99.44$\pm$1.13            &97.87$\pm$2.80      &97.24$\pm$3.04            &98.14$\pm$3.04       &97.69$\pm$2.43    &98.61$\pm$1.75             &98.52$\pm$1.89            &99.52$\pm$1.08            \\
dna                &\textbf{96.30$\pm$0.79}    &91.13$\pm$1.22            &92.02$\pm$1.50      &95.89$\pm$0.56            &95.93$\pm$0.74       &64.77$\pm$2.02    &96.27$\pm$0.68             &95.06$\pm$0.92            &95.84$\pm$0.61\\
glass              & 73.65$\pm$5.83            &\textbf{75.19$\pm$5.05}   &63.95$\pm$6.04      &70.87$\pm$5.58            &70.23$\pm$6.24       &71.24$\pm$8.14    &69.07$\pm$8.08             &74.03$\pm$6.41            &72.46$\pm$6.12\\
iris               &97.33$\pm$2.53             &\textbf{98.00$\pm$2.25}            &87.89$\pm$4.33      &96.76$\pm$2.63            &96.11$\pm$3.04    &96.11$\pm$4.11    &96.00$\pm$3.65             &95.44$\pm$3.66            &95.56$\pm$3.07\\
svmguide2          &\textbf{85.17$\pm$3.83}    &82.69$\pm$5.65            &82.15$\pm$3.41      &82.01$\pm$3.29            &82.31$\pm$3.07       &82.15$\pm$3.95    &86.16$\pm$3.63             &83.84$\pm$4.21            &82.91$\pm$3.09\\
satimage           &\textbf{91.78$\pm$0.82}    &90.86$\pm$1.07            &84.95$\pm$1.15      &90.67$\pm$0.91            &89.29$\pm$0.96       &89.97$\pm$0.81    &00.00$\pm$00.00            &90.43$\pm$1.27            &91.92$\pm$0.83\\
\bottomrule
\end{tabular*}
\vspace{-0.3cm}
\end{table*}

In this section, we will  compare our proposed \texttt{Conv-MKL} (Algorithm \ref{algorithm1}) and \texttt{SG-MKL} (\ref{algorithm2})
with 7 popular multi-class classification methods:
 One-against-One \cite{knerr1990single}, One-against-the-Rest \cite{bottou1994comparison},
$\ell_1$-norm linear multi-class SVM (LMC) \cite{CrammerS02},
Generalized Minimal Norm Problem solver (GMNP)\cite{franc2005optimization},
the Multiclass MKL (MC-MKL) for $\ell_1$-norm and $\ell_2$-norm \cite{ZienO2007}
 the mixed-norm MKL solved by stochastic gradient descent(UFO-MKL)\cite{OrabonaL11}.
Actually, we use LIBSVM\footnote{Available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/}
(implementing One-against-One and One-against-the-Rest implemented),
the DOGMA libary \footnote{Available at http://dogma. sourceforge. net} \cite{orabona2009dogma} (implementing C\&S, GMNP,
$\ell_1$-nomr and $\ell_2$-norm MC-MKL) and the SHOGUN-6.1.3\footnote{Available at http://www.shogun-toolbox.org/}(implementing UFO-MKL).

We experiment on 14 publicly avaliable datasets:
four of them evaluated in \cite{ZienO2007} (plant, nonpl, psortPos, and psortNeg)
\footnote{Available at http://www.raetschlab.org/suppl/protsubloc}
and the others from LIBSVM Data \footnote{Available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/}.
For each data set, we use the Gaussian kernel
$K(\mathbf{x}, \mathbf{x}')=\exp\Big(-\frac{\|\mathbf{x}-\mathbf{x}'\|_2^2}{2\tau}\Big)$ as our basic kernels,
where $\tau \in {2^i, i=-10,-9,\ldots,9,10}$. For single Kernel methods (One vs. One, One vs. Rest, GMNP),
we choose one kernel which have the highest performance among basic kernels by cross-validation.
Meanwhile, use all basic kernels for MKL methods (SG-MKL, conv, $\ell_1$ MC-MKL, $\ell_2$ MC-MKL, UFO-MKL).
For our algorithms \texttt{conv} and \texttt{SG-MKL}, we fix the parameter $\theta=2$ to curve the local Rademancher complexity.
The regularization parameterized $C \in {2^i, i=-2, \ldots, 12}$ of all algorithms,
the parameter $\gamma \in {10^i, i=-4, \ldots, 1}$ of \texttt{SG-MKL} are determined by 5-folds cross-validation on training data.
The other parameters for the compared algorithms follow the same experimental setting in their papers.
For each data set, we run all methods 50 times with randomly selected 80\% for training and 20\% for testing.
The use of multiple training/testing partitions allows an estimate of the statistical significance of differences
in performance between methods. Let $A_i$ and $B_i$ be the test errors of methods A and B in partition $i$,
and $d_i=B_i-A_i, i=1,\ldots,30$. Let $\bar{d}$ and $S_d$ be the mean and standard error of $d_i$.
Then under $t$-test, with confidence level 95\%, we claim that A is significantly better than B (or equivalently B significantly worse than A)
if the $t$-statistic $\frac{\bar{d}}{S_d/\sqrt{50}} > 1.676$.
All statement of statistical significance in the remainder refer to a 95\% level of significance.

The average test accuracies are reported in Table \ref{tabel:accuracy}.
The results show: 1) Our \texttt{SG-MKL} and \texttt{Conv-MKL} methods lead to a consistent improvement over all data sets except \textit{vehicle};
2) Our \texttt{SG-MKL} and \texttt{Conv-MKL} methods is significantly
better than the compared Multiple Kernel Learning methods ($\ell_1$ MC-MKL,
$\ell_2$ MC-MKL and UFO-MKL) over all data sets;
3) \texttt{SG-MKL}  is significantly better than state-of-the-art \texttt{conv} on most data sets;
4) The multiple kernel methods usually better than the compared single kernel methods (One vs. One, One vs. Rest, GMNP);
5) The kernel based methods usually have better performance than linear classification machine (LMKL).

\section{Conclusion}
In this paper, we studied the generalization performance of the multi-class classification,
and derived a sharper data dependent generalization error bound based on the notion of local Rademacher complexity,
which is much sharper than the existing data-dependent generalization bound of multiclass classification.
Then, we design a two algorithms: 1) \texttt{conv}.
Using precomputed kernel matrices regularized by the local Rademancher complexity,
this method can be implemented by any $\ell_p$-norm multi-class MKL solvers.
2) \texttt{SG-MKL}. This method puts the local Rademancher complexity
in penalized ERM with $\ell_{2,p}$-norm regularizer and multi-class hinge loss function,
implemented by stochastic sub-gradient descent by updating
dual weights with statistical guarantees and fast convergence rates.
Empirical results show our algorithms outperform the state-of-the-art multiclass classification methods.
\bibliography{MC_MKL_LR}
\bibliographystyle{icml2018}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
