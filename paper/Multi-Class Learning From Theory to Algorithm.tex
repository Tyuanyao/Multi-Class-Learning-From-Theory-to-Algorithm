\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

%\usepackage[final]{nips_2018}
% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2018}
\usepackage{algorithm}
\usepackage{algorithmic}
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,amsfonts}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\usepackage[small]{caption}
%\usepackage[noend]{algpseudocode}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm
%\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\title{Multi-Class Learning: From Theory to Algorithm}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
Jian Li$^{1,2}$, Yong Liu$^1$\thanks{Corresponding author}~~,  Rong Yin$^{1,2}$, Hua Zhang$^{1}$, Lizhong Ding$^5$, Weiping Wang$^{1,3,4}$\\
%Jian Li$1$, Yong Liu\thanks{Corresponding author}, Rong Yin, Hua Zhang, Lizhong Ding, Weiping Wang\\
$ ^1$Institute of Information Engineering, Chinese Academy of Sciences\\
$ ^2$School of Cyber Security, University of Chinese Academy of Sciences\\
$ ^3$National Engineering Research Center for Information Security\\
$ ^4$National Engineering Laboratory for Information Security Technology\\
$ ^5$Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE\\
\texttt{\{lijian9026,liuyong,yinrong,wangweiping\}@iie.ac.cn}\\
\texttt{lizhong.ding@inceptioniai.org}
  %David S.~Hippocampus\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 \\
%  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
In this paper, we study the generalization performance of multi-class classification and obtain a shaper data-dependent generalization error bound with fast convergence rate, substantially improving the state-of-art bounds in the existing data-dependent generalization analysis. The theoretical analysis motivates us to devise two effective multi-class kernel learning algorithms with statistical guarantees. Experimental results show that our proposed methods can significantly outperform the existing multi-class classification methods.
\end{abstract}
\section{Introduction}
\label{submission}
Multi-class classification is an important problem in various applications,
such as natural language processing, information retrieval, computer vision,
web advertising, etc.
The statistical learning theory of binary classification is by now relatively well developed \cite{Liu2013epks,Liu2014eacvkmbif,Liu2014pcvks,Liu2011learningkernels,mohri2012foundations,vapnik1998naturestatistical},
%similar analysis has been explored in multi-label learning \cite{xu2016local} as well,
but there are still numerous statistical challenges to its multi-class extensions \cite{maximov2016tight}.

To understand the existing multi-class classification algorithms and guide the development of new ones,
people have investigated the generalization ability of multi-class classification algorithms.
%A sharper generalization bound usually implies more consistent
%performance on the training set and the test set.
In recent years, some generalization bounds have been proposed to
estimate the ability of multi-class classification algorithms based on different measures,
such as
VC-dimension \cite{allwein2000reducing},
Natarajan dimension \cite{daniely2014optimal},
covering Number \cite{guermeur2002combining,Hill2007,zhang2004statistical},
Rademacher Complexity \cite{cortes2013multi,koltchinskii2002empirical,mohri2012foundations},
Stability \cite{Hardt2016}, PAC-Bayesian \cite{McAllester2013}, etc.
Although there have been several recent advances in the studying of
generalization bounds of multi-class classification algorithms,
convergence rates of the existing generalization bounds are usually %at least%
 $\mathcal{O}\big({K^2}/{\sqrt{n}}\big)$,
where $K$ and $n$ are the number of classes and size of the sample, respectively.

In this paper, we derive a novel data-dependent generalization bound for multi-class classification
via the notion of local Rademacher complexity and
further devise two effective multi-class kernel learning algorithms based on the above theoretical analysis.
The rate of this bound is $\mathcal{O}\big({(\log K)^{2+{1}/{\log K}}}/{n}\big)$,
which substantially improves on the existing data-dependent generalization bounds.
Moreover, the proposed multi-class kernel learning algorithms have statistical guarantees and fast convergence rates.
Experimental results on lots of benchmark datasets show that our proposed methods can significantly
outperform the existing multi-class classification methods.
The major contributions of this paper include:
 1) A new local Rademacher complexity based bound with fast convergence rate for multi-class classification is established.
   Existing works \cite{kuznetsov2014multi,mohri2012foundations} for multi-class classifiers with Rademacher complexity
   does not take into account couplings among different classes.
   To obtain sharper bound, we introduce a new structural complexity result on function classes induced by general classes via the maximum operator,
   while allowing to preserve the correlations among different components meanwhile.
   Thus, our result in this paper is a non-trivial extension of the binary classification of local Rademacher complexity to multi-classification;
 2) Two novel multi-class classification algorithms are proposed with statistical guarantees: a) \texttt{Conv-MKL}.
Using precomputed kernel matrices regularized by local Rademacher complexity,
this method can be implemented by any $\ell_p$-norm multi-class MKL solvers;
b) \texttt{SMSD-MKL}.
This method puts local Rademacher complexity
in penalized ERM with $\ell_{2,p}$-norm regularizer,
implemented by stochastic sub-gradient descent with updating dual weights.

%The rest of the paper is organized as follows.
%In Section 2, we introduce the related work.
%In Section 3, we give some notations and preliminaries.
%In Section 4,
%we propose the upper bound of the local Rademacher complexity for multi-class classification,
%and further derive a generalization error bound of fast convergence rate.
% We derive two novel multi-class classification algorithms based on local Rademacher complexity in Section 5.
%We empirically analyze the performance of our proposed bounds in Section 6.
%We end in Section 7 with the conclusion.
%Due to limited space,
%all the proofs are given in the supplementary material.
\section{Related Work}
\subsection{Multi-Class Classification Bounds}
\textbf{Rademacher Complexities Bounds.}
   Koltchinskii and Panchenko \cite{koltchinskii2002empirical}
    and Koltchinskii, Panchenko, and Lozano \cite{koltchinskii2001some}
    first introduced a margin-based bound for multi-class classification in terms of Rademacher complexity.
    This bound was slightly improved in \cite{mohri2012foundations,cortes2013multi}.
    Maximov and Reshetova \cite{maximov2016tight} gave a new Rademacher complexity based bound
    that is linear in the number of classes.
    % give a new Rademacher complexity based
    %bound depends linearly in the number of classes of $\mathcal{O}\big(\frac{k}{\sqrt{n}}\big)$.
    %The bound involves the marginal distribution
    %of the classifier and the Rademacher complexity of the hypothesis class
    Based on the $\ell_p$-norm regularization,
    Lei, Binder, and Klof \cite{lei2015multi} introduced a bound
    with a logarithmic dependence on the number of class size.
    Instead of global Rademacher complexity,
    in this paper, we use local Rademacher complexity
    to obtain a sharper bound, which  substantially improves
    generalization performance upon existing global Rademacher complexity methods.

\textbf{VC-dimension Bounds.}
    Allwein, Schapire, and Singer \cite{allwein2000reducing}  used the notion of VC-dimension
    for multi-class learning problems,
    and derived a VC-dimension based bound.
    Natarajan dimension was introduced in \cite{Natarajan1989} in
    order to characterize multi-class PAC learnability,
    which exactly matches the notion of Vapnik-Chervonenkis
    dimension in the case of binary classification.
    Daniely and Shalev-Shwartz \cite{daniely2014optimal} derived a risk bound with Natarajan dimension for multi-class classification.
    VC dimension and Natarajan dimension are important tools to derive generalization bounds,
    however, these bounds are usually dimension
    dependent, which makes them hardly applicable to practical large-scale problems (such as typical computer vision problems).

\textbf{Covering Number  Bounds.}
    Based on the $\ell_\infty$-norm covering number bound of
    linear operators, Guermeur \cite{guermeur2002combining} obtained
    a generalization bound exhibiting a linear dependence on the class size,
    which was improved by \cite{zhang2004statistical} to a radical dependence.
    Hill and Doucet \cite{Hill2007} derived a class-size independent risk guarantee.
    However, their bound is based on a delicate definition of margin,
    which is not commonly used in mainstream multi-class literature.

\textbf{Stability Bounds and PAC-Bayesian Bounds.}
    Stability \cite{Hardt2016} and PAC-Bayesian \cite{McAllester2013} are two popular tools
    to analyze generalization performance on neural networks for multi-class setting.
    Hardt, Recht and Singer \cite{Hardt2016} generated generalization bounds
    for models learned with stochastic gradient descent.
     McAllester \cite{McAllester2013} proposed a dropout bound for neural networks with  PAC-Bayesian.
    However, the convergence rate based on stability and PAC-Bayesian is usually at most $\mathcal{O}(1/\sqrt{n})$.


\subsection{Local Rademacher Complexity}
%One of the useful data-dependent complexity measures used in the generalization analysis
%for  traditional  binary classification
%is the notion of (global) Rademacher complexity \cite{bartlett2003rademacher}.
%However,
%it provides global estimates of the complexity of the function class,
%that is, it does not reflect
%the fact that the algorithm will likely pick functions that have a small error.
In recent years,
several authors have applied
 \emph{local} Rademacher complexity to obtain  better generalization error bounds
for traditional binary classification \cite{Bartlett2005lrc,Koltchinskii2006lrcoiirm,Liu2015er,Liu2017ep},
similar analysis has been explored in multi-label learning \cite{xu2016local} and multi-task learning \cite{Yousefi2016} as well.
%The local Rademacher complexity considers Rademacher averages
%of a smaller subset of the hypothesis set,
%so it is always smaller than the corresponding global one.
%There are some work consider the use of local Rademacher complexity to
%derive tighter bounds for binary classification.
However,
numerous statistical challenges remain in the multi-class case, and it is still unclear how to use
this tool to derive a tighter bound for multi-class.
In this paper,
we bridge this gap by deriving a sharper generalization bound using local Rademacher complexity.
%we show how to use the notion of local Rademacher complexity to derive a sharper generalization bound,
%which fills this gap.
%we concern the structural relationship
%of Gaussian complexities since it is based on a comparison result
%among different Gaussian processes.
%Our result is a non-trivial extension of the local Rademacher complexity of
%binary classification for multi-class classification.

\subsection{Multi-Class Kernel Learning Algorithms}
As one of the success stories in multiple kernel learning, improvements in multi-class MKL have emerged \cite{ZienO2007},
in which a one-stage multi-class MKL algorithm was presented as a generalization of multi-class loss function \cite{CrammerS02,TsochantaridisHJA04}.
And Orabona designed stochastic gradient methods, named
OBSCURE \cite{OrabonaJC10} and
UFO-MKL \cite{OrabonaL11}, which optimize primal versions of equivalent problems.
In this paper, we consider the use of  the local Rademacher complexity to devise the novel  multi-class classification
algorithms, which have statistical guarantees and fast convergence rates.

\section{Notations and Preliminaries}
We consider multi-class classification problems with $K\geq 2$ classes in this paper.
Let $\mathcal{X}$ be the input space and $\mathcal{Y}=\{1,2,\ldots,K\}$ the output space.
Assume that we are given a sample
$
  \mathcal{S}=\{z_1=(\mathbf  x_1,y_1),\ldots, z_n=(\mathbf  x_n,y_n)\}
$
of size $n$ drawn i.i.d. from a fixed,
but unknown probability distribution $\mu$ on $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$.
Based on the training examples $\mathcal{S}$,
we wish to learn a scoring rule $h$ from a space $\mathcal{H}$ mapping from $\mathcal{Z}$ to $\mathbb{R}$
and use the mapping $\mathbf x\rightarrow \argmax_{y\in\mathcal{Y}} h(\mathbf x,y)$ to predict.
For any hypothesis $h\in\mathcal{H}$,
the margin of a labeled example $z=(\mathbf x,y)$ is defined as
\begin{align*}
  \rho_h(z):= h(\mathbf x,y)-\max_{y'\not =y} h(\mathbf x,y').
\end{align*}
The $h$ misclassifies the labeled example $z=(\mathbf x,y)$
if $\rho_h(z)\leq 0$ and thus the expected risk incurred
from using $h$ for prediction is
$
  L(h):=\mathbb{E}_\mu[1_{\rho_h(z)\leq 0}],
$
where $1_{t\leq 0}$ is the 0-1 loss,
$1_{t\leq 0}=1$ if $t\leq 0$, otherwise 0.
Since 0-1 loss is hard to handle in learning machines,
one usually considers the proxy loss:
such as the square hinge $\ell(t)=(1-t)_+^2$ and the square margin loss $\ell^s(t)=\left(1_{t\leq 0}+(1-ts^{-1})1_{0<t\leq s}\right)^2$, $s>0$.
In the following, we assume that:
1) $\ell(t)$ bounds the 0-1 loss: $1_{t\leq 0}\leq \ell(t)$;
2) $\ell$ is decreasing and it has a zero point $c_\ell$, i.e., $\ell(c_\ell)=0$;
3) $\ell$ is $\eta$-smooth, that is $|\ell'(t)-\ell'(s)|\leq \zeta|t-s|$.
Note that both square hinge loss and margin loss satisfy the above assumptions.
%\begin{definition}[Regular Loss]
%  We call $\ell$ a $\zeta$-regular loss if it satisfies the following properties:
%  1) $\ell(t)$ bounds the 0-1 loss: $1_{t\leq 0}\leq \ell(t)$;
%  2) $\ell$ is $\zeta$-smooth, i.e., $|\ell'(t)-\ell'(s)|\leq \zeta|t-s|$;
%  3) $\ell$ is decreasing and it has a zero point $c_\ell$, i.e., $\ell(c_\ell)=0$.
%\end{definition}
%Some examples of $\zeta$-regular loss functions include the popular squared Hinge loss
%$\ell(t)=\left(\max\{0,1-t\}\right)^2$, squared margin loss $\ell(t)=\left(\max\{0,1-t+\rho\}\right)^2$.
%Note that when $\theta\rightarrow 0$, ramp loss and Huber loss converge to the  0-1 loss and
%Hinge loss $\ell(t)=\max\{0,1-t\}$, respectively.

Any function $h:\mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}$ can be equivalently represented by the vector-valued function
$(h_1,\ldots,h_K)$ with $h_j(\mathbf x)=h(\mathbf x,j)$, $\forall j=1,\ldots,K$.
Let $\kappa:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}$ be a Mercer kernel with $\phi$ being the associated feature map,
i.e., $\kappa(\mathbf x,\mathbf x')=\langle \phi(\mathbf x),\phi(\mathbf x')\rangle$.
The $\ell_p$-norm hypothesis space associated with the kernel $\kappa$ is denoted by:
 \begin{align}
 \label{hypothspapce}
   \mathcal{H}_{p,\kappa}=&\Big\{h_\mathbf{w}=\left(\langle \mathbf w_1,\phi(\mathbf x)\rangle,\ldots, \langle\mathbf w_K,\phi(\mathbf x)\rangle\right):
   \left\|\mathbf  w \right\|_{2,p}\leq 1, 1\leq p\leq 2
  \Big\},
 \end{align}
 where $\mathbf w=(\mathbf w_1,\ldots,\mathbf w_K)$ and $\|\mathbf w\|_{2,p}=\left[\sum_{i=1}^K\|\mathbf w_i\|_2^p\right]^{\frac{1}{p}}$
 is the $\ell_{2,p}$-norm.
 For any $p\geq 1$, let $q$ be the dual exponent of $p$ satisfying ${1}/{p}+{1}/{q}=1$.
% Note that,  $\forall 1\leq p\leq 2$, we have $q\geq 2$.

The space of loss function associated with $\mathcal{H}_{p,\kappa}$  is denoted by
\begin{align}
\label{eq-sapce-loss-functions}
  \mathcal{L}=\left\{\ell_h:=\ell(\rho_h(z)):h\in\mathcal{H}_{p,\kappa}\right\}.
\end{align}
Let $L(\ell_h)$ and  $\hat{L}(\ell_h)$ be expected generalization error and
 empirical error  with respect to $\ell_h$:
$$
  L(\ell_h):=\mathbb{E}_\mu[\ell(\rho_h(z))] \text{ and } \hat{L}(\ell_h)=\frac{1}{n}\sum_{i=1}^n\ell(\rho_h(z_i)).
$$




%Note that if $\ell$ is a $\zeta$-regular loss,
%then
%\begin{align*}
%  L(h)\leq  L(\ell_h).
%\end{align*}
%Thus, we can use $L(\ell_h)$ to bound $L(h)$.

%In order to estimate $L(\ell_h)$, we introduce the Rademacher complexity.
%To this end,
%we have to study the behavior of the difference between the
%risk and the empirical risk.
%To this end, we introduce the notion of uniform deviation of $\mathcal{L}$,
%denoted as
%\begin{align}
%\label{eq-Uniform-deviation-definition}
%  \hat{U}_n(\mathcal{L})
%    =\sup_{\ell_h\in\mathcal{L}}\left\{L(\ell_h)-\hat{L}(\ell_h)\right\}.
%\end{align}
%Note that
%$
%  \left\{L(\ell_h)-\hat{L}(\ell_h)\right\}_{\ell_h\in\mathcal{L}}
%  \leq \hat{U}_n(\mathcal{L}),
%$
%so we have
%\begin{align*}
%  L(\ell_h)\leq \hat{L}(\ell_h)+
%  \hat{U}_n(\mathcal{L}),
%  \forall \ell_h\in\mathcal{L}.
%\end{align*}
%$\hat{U}_n(\mathcal{L})$
%is not computable,
%but we can bound its value via  the
% Rademacher complexity $\mathcal{L}$:
\begin{definition}[Rademacher complexity]
Assume $\mathcal{L}$ is a space of loss functions as defined in Equation \eqref{eq-sapce-loss-functions}.
Then the empirical Rademacher complexity of $\mathcal{L}$ is:
%Then the empirical \textbf{ranking Rademacher complexity} of $\mathcal{L}$ is:
%    Let $\mathcal{H}$ be a family of real-valued functions
%    defined on $\mathcal{Z}$ and $\mathcal{S}=(z_1,\ldots,z_n)$ a fixed sample of size $n$ with elements in $\mathcal{Z}$.
    %Then, the empirical Rademacher and Gaussian complexities of $\mathcal{H}$ with respect to the sample $\mathcal{S}$ are defined by
    \begin{align*}
      \hat{\mathcal{R}}(\mathcal{L}):=\mathbb{E}_{\bm \sigma}
      \left[\sup_{\ell_h\in\mathcal{L}}
     \frac{1}{n}\sum_{i=1}^n\sigma_i \ell_{h}(z_i)
          \right],
    \end{align*}
     where $\sigma_1,\sigma_2, \ldots,\sigma_n$
    is an i.i.d. family of Rademacher variables taking values -1 and 1
    with equal probability independent of the sample $\mathcal{S}=(z_1,\ldots,z_n)$.
    The  Rademacher complexity of $\mathcal{L}$ is
    $
      %\label{def-expect-Rademacher}
      \mathcal{R}(\mathcal{L})=\mathbb{E}_{\mu}\hat{\mathcal{R}}(\mathcal{L}).
   $
\end{definition}
%Rademacher complexity is the most powerful tool to get generalization bounds for multi-class classification, currently.
Generalization bounds based on the notion of Rademacher complexity for multi-class classification
are standard \cite{koltchinskii2002empirical,koltchinskii2001some,mohri2012foundations}:
with probability $1-\delta$,
$
  L(h)\leq \inf_{0<\gamma<1}\big(\hat{L}(h_\gamma)+\mathcal{O}\big({\mathcal{R}(\mathcal{L})}/{\gamma}+{\log(1/\delta)}/{\sqrt{n}}\big)\big),
$
where $\hat{L}(h_\gamma)=\frac{1}{n}\sum_{i=1}^n\left[1_{\rho_h(z_i)\leq \gamma}\right]$.
Since $\mathcal{R}(\mathcal{L})$ is in the order of $\mathcal{O}({K^2}/{\sqrt{n}})$ for various  kernel multi-class in practice,
so the standard Rademacher complexity bounds converge at rate
$\mathcal{O}\big({K^2}/{\sqrt{n}}\big)$, usually.

Although Rademacher complexity is widely used in bound generalization analysis,
it does not take into consideration the fact that,
typically, the hypotheses selected by a learning algorithm
have a better performance than in the worst case and
belong to a more favorable sub-family of the set of all hypotheses \cite{cortes2013learning}.
Therefore, to derive sharper generalization bound,
we consider the use of the local Rademacher complexity in this paper.
%To this end,
%  let $\mathcal{L}^r$ be a star-shaped space of $\mathcal{L}$ with respect to $r>0$,
%  \begin{align}
%  \label{def-localrademchercomplexity}
%    \mathcal{L}^r=\left\{
%        a\ell_h\Big|a\in[0,1],\ell_h\in\mathcal{L},
%        L[(a\ell_h)^2]\leq r
%    \right\}.
%  \end{align}
% Note that
% \begin{align*}
%   \forall \ell_h\in\mathcal{L}^r,
%   V^2(\ell_h)=L(\ell_h^2)-[L(\ell_h)]^2
%   \leq L(\ell_h^2)\leq r,
% \end{align*}
% so the variance of each element in $\mathcal{L}^r$ is smaller than $r$.
 %Therefore,
% for all $\ell_h^r\in\mathcal{L}^r$,
% its variance  $V^2(\ell_h)=L(\ell_h^2)-[L(\ell_h)]^2$,
% so the following inequality holds
 %Since such a small class can  also have a substantially
% smaller complexity, we can obtain sharp bound.
%The local ranking Rademacher complexity is defined as follows:
%For this purpose, we consider using the  local
%ranking Rademacher complexity defined as follows:
 \begin{definition}[Local Rademacher Complexity]
 \label{def-thereee}
   For any $r>0$, the local Rademacher complexity of $\mathcal{L}$ is defined as
   \begin{align*}
     \mathcal{R}(\mathcal{L}^r):=
     \mathcal{R}\left\{
        a\ell_h\Big|a\in[0,1],\ell_h\in\mathcal{L},
        L[(a\ell_h)^2]\leq r
    \right\},
   \end{align*}
   where $L(\ell_h^2)=\mathbb{E}_\mu\left[\ell^2(\rho_h(z))\right]$.
 \end{definition}
 The key idea to obtain sharper generalization error bound
 is to choose a much smaller class $\mathcal{L}^r\subseteq\mathcal{L}$
 with as small a variance as possible,
 while requiring that the solution is still in $\{h|h\in\mathcal{H}_{p,\kappa},\ell_h\in \mathcal{L}^r\}$.

In the following, we assume that
$\vartheta=\sup_{\mathbf x\in \mathcal{X}}\kappa(\mathbf x,\mathbf x)< \infty$,
and $\ell_h:\mathcal{Z}\rightarrow [0,d]$,
$d>0$ is a constant.
The above two assumptions are two common restrictions on kernel function and loss functions,
which are satisfied by the popular Gaussian kernels and the bounded hypothesis, respectively.

\section{Sharper Generalization Bounds}
In this section, we first estimate the local Rademacher complexity,
and further derive a sharper generalization bound.

\subsection{Local Rademacher Complexity}
The estimate the local Rademacher complexity of multi-class classification is given as follows.
\begin{theorem}
\label{rademacherlocal}
  With probability at least $1-\delta$,
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r) \leq \frac{c_{d,\vartheta}\xi(K)\sqrt{\zeta r}\log^{\frac{3}{2}}(n)}{\sqrt{n}}+\frac{4\log(1/\delta)}{n},
  \end{align*}
  where
  \begin{align*}
  \xi(K)=
  \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } q\geq 2\log K,\\
      &(2q)^{1+\frac{1}{q}}K^{\frac{1}{q}}, &&\text{otherwise},
      \end{aligned}
      \right.
  \end{align*}
  $c_{d,\vartheta}$ is a constant depends on $d$ and $\vartheta$.
\end{theorem}
%The square loss and
Note that the order of the (global) Rademacher complexity over $\mathcal{L}$ is usually
$\mathcal{O}\big({K^2}/{\sqrt{n}}\big)$ for various kernel multi-classes.
From Theorem \ref{rademacherlocal}, one can see that the order of the local Rademacher complexity is
$
  \mathcal{R}(\mathcal{L}^r)=\mathcal{O}\big({\sqrt{r}\xi(K)}/{\sqrt{n}}+{1}/{n}\big).
$
Note that $\xi(K)$ is logarithmic dependence on $K$ when $q\geq 2\log K$.
For $2\leq q < 2\log K$, $\xi(K)=\mathcal{O}(K^{\frac{2}{q}})$
which is also substantially milder than the quadratic dependence for Rademacher complexity.
If we choose a suitable value of $r$,
the order can even reach $\mathcal{O}\big({(\log K)^{2+1/\log K}}/{n}\big)$ (see in the next subsection),
which substantially improves the Rademacher complexity bounds.
%\begin{remark}
%  Existing work \cite{kuznetsov2014multi,mohri2012foundations} for multi-class classifiers usually builds on the following result on Rademacher complexity:
%  \begin{align*}
%    \mathcal{R}\big(\max\{h_1,\ldots,h_K\}:h_j\in \mathcal{H}_j, j=1,\ldots,K\big)\leq \sum_{j=1}^K \mathcal{R}(\mathcal{H}_j),
%\end{align*}
%where $\mathcal{H}_1,\ldots, \mathcal{H}_M$ are $K$ hypothesis sets.
%This result is crucial, but it does not take into account the coupling among different $K$ classes.
%To obtain the sharper bound, we introduce a new structural complexity result on function classes induced by general classes
%via the maximum operator, while allowing to preserve the correlations among different components meanwhile,
%which is the main technical challenges of this paper.
%\end{remark}
\subsection{A Sharper Generalization Bound}
A sharper bound for multi-class classification based on
the notion of local Rademacher complexity is derived as follows.
\begin{theorem}
\label{theorem-finally}
  $\forall h\in\mathcal{H}_{p,\kappa}$ and $\forall k>\max(1,\frac{\sqrt{2}}{2d})$,  with probability at least
  $1-\delta$, we have
  \begin{align*}
   L(h)\leq \max\left\{
        \frac{k}{k-1}\hat{L}(\ell_h),
       \hat{L}(\ell_h)+\frac{c_{d,\vartheta,\zeta, k}\xi^2(K) \log^3 n}{n}+\frac{c_{\delta}}{n}
     \right\},
\end{align*}
where \begin{align*}
  \xi(K)=
  \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } q\geq 2\log K,\\
      &(2q)^{1+\frac{1}{q}}K^{\frac{1}{q}}, &&\text{otherwise},
      \end{aligned}
      \right.
  \end{align*}
  $c_{d,\vartheta}$ is a constant depending on $d,\vartheta,\zeta, k$,
  and $c_{\delta}$ is a constant depending on $\delta$.
\end{theorem}

The order of the  generalization bound in Theorem \ref{theorem-finally} is
$
 \mathcal{O}\big({\xi^2(K)}/{n}\big).
$
From the definition of $\xi(K)$, we can obtain that
 \begin{align*}
  \mathcal{O}\left(\frac{\xi^2(K)}{n}\right)=
  \left\{
      \begin{aligned}
      &\mathcal{O}\left({(\log K)^{2+{1}/{\log K}}}/{n}\right), &&\text{if } q\geq 2\log K,\\
      &\mathcal{O}\left({K^{2/q}}/{n}\right), &&\text{if } 2\leq q< 2\log K.
      \end{aligned}
      \right.
  \end{align*}
  Note that our bounds is linear dependence on the reciprocal of sample size $n$,
  while for the existing data-dependent bounds are all radical dependence.
 % which is substantially faster than the dependence on $\mathcal{O}\left(\frac{1}{\sqrt{n}}\right)$ established in the existing
%  work for multi-class classification.
Furthermore,
  our bounds enjoy a mild dependence on the number of classes.
  The dependence is polynomial with degree $2/q$ for $2\leq q < 2\log K$ and
  becomes logarithmic if $q \geq 2\log K$,
  which is substantially milder than the quadratic dependence
  established in \cite{koltchinskii2002empirical,koltchinskii2001some,mohri2012foundations,cortes2013multi}.

%To the best of our knowledge,
%the generalization bound  of multi-class classification
%with  linear dependence on reciprocal on the size of the sample and logarithmic dependence on the class size
%have never given before.

\subsection{Comparison with the Related Work}
\textbf{Rademacher Complexity Bounds}
%Currently Rademacher complexity are the most powerful tools to get generalization bounds for
%multi-class classification.
%The important property of Rademacher complexity based bounds is that the
%bounds are applicable in arbitrary Banach spaces and do not depend on the dimension of the feature
%space directly.
Koltchinskii and Panchenko \cite{koltchinskii2002empirical}
and Koltchinskii, Panchenko, and Lozano \cite{koltchinskii2001some}
introduce a margin-based bound for multi-class classification in terms of Rademacher complexities:
$
  L(h)\leq \inf_{0<\gamma<1}\hat{L}(h_\gamma)+\mathcal{O}\big(\frac{K^2}{\gamma\sqrt{n}}+\frac{\log1/\delta}{\sqrt{n}}\big).
$
The order is $\mathcal{O}\big(\frac{K^2}{\sqrt{n}}\big)$,
which is slightly improved (by a constant factor prior to the Rademacher complexity term)
by \cite{mohri2012foundations,cortes2013multi}.
Maximov and Reshetova \cite{maximov2016tight} give a new Rademacher complexity bound:
$
  L(h)\leq \inf_{0<\gamma<1}\hat{L}(h_\gamma)+\mathcal{O}
  \big({K}/{(\gamma\sqrt{n})}+{\log(1/\delta)}/{\sqrt{n}}\big),
$
which has the form of $\mathcal{O}\big({K}/{\sqrt{n}}\big)$.
% give a new Rademacher complexity based
%bound depends linearly in the number of classes of $\mathcal{O}\big(\frac{k}{\sqrt{n}}\big)$.
%The bound involves the marginal distribution
%of the classifier and the Rademacher complexity of the hypothesis class
Based on the $\ell_p$-norm regularization,
Lei, Binder, and Klof \cite{lei2015multi} derive a new bound:
$
   L(h)\leq  \hat{L}(\ell_h) +\mathcal{O}\big({\log^2 K}/{\sqrt{n}}\big).
$
The existing bounds based on Rademacher complexity are all radical dependence
on the reciprocal of sample size.
%of the form $\mathcal{O}\big(\frac{\log^2 K}{\sqrt{n}}\big)$
%for some value of $p$.

In this paper,
we derive a sharper bound based on the local Rademacher complexity with order
$
\mathcal{O}\big({(\log K)^{2+\frac{1}{\log K}}}/{n}\big),
$
substantially sharper
than the existing bounds of  Rademacher complexity.

\textbf{Covering Number Bounds}
Based on the $\ell_\infty$-norm covering number bound of
linear operators, Guermeur \cite{guermeur2002combining} obtains
a generalization of form $\mathcal{O}\big({K}/{\sqrt{n}}\big)$,
which is improved by \cite{zhang2004statistical} to a radical dependence:
  $
   L(h)\leq  \hat{L}(\ell_h) +\mathcal{O}\big(\sqrt{{K}/{n}}\big).
$
Hill and Doucet \cite{Hill2007} derive a class-size independent risk guarantee
of form $\mathcal{O}\big(\sqrt{{1}/{n}}\big)$.
However, their bound is based on a delicate definition of margin,
which is not commonly used in mainstream multi-class literature.
%Related results for metric spaces with low doubling dimension were obtained by \cite{Kontorovich2014},
%who used the nearest neighbors method to improve the dependence on the number
%of classes in favor of (doubling) dimension dependence.

\textbf{VC-dimension Bounds}
VC-dimension is an important tool to derive the generalization bound for binary classification.
Allwein, Schapire, and Singer \cite{allwein2000reducing} show how to use it
for multi-class learning problems,
and derive a VC-dimension based bounds:
$
   L(h)\leq  \hat{L}(h_\gamma) +\mathcal{O}\big({\sqrt{V}\log K}/{\sqrt{n}}\big),
$
where $V$ is the VC-dimension.
Natarajan dimension is introduced in \cite{Natarajan1989} in
order to characterize multi-class PAC learnability.
%It exactly matches the notion of Vapnik-Chervonenkis
%dimension in the case of two classes.
Daniely and Shalev-Shwartz \cite{daniely2014optimal} derive a generalization bound with Natarajan dimension:
$
   L(h)\leq  \hat{L}(h_\gamma) +\mathcal{O}\big({d_{Nat}}/{n}\big),
$
where $d_{Nat}$ is the Natarajan dimension.
Note that VC dimension bounds, as well as Natarajan dimension bounds, are usually dimension
dependent, which makes them hardly applicable for practical large
scale problems (such as typical computer vision problems).

\textbf{Stability and PAC-Bayesian Bounds}
Stability \cite{Hardt2016} and PAC-Bayesian \cite{McAllester2013} are two useful tools
to analyze generalization performance on neural networks for a multi-class setting.
Hardt, Recht and Singer \cite{Hardt2016} generated generalization bounds for models learned with stochastic gradient descent using stability:
$
   L(h)\leq  \hat{L}(h_\gamma) +\mathcal{O}\big({1}/{\sqrt{n}}\big).
$
McAllester \cite{McAllester2013} used the PAC-Bayesian theory
to derive generalization bound:
$
   L(h)\leq  \hat{L}(h_\gamma) +\mathcal{O}\big(\sqrt{\hat{L}(h_\gamma)/{n}}\big).
$
%But the convergence rate based on stability and PAC-Bayesian is usually at most $\mathcal{O}(1/\sqrt{n})$.
%\begin{remark}
%  Stability and PAC
%\end{remark}
%\textbf{Stability}
%The above theoretical analysis indicates that it is a good
%choice to use the local Rademacher complexity to analyze the generalization
%ability of multi-class classification.

\section{Multi-Class Multiple Kernel Learning}
Motivated by the above analysis of generalization bound,
we will exploit the properties of the local Rademacher complexity
to devise two algorithms for multi-class multiple kernel learning (MC-MKL).
%\subsection{Motivation and Anlysis}
%As we know, there is a standard way to learn binary classification problems,
%in which the prediction of $\mathbf x$ is the scalar product between a hyperplane $\mathbf w$ and the transformed sample $\phi(\mathbf x)$,
%corresponding kernel $k(\mathbf x, \mathbf x')$ defined as $\phi(\mathbf x) \cdot \phi(\mathbf x')$.

In this paper, we consider the use of multiple kernels,
$\kappa_{\bm \mu}=\sum_{m=1}^M \mu_m\kappa_m.$
A common approach to multi-class classification is the use of joint feature
maps $\phi(\mathbf x):\mathcal X \to \mathcal H$ \cite{TsochantaridisHJA04}.
For multiple kernel learning, we have $M$ feature mappings $\phi_m$, $m=1,\ldots,M$ and
$\kappa_m(\mathbf x,\mathbf x')=\langle \phi_m(\mathbf x), \phi_m(\mathbf x')\rangle$,
where $m=1,\ldots,M$.
Let $\phi_{\bm \mu}(\mathbf x)=[\phi_1(\mathbf x),\ldots,\phi_M(\mathbf x)]$.
%We know $\mathbf{K}_{\bm \mu}(i,j)=\langle \phi_{\bm \mu}(\mathbf{x}_i), \phi_{\bm \mu}(\mathbf{x}_j)\rangle$.%
%So, the $\ell_{2,p}$-norm of $\mathbf{w}$ can be defined as $\|\mathbf w\|_{2,p}=\|[\|\mathbf{w}_1\|_2,\|\mathbf{w}_2\|_2,\ldots,\|\mathbf{w}_M\|_2]\|_p$. The dual norm of $\|\cdot\|_{2,p}$ is $\|\cdot\|_{2,q}$, where $1/p+1/q=1$ \cite{kakade2009duality}.
Using Theorem \ref{theorem-finally}, to obtain a shaper generalization bound,
we confine $q \geq 2\log K$, thus $1 < p \leq \frac{2\log K}{2\log K-1}$.
The $\ell_p$ hypothesis space of multiple kernels can be written as:
 \begin{align*}
 \label{hypothspapcemkl}
   \mathcal{H}_{mkl}=\Big\{h_{\mathbf{w},\kappa_{\bm \mu}}&=\left(\langle \mathbf w_1,\phi_{\bm \mu}(\mathbf x)\rangle,\ldots, \langle\mathbf w_K,\phi_{\bm \mu}(\mathbf x)\rangle\right),
   \left\|\mathbf  w \right\|_{2,p}\leq 1, 1 < p \leq \frac{2\log K}{2\log K-1}
  \Big\}.
 \end{align*}

 %Note that the tail sum is the difference between the trace and
% the $\theta$ largest eigenvalues:$\sum_{j>\theta}\lambda_j(\mathbf{K})=\mathrm{Tr}(\mathbf{K})-\sum_{j=1}^\theta\lambda_j(\mathbf{K})$,
% thus the tail sum can be calculated in $O(n^2\theta)$ for each kernel.
\subsection{Conv-MKL}
%The trace of kernel matrix $\mathbf{K}_{\bm \mu}$ can be the upper-bound of the global Rademacher complexity of $\mathcal{H}_{mkl}$.
The global Rademacher complexity of $\mathcal{H}_{mkl}$ can be bounded by the trace of kernel matrix $\mathbf{K}_{\bm \mu}=\sum_{m=1}^M\mathbf K_m$.
Existing works on \cite{Lanckriet2004ltksp,SonnenburgRSS06} use the following constraint to $\mathcal{H}_{mkl}$:
$\mathrm{Tr}(\mathbf{K}_{\bm \mu}) \leq 1.$
According to the above theoretical analysis,
the local Rademacher complexity (the tail sum of the eigenvalues of the kernel) leads to
tighter generalization bounds than the global Rademacher complexity (the trace).
Thus, we add the local Rademacher complexity to restrict $\mathcal{H}_{mkl}$:
\begin{align*}
    \mathcal{H}_{1}=\Big\{h_{\mathbf{w}, \kappa_{\bm \mu}} \in \mathcal{H}_{mkl}:\sum_{j > \zeta} \lambda_j(\mathbf{K}_{\bm \mu}) \leq 1\Big\},
\end{align*}
where $\lambda_j(\mathbf K_{\bm \mu})$ is the $j$-th eigenvalues of $\mathbf K_{\bm \mu}$ and
$\zeta$ is free parameter removing the $\zeta$ largest eigenvalues to control the tail sum.
Note that the tail sum is the difference between the trace and
the $\zeta$ largest eigenvalues: $\sum_{j>\zeta}\lambda_j(\mathbf{K}_{\bm \mu})=\mathrm{Tr}(\mathbf{K}_{\bm \mu})-\sum_{j=1}^\zeta\lambda_j(\mathbf{K}_{\bm \mu})$,
thus the tail sum can be calculated in $O(n^2\zeta)$ for each kernel.

One can see that $\mathcal{H}_{1}$ is not convex, and we know that:
$
        \sum_{m=1}^M\mu_m\sum_{j>\zeta}\lambda_{j}(\mathbf{K}_m)
         =\sum_{m=1}^M\mu_m/\|\bm \mu\|_1 \sum_{j>\zeta}\lambda_{j}(\|\bm \mu\|_1\mathbf{K}_m)
         \leq \sum_{j>\zeta} \lambda_j\big(\mathbf{K}_{\bm \mu}\big).
$
Thus, we consider the use of the convex $\mathcal{H}_2$:
\begin{align*}
    \mathcal{H}_{2}=\Big\{h_{\mathbf{w}, \kappa_{\bm \mu}} \in \mathcal{H}_{mkl}:
    \sum_{m=1}^M\mu_m\sum_{j>\zeta}\lambda_{j}(\mathbf{K}_m) \leq 1\Big\}.
\end{align*}
%Note that by a renormalization of the kernels $\kappa_1,\ldots,\kappa_M$,
%according to
According to normalized kernels
$
    \tilde{\kappa}_m=\Big(\sum_{j>\zeta}\lambda_j(\mathbf K_m)\Big)^{-1}\kappa_m
    \text{ and }
    \tilde{\kappa}_{\bm \mu}=\sum_{m=1}^M\mu_m\tilde{\kappa}_m,
$
we can simply rewrite $\mathcal{H}_2$ as
$
   \Big\{h_{\mathbf{w},\tilde{\kappa}_{\bm \mu}}=\left(\langle \mathbf w_1,\tilde{\phi}_{\bm \mu}(\mathbf x)\rangle,
   \ldots, \langle\mathbf w_K,\tilde{\phi}_{\bm \mu}(\mathbf x)\rangle\right),
   \left\|\mathbf  w \right\|_{2,p}\leq 1, 1 < p \leq \frac{2\log K}{2\log K-1},
   \bm \mu\succeq 0,\|\bm \mu\|_1 \leq 1\Big\},
$
which is a commonly studied hypothesis class in multi-class multiple kernel learning.
A simple process with precomputed kernel matrices regularized by local Rademacher complexity can be seen in Algorithm  \ref{algorithm1}:
\begin{algorithm}[h]
   \caption{Conv-MKL}
   \label{algorithm1}
    \begin{algorithmic}
       \STATE {\bfseries Input:} precomputed kernel matrices $\mathbf{K}_1,\ldots,\mathbf{K}_M$ and $\zeta$
       \FOR{$i=1$ {\bfseries to} $M$}
        \STATE Compute tail sum: $r_m=\sum_{j>\zeta}\lambda_j\left(\mathbf{K}_m\right)$
        \STATE Normalize precomputed kernel matrix: $\widetilde{\mathbf{K}}_m=\mathbf{K}_m/r_m$
       \ENDFOR
       %\STATE Compute the minimizer of ERM over $\mathcal{H}_2$:%
       %\STATE Use $\widetilde{\mathbf{K}}_{\bm \mu}=\sum_{m=1}^M \mu_m \widetilde{\mathbf{K}}_m$ in any $\ell_p$-norm MKL solver
       \STATE Use $\widetilde{\mathbf{K}}_m$, $m=1,\ldots, M$, as the basic kernels in any $\ell_p$-norm MKL solver
    \end{algorithmic}
\end{algorithm}



\subsection{SMSD-MKL}
%According to local Rademacher analysis in Theorem \ref{theorem-finally},
%local Rademacher complexity leads to a shaper generalization bound.
Considering a more challenging case,
we perform penalized ERM over the class $\mathcal{H}_{1}$,
aiming to solve a convex optimization problem with an additional term representing local Rademacher complexity :
\begin{equation}
\label{optimization-problem}
    \min_{\mathbf w,\bm \mu}
    \underbrace{\frac{1}{n}\sum_{i=1}^n\ell(\mathbf w,\phi_{\bm \mu}(\mathbf{x}_i), y_i)}_{C(\mathbf w)}
    +\underbrace{\frac{\alpha}{2}\|\mathbf w\|_{2,p}^2
    +\beta\sum_{m=1}^M\mathbf{\mu}_mr_m}_{\Omega(\mathbf w)},
\end{equation}
where
$\ell(\mathbf w,\phi_{\bm \mu}(\mathbf{x}_i), y_i)=
\left|1-\left(\langle\mathbf w_{y_i},
\phi_{\bm \mu}(\mathbf{x}_i)\rangle -\max\limits_{y \not= y_i}\langle\mathbf w_y, \phi_{\bm \mu}(\mathbf{x}_i)\rangle\right)\right|_+$
and
$r_m=\sum_{j>\zeta}\lambda_j(\mathbf{K}_m)$ is the tail sum of the eigenvalues of the $m$-th kernel matrix,  $m=1,\ldots, M$.

\begin{algorithm}[t]
   \caption{SMSD-MKL}
   \label{algorithm2}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $\alpha, \beta, \bm r, T$
       \STATE {\bfseries Initialize:} $\mathbf w^1=\mathbf{0}, \pmb{\theta}^1=\mathbf{0}, \bm \mu^1=\mathbf{1}, q=2\log K $
       \FOR{$t=1$ {\bfseries to} $T$}
       \STATE Sample at random $(\mathbf{x}^t, y^t)$
       \STATE Compute the dual weight: $\pmb{\theta}^{t+1}=\pmb{\theta}^t-\partial{C(\mathbf w^t)}$
       \STATE $\nu_m^{t+1}=\|\bm \theta_m^{t+1}\|-t\beta r_m$, $\forall m=1,\ldots, M$
       \STATE $\mu_m^{t+1}=\frac{\mathrm{sgn}(\nu_m^{t+1})|\nu_m^{t+1}|^{q-1}}
       {\alpha\|\bm \theta_m^{t+1}\||\nu_m^{t+1}|_q^{q-2}}$, $\forall m=1,\ldots, M$
%       \STATE $\mathbf w_m=\frac{sgn(\nu_m^{t+1})\bm \theta_m^{t+1}|\nu_m^{t+1}|^{q-1}}
%       {\alpha\|\bm \theta_m^{t+1}\||\nu_m^{t+1}|_q^{q-2}}$, $\forall m=1,\ldots, M.$
       \ENDFOR
    \end{algorithmic}
\end{algorithm}

%We cope with the non-differentiability of the hinge loss directly by sub-gradients instead of gradients.
Based on the stochastic mirror descent framework for minimization problems in \cite{Shalev-ShwartzT11, OrabonaL11},
we design a stochastic mirror and sub-gradient descent algorithm, called \texttt{SMSD-MKL}, to minimize \eqref{optimization-problem},
seen in Algorithm \ref{algorithm2}.

 As shown in the mirror descent algorithm, it maintains two weight vectors: the primal vector $\mathbf w$ and the dual vector $\pmb{\theta}$.
 Meanwhile, the optimization formulation can be divided into two parts: $C(\mathbf w)$
 to update $\pmb{\theta}$  and $\Omega(\mathbf w)$ to update $\mathbf w$ by
 the gradient of the Fenchel dual of $\Omega$.
 Actually, the algorithm puts the kernel weight $\pmb{\mu}$ aside when updating $\pmb{\theta}$,
 but $\pmb{\mu}$ is updated together with $\mathbf w$ according to a tricky link function given in Theorem \ref{theorem-fenchel-dual}.
 %The connection between the two vectors is via a link function $\pmb{\theta}=f(\mathbf w)$, where $f:\mathcal{K}\times\mathcal{M}\times\mathcal{H} \to \mathcal{K}\times\mathcal{M}\times\mathcal{H}$. Function $\Omega$ is strictly convex, so the algorithm use $\pmb{\theta}=f(\mathbf w)=\nabla\Omega(\mathbf w)$ as link function and $\mathbf w==\nabla\Omega^\ast(\pmb{\theta})$ as the inverse function.Similar with UFO-MKL in \cite{OrabonaL11},

\begin{itemize}
\item For $C(\mathbf w)$, the algorithm updates the dual vector with the gradient of $C(\mathbf w)$.
Since hinge loss used in $C(\mathbf w)$ is not differentiable,
the algorithm uses sub-gradient of $z^t=\partial\ell(\mathbf w^t,\phi_{\bm \mu}(\mathbf{x}^t), y^t)$,
where $\partial\ell(\mathbf w^t,\phi_{\bm \mu}(\mathbf{x}^t), y^t)$ is the sub-gradient w.r.t $\mathbf w^t$.

\item For $\Omega(\mathbf w)$, as in the UFO-MKL \cite{OrabonaL11},
the algorithm uses $\mathbf w=\nabla\Omega^\ast(\pmb{\theta})$ to update the primal vector $\mathbf w$,
of which the calculation has been given in Theorem \ref{theorem-fenchel-dual}.
\end{itemize}

The algorithm starts with $\mathbf{w}^1=\mathbf{0}$, $\pmb{\theta}^1=\mathbf{0}$ and $\pmb{\mu}^1=\mathbf{1}$.
Especially, the algorithm initializes $q=2\log K$ to make the order of generalization reach
$\mathcal{O}\big(\frac{(\log K)^{2+1/\log K}}{n}\big)$, according to Theorem \ref{theorem-finally}.
In each iteration, the algorithm randomly samples a training example from the train set.

Actually, the algorithm updates real numbers $\|\bm \theta_m^{t+1}\|$, $\nu_m^{t+1}$ and $\mu_m^{t+1}$ in scalar products instead of high-dimensional variables $\mathbf w^{t+1}$ and $\bm \theta_m^{t+1}$.
The $\|\bm \theta_m^{t+1}\|$ can be calculated in an efficient incremental way by scalar values as following:
\begin{align*}
\|\pmb{\theta}_m^{t+1}\|
&=\|\pmb{\theta}_m^{t}-z^t\|_2^2
=\|\pmb{\theta}_m^{t}\|_2^2-2\pmb{\theta}_m^t \cdot z^t +\|z^t\|_2^2\\
&=\|\pmb{\theta}_m^{t}\|_2^2-2\pmb{\theta}_m^t \cdot (\phi_m(\mathbf{x}_i)-\phi_m(\mathbf{x}_j))
+ 2\kappa_m(\mathbf{x}_i, \mathbf{x}_i)^2
\end{align*}
where $z^t=\partial\ell(\mathbf w^t,\phi_{\bm \mu}(\mathbf{x}^t), y^t)$.

\begin{theorem}
\label{theorem-fenchel-dual}
Let
$
    \bm \nu=\Big[\|\pmb{\theta}_1\|-\beta r_1,\ldots,
    \|\pmb{\theta}_M\|-\beta r_M\Big],
$
then the component $m$-th of $\;\nabla\Omega^\ast(\pmb{\theta})$ is
\begin{align*}
\frac{\mathrm{sgn}(\nu_m) \pmb{\theta}_m}{\alpha\|\pmb{\theta}_m\|}
       \frac{|\nu_m|^{q-1}}{\|\bm \nu\|_q^{q-2}},
\end{align*}
where $\mathrm{sgn}(x)$ is defined as
$\mathrm{sgn}(x)=1$ if $x>0$, $\mathrm{sgn}(x)=-1$ if $x<0$
and $\mathrm{sgn}(x)\in[-1,+1]$, if $x=0$.
\end{theorem}

%\begin{lemma}
%  \label{inverse-mapping}
%  Let $p \in (1,2]$ and $q=p/(p-1)$, and then the norms $\|\pmb{c}\|_p$ and $\|\pmb{c}^\ast\|_q$ are dual to each other. Define the mapping $f:\mathcal{M} \to \mathcal{M}$ with
%  \begin{align*}
%    \pmb{c}^\ast_i=f_i(\pmb{c})=\nabla_i\Big(\frac{1}{2}\|\pmb{c}\|_p^2\Big)=\frac{sng(\pmb{c}_i)|\pmb{c}_i|^{p-1}}{\|\pmb{c}\|_p^{p-2}}, i=1,\ldots,n,
%  \end{align*}
%and the inverse mapping $f^{-1}$ with
%  \begin{align*}
%    \pmb{c}_i=f^{-1}_i(\pmb{c}^\ast)=\nabla_i\Big(\frac{1}{2}\|\pmb{c}^\ast\|_q^2\Big)=\frac{sng(\pmb{c}^\ast_i)|\pmb{c}^\ast_i|^{q-1}}{\|\pmb{c}^\ast\|_q^{q-2}}, i=1,\ldots,n,
%  \end{align*}
%\end{lemma}
%These mapping are often called {\em link functions} in machine learning (e.g., \cite{Gentile03a}).

%\begin{remark}
%  Both \texttt{Conv-MKL} and \texttt{SMSD-MKL} algorithms are convergent.
%  The convergence analysis of these two algorithms are seen in supplementary material.
%\end{remark}


\section{Experiments}
In this section, we compare our proposed \texttt{Conv-MKL} (Algorithm \ref{algorithm1}) and \texttt{SMSD-MKL} (Algorithm \ref{algorithm2})
with 7 popular multi-class classification methods:
 One-against-One \cite{knerr1990single}, One-against-the-Rest \cite{bottou1994comparison},
$\ell_1$-norm linear multi-class SVM (LMC) \cite{CrammerS02},
generalized minimal norm problem solver (GMNP) \cite{franc2005optimization},
the Multiclass MKL (MC-MKL) with $\ell_1$-norm and $\ell_2$-norm \cite{ZienO2007}
and mixed-norm MKL solved by stochastic gradient descent (UFO-MKL) \cite{OrabonaL11}.
Actually, we complete comparison tests via implements in LIBSVM
(One-against-One and One-against-the-Rest),
the DOGMA libary \footnote{Available at http://dogma. sourceforge. net}
(LMC, GMNP,
$\ell_1$-nomr and $\ell_2$-norm MC-MKL) and the SHOGUN-6.1.3
\footnote{Available at http://www.shogun-toolbox.org/}
(UFO-MKL).
We implement our proposed \texttt{Conv-MKL} and \texttt{SMSD-MKL} algorithms based on UFO-MKL.

We experiment on 14 publicly available datasets:
four of them evaluated in \cite{ZienO2007} (plant, nonpl, psortPos, and psortNeg)
and others from LIBSVM Data. %\footnote{Available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/}.
For each dataset, we use the Gaussian kernel
$K(\mathbf{x}, \mathbf{x}')=\exp\big(-{\|\mathbf{x}-\mathbf{x}'\|_2^2}/{2\tau}\big)$ as our basic kernels,
where $\tau \in {2^i, i=-10,-9,\ldots,9,10}$.
For single kernel methods (One vs. One, One vs. Rest and GMNP),
we choose the kernel which have the highest performance among basic kernels estimated by 10-folds cross-validation.
Meanwhile, we use all basic kernels in MKL methods (\texttt{Conv-MKL}, \texttt{SMSD-MKL}, $\ell_1$ MC-MKL, $\ell_2$ MC-MKL and UFO-MKL).
%For our algorithms \texttt{Conv-MKL} and \texttt{SMSD-MKL}, we fix the parameter $\zeta=2$ to curve local Rademacher complexity.
The regularization parameterized $\alpha \in {2^i, i=-2, \ldots, 12}$ in all algorithms and
$\zeta\in{2^i,i=1,2,\ldots,4}, \beta \in {10^i, i=-4, \ldots, 1}$ in \texttt{SMSD-MKL} are determined by 10-folds cross-validation on training data.
Other parameters in compared algorithms follow the same experimental setting in their papers.
For each dataset, we run all methods 50 times with randomly selected 80\% for training and 20\% for testing,
offering an estimate of the statistical significance of differences
in performance between methods.
% Let $A_i$ and $B_i$ be the test errors of methods A and B in partition $i$,
%and $d_i=B_i-A_i, i=1,\ldots,30$. Let $\bar{d}$ and $S_d$ be the mean and standard error of $d_i$.
%Under $t$-test, with confidence level 95\%, we claim that A is significantly better than B (or equivalently B significantly worse than A)
%if the $t$-statistic $\frac{\bar{d}}{S_d/\sqrt{50}} > 1.676$.
All statement of statistical significance in the remainder refer to a 95\% level of significance under $t$-test.

\begin{table*}[t]
\small
\footnotesize
\scriptsize
%\tiny
%\renewcommand{\captionfont}{\scriptsize}
   \caption{
     \normalsize  Comparison of average test accuracies of our \texttt{Conv-MKL} and \texttt{SMSD-MKL} with the others. %including
%    the linear multi-class SVM (LMC),
%    One-against-One (One vs. One),
%    One-against-the-Rest(One vs. Rest),
%    Generalized Minimal Norm Problem solver (GMNP),
%    the $\ell_1$-norm Multiclass MKL ($\ell_1$ MC-MKL),
%    the $\ell_2$-norm Multiclass MKL ($\ell_2$ MC-MKL),
%    and mixed-norm MKL solved by stochastic gradient descent (UFO-MKL).
    We bold the numbers of the best method and underline the numbers of the other methods
    which are not significantly worse than the best one.
   %We set the numbers of our method (EPSVM) to be bold if our method outperforms all other methods (KTA, CKTA, FSM, 3-CV, 5-CV, and 10-CV).
   }
   \label{tabel:accuracy}
    %\centering
    \begin{tabular*}{\linewidth}{@{\extracolsep{-0.24cm}}lccccccccc}
    \toprule
                   &\texttt{Conv-MKL}          &\texttt{SMSD-MKL}         &LMC                 & One vs. One              & One vs. Rest                                              & GMNP                      & $\ell_1$ MC-MKL          & $\ell_2$ MC-MKL    & UFO-MKL                  \\ \hline
plant              &77.14$\pm$2.25             & \textbf{78.01$\pm$2.17}  & 70.12$\pm$2.96     & 75.83$\pm$2.69           &75.17$\pm$2.68       &75.42$\pm$3.64    & \underline{77.60$\pm$2.63}&75.49$\pm$2.48            &76.77$\pm$2.42\\
psortPos           &74.41$\pm$3.35             & \textbf{76.23$\pm$3.39}  &63.85$\pm$3.94      &73.33$\pm$4.21            &71.70$\pm$4.89       & 73.55$\pm$4.22   &71.87$\pm$4.87             &70.70$\pm$4.89            &74.56$\pm$4.04\\
psortNeg           &74.07$\pm$2.16             & \textbf{74.66$\pm$1.90}  &57.85$\pm$2.49      &73.74$\pm$2.87            &71.94$\pm$2.50   &\underline{74.27$\pm$2.51}   &72.83$\pm$2.20                             &72.42$\pm$2.65      &73.80$\pm$2.26 \\
nonpl              & \textbf{79.15$\pm$1.51}   &78.69$\pm$1.58            &75.16$\pm$1.48      &77.78$\pm$1.52            &77.49$\pm$1.53 &78.35$\pm$1.46    &77.89$\pm$1.79             &77.95$\pm$1.64            &78.07$\pm$1.56\\
sector             &\underline{92.83$\pm$2.62} & \textbf{93.39$\pm$0.70}  &93.16$\pm$0.66      &90.61$\pm$0.69            &91.34$\pm$0.61                   &$\setminus$                         &$\setminus$                   &92.15$\pm$2.57            &92.60$\pm$0.47\\
segment            &96.79$\pm$0.91             & \textbf{97.62$\pm$0.83}  &95.07$\pm$1.11      &97.08$\pm$0.61            &97.02$\pm$0.80       &96.87$\pm$0.80    &96.98$\pm$0.64             &\underline{97.58$\pm$0.68}&97.20$\pm$0.82\\
vehicle            &\textbf{79.35$\pm$2.27}             & 77.28$\pm$2.78           &75.61$\pm$3.56      &78.72$\pm$1.92            &79.11$\pm$1.94       &81.57$\pm$2.24    &74.96$\pm$2.93             &76.27$\pm$3.15            &76.92$\pm$2.83\\
vowel              &98.82$\pm$1.19             &\textbf{98.83$\pm$5.57}   &62.32$\pm$4.97      &98.12$\pm$1.76            &98.22$\pm$1.83       &97.04$\pm$1.85    &98.27$\pm$1.22             &97.86$\pm$1.75            &98.22$\pm$1.62\\
wine               &\textbf{99.63$\pm$0.96}    &\textbf{99.63$\pm$0.96}   &97.87$\pm$2.80      &97.24$\pm$3.05            &98.14$\pm$3.04       &97.69$\pm$2.43    &98.61$\pm$1.75             &98.52$\pm$1.89            &99.44$\pm$1.13            \\
dna                &96.08$\pm$0.83             &\textbf{96.30$\pm$0.79}   &92.02$\pm$1.50      &95.89$\pm$0.56            &95.61$\pm$0.73       &94.60$\pm$0.94    &\underline{96.27$\pm$0.68}             &95.06$\pm$0.92            &95.84$\pm$0.61\\
glass              &\textbf{75.19$\pm$5.05}    & 73.72$\pm$5.80           &63.95$\pm$6.04      &71.98$\pm$5.75            &70.00$\pm$5.75       &71.24$\pm$8.14    &69.07$\pm$8.08             &74.03$\pm$6.41            &72.46$\pm$6.12\\
iris               &\underline{96.67$\pm$2.94} &\textbf{97.00$\pm$2.63}   &88.00$\pm$7.82      &95.93$\pm$3.25            &95.87$\pm$3.20       &95.40$\pm$7.34    &95.40$\pm$6.46             &94.00$\pm$7.82            &95.93$\pm$2.88\\
svmguide2          &82.69$\pm$5.65             &\textbf{85.17$\pm$3.83}   &81.10$\pm$4.15      &\underline{84.79$\pm$3.45}&\underline{84.27$\pm$3.03}  &81.77$\pm$3.45    &83.16$\pm$3.63             &\underline{83.84$\pm$4.21}            &82.91$\pm$3.09\\
satimage           &91.64$\pm$0.88             &\underline{91.78$\pm$0.82}&84.95$\pm$1.15      &90.67$\pm$0.91            &89.29$\pm$0.96       &89.97$\pm$0.81    &\underline{91.86$\pm$0.62} &90.43$\pm$1.27            &\textbf{91.92$\pm$0.83}\\
\bottomrule
\end{tabular*}
\vspace{-0.5cm}
\end{table*}

The average test accuracies are reported in Table \ref{tabel:accuracy}.
The results show:
1) Our methods \texttt{Conv-MKL} and \texttt{SMSD-MKL} give best results on nearly all datasets
except \textit{vehicle} and \textit{satimage};
2) \texttt{SMSD-MKL} is better than \texttt{Conv-MKL} because it wins on 2/3 datasets;
3) Compared with typical MKL methods,
our methods get better results over almost all datasets except that only UFO-MKL works slightly better than ours on \textit{satige};
4) The MKL methods usually work better than the compared single kernel methods (One vs. One, One vs. Rest and GMNP);
%but \textit{vehicle} leads fairly good results in single kernel methods but worse results in MKL methods;
5) The kernel classification methods have better performance than the linear classification machine (LMC) on all datasets.

The above results show that the use of the local Rademacher complexity can significantly improve
the performance of multi-class multiple kernel learning algorithms,
which conforms to our theoretical analysis.

%\textbf{Note}: Source code is attached in Supplementary Material.

\section{Conclusion}
In this paper, we studied the generalization performance of multi-class classification,
and derived a sharper data dependent generalization error bound using the local Rademacher complexity,
which is much sharper than existing data-dependent generalization bounds of multi-class classification.
Then, we designed two algorithms with statistical guarantees and fast convergence rates:
\texttt{Conv-MKL} and \texttt{SMSD-MKL}.
%a convex optimization way based on any existing MC-MKL algorithms,
%and the other that put local Rademacher complexity in optimization formulation,
%for which we give a stochastic mirror and sub-gradient descent method.
%Empirical results show our methods outperform the state-of-the-art multi-class classification methods.
Based on local Rademacher complexity, our analysis can be used as a solid basis for the
design of new multi-class kernel learning algorithms.

%In our future, we will extend our result for other multi-class classification of the loss function,
%such as cross-entropy loss.
%is a widely used loss in deep learning, while for other methods, margin loss is still a widely used loss.
%To obtain theoretical results, we only assume that the loss is \zeta-smooth, did not use lots of information about the margin loss. Thus, we believe that it is not very hard to extend our results to cross-entropy loss.
\section*{Acknowledgments}
This work is supported in part by
the National Key Research and Development Program of China (No.2016YFB1000604),
the Science and Technology  Project of Beijing (No.Z181100002718004),
the National Natural Science Foundation of China (No.6173396, No.61673293, No.61602467)
 and the Excellent Talent Introduction of Institute of Information Engineering of CAS (Y7Z0111107).

\bibliographystyle{abbrv}
%\bibliographystyle{unsrt}
\bibliography{MC_MKL_LR}


%\bibliography{NIPS2018}
\end{document}
