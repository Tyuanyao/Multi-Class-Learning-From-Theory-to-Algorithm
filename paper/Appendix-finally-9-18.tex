\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{nips_2018}
\usepackage{algorithm}
\usepackage{algorithmic}
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,amsfonts}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

%\newcommand{\ra}{{\lfloor n/2\rfloor}}
%\newcommand{\nra}{{\lfloor \frac{n}{2}\rfloor}}
%%\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
%%\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\argsup}{\operatornamewithlimits{arg\,sup}}
%\usepackage[noend]{algpseudocode}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm
%\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\title{Appendix: Multi-Class Learning: From Theory to Algorithm}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  %David S.~Hippocampus\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 \\
%  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Appendix A : Proof of Theorem \ref{rademacherlocal}}
\begin{theorem}
\label{rademacherlocal}
With probability at least $1-\delta$,
  we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r) \leq \frac{c_{d,\vartheta}\xi(K)\sqrt{\zeta r}\log^{\frac{3}{2}}(n)}{\sqrt{n}}+\frac{4\log(1/\delta)}{n},
  \end{align*}
  where
  \begin{align*}
  \xi(K)=
  \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } q\geq 2\log K,\\
      &(2q)^{1+\frac{1}{q}}K^{\frac{1}{q}}, &&\text{otherwise},
      \end{aligned}
      \right.
  \end{align*}
  $c_{d,\vartheta}$ is a constant depends on $d$ and $\vartheta$.
\end{theorem}
To prove theorem \ref{rademacherlocal}, we first introduce the following two lemmas.
\begin{lemma}
\label{hatRLlemma}\cite{lei2015multi}
Suppose that $\mathcal{L}$ is defined in equation \textrm{(2)} in the paper,
then we have
%Suppose that the hypothesis space $\mathcal{H}_{p,\kappa}$ is defined by
% \begin{align*}
%  \left\{ h_\mathbf{w}=\left(\langle \mathbf w_1,\phi(\mathbf x)\rangle,\ldots, \langle\mathbf w_K,\phi(\mathbf x)\rangle\right):
%   \left\|\mathbf  w \right\|_{2,p}\leq 1
%   \right\}
% \end{align*}
% and
% \begin{align*}
%  \mathcal{L}=\left\{\ell_h:=\ell(\rho_h(z)):h\in\mathcal{H}_{p,\kappa}\right\},
%\end{align*}
%we have
\begin{align*}
  \hat{\mathcal{R}}(\mathcal{L})\leq\frac{\sqrt{2\pi}}{n}\mathbb{E}_{\mathbf g}\sup_{h=(h_1,\ldots,h_K)
  \in\mathcal{H}_{p,\kappa}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}h_j(\mathbf x_i),
\end{align*}
 where $g_1,\ldots,g_{nK}$ are  independent $N(0,1)$ random variables.
\end{lemma}
\begin{proof}
The empirical Gaussian complexities of $\mathcal{H}_{p,\kappa}$ is denote as
\begin{align*}
  \hat{\mathcal{G}}(\mathcal{H}_{p,\kappa})=\mathbb{E}_{\bm g}\left[\sup_{h\in \mathcal{H}_{p,\kappa}}\frac{1}{n}\sum_{i=1}^ng_{i}h(\mathbf x_i)\right],
\end{align*}
where $g_1,\ldots, g_n$ are independent $N(0,1)$ random variables.
For any $\gamma>0$, let  $\rho_{\gamma,h}(\mathbf x,y)$ be
  \begin{align*}
    \rho_{\gamma,h}(\mathbf x,y)&=h(\mathbf x,y)-\max_{y'\in\mathcal{Y}}[h(\mathbf x,y')-\gamma1_{y'=y}]\\
    &=\min_{y'\in\mathcal{Y}}[h(\mathbf x,y)-h(\mathbf x,y')+\gamma1_{y'=y}].
  \end{align*}
  It is easy to checked that $\rho_{\gamma,h}(\mathbf x,y)=\min(\rho_h(\mathbf x,y),\gamma)$.
  For the fixed parameter $\gamma=c_\ell$, we observe that $\rho_{\gamma,h}(\mathbf x,y)=\min(\rho_h(\mathbf x,y),c_\ell)$.
  If $\rho_h(\mathbf x,y)>c_\ell$,
  we get
  \begin{align*}
    \ell(\rho_{\gamma,h}(\mathbf x,y))=\ell(c_\ell)=0=\ell(\rho_h(\mathbf x,y)).
  \end{align*}
  Thus, we have $\rho_{\gamma,h}(\mathbf x,y)=\rho_h(\mathbf x,y)$.
  Therefore, for any $z=(\mathbf x,y)\in\mathcal{Z}$ we have $\ell(\rho_{\gamma,h}(\mathbf x,y))=\ell(\rho_h(\mathbf x,y))$,
  and
  \begin{align*}
    \mathcal{L}_\gamma:=\{\rho_{\gamma,h}(\mathbf x,y):h\in\mathcal{H}_{p,\kappa}\}=\mathcal{L}.
  \end{align*}
  Thus,  $\mathcal{L}_\gamma$ satisfies the following inequality:
  \begin{align*}
    &\hat{\mathcal{R}}(\mathcal{L})=\hat{\mathcal{R}}(\mathcal{L}_\gamma)\\&=\frac{1}{n}\mathbb{E}_{\bm \sigma}
        \sum_{i=1}^n\sigma_i\left(h(\mathbf x_i,y_i)-\max_{y\in\mathcal{Y}}(h(\mathbf x_i,y_i)-\gamma 1_{y=y_i})\right)\\
    &\leq\frac{1}{n}\mathbb{E}_{\bm \sigma}\sup_{h\in\mathcal{H}_{p,\kappa}}\left[\sum_{i=1}^n\sigma_ih(\mathbf x_i,y_i)\right]+
    \frac{1}{n}\mathbb{E}_{\bm \sigma}\left[\sup_{h\in\mathcal{H}_{p,\kappa}}\sum_{i=1}^n\sigma_i\max_{y\in Y}(h(\mathbf x_i,y_i)-\gamma 1_{y=y_i})\right]\\
    &=\hat{\mathcal{R}}(\mathcal{H}_{p,\kappa})+\mathbb{E}_{\bm \sigma}\left[\sup_{h\in\mathcal{H}_{p,\kappa}}\frac{1}{n}\sum_{i=1}^n\sigma_i\max_{y\in Y}(h(\mathbf x_i,y_i)-\gamma 1_{y=y_i})\right]
   % &\leq \sqrt{\frac{\pi}{2}}\hat{\mathcal{G}}(\mathcal{L})+\\
   % &~~\frac{1}{n}\sqrt{\frac{\pi}{2}}\underbrace{\mathbb{E}_{\bm g}\left[g_i\max(h_1(\mathbf x_i)-\gamma 1_{y_i=1},\ldots,h_c(\mathbf x_i)-\gamma 1_{y_i=c})\right]}_{:=A_1}.
  \end{align*}
  From \cite{ledoux2013probability}, we know that
  \begin{align*}
    \hat{\mathcal{R}}(\mathcal{H}_{p,\kappa})\leq \sqrt{\frac{\pi}{2}}\hat{\mathcal{G}}(\mathcal{H}_{p,\kappa}).
  \end{align*}
  According to the Lemma 4 of \cite{lei2015multi}, we have
  \begin{align*}
    \hat{\mathcal{G}}(\mathcal{H}_{p,\kappa})
    \leq \frac{1}{n}\mathbb{E}_{\bm g}\sup_{h=(h_1,\ldots,h_K)\in H}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}h_j(\mathbf x_i)
  \end{align*}
  Thus, we have
  \begin{align*}
    &\hat{\mathcal{R}}(\mathcal{L}) \leq \frac{1}{n}\sqrt{\frac{\pi}{2}}\mathbb{E}_{\bm g}\sup_{h=(h_1,\ldots,h_K)\in H}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}h_j(\mathbf x_i)+\\
    &~~~~~\frac{1}{n}\sqrt{\frac{\pi}{2}}\underbrace{\mathbb{E}_{\bm g}\left[g_i\max(h_1(\mathbf x_i)-\gamma 1_{y_i=1},\ldots,h_c(\mathbf x_i)-\gamma 1_{y_i=c})\right]}_{:=A_3}.
  \end{align*}
 % According to the Lemma 4 of \cite{lei2015multi},
%  %for all $h=(h_1,\ldots,h_K)$,
%  \begin{align*}
%    &\hat{G}\left(\max\{h_1,\ldots,h_c\}:h=(h_1,\ldots,h_K)\right)\\&\leq \frac{1}{n}\mathbb{E}_{\bm g}\sup_{h=(h_1,\ldots,h_K)\in H}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}h_j(\mathbf x_i)
%  \end{align*}
In the next, we bound $A_3$:
  \begin{align*}
    A_3&\leq\mathbb{E}_{\bm g}\sup_{h=(h_1,\ldots,h_K)\in \mathcal{H}_{p,\kappa}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}\left(h_j(\mathbf x_i)-\gamma 1_{y_i=c}\right)\\
       &=\mathbb{E}_{\bm g}\sup_{h=(h_1,\ldots,h_K)\in \mathcal{H}_{p,\kappa}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}\left(h_j(\mathbf x_i)\right)
       -\underbrace{\mathbb{E}_{\bm g}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}\gamma 1_{y_i=j}}_{=0}\\
       &=\mathbb{E}_{\bm g}\sup_{h=(h_1,\ldots,h_K)\in \mathcal{H}_{p,\kappa}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}\left(h_j(\mathbf x_i)\right).
  \end{align*}
  With this inequality, we immediately derive the following bound on $\hat{\mathcal{R}}(\mathcal{L})$:
  \begin{align*}
    \hat{\mathcal{R}}(\mathcal{L})\leq \frac{\sqrt{2\pi}}{n}\mathbb{E}_{\mathbf g}\sup_{h=(h_1,\ldots,h_K)
  \in\mathcal{H}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}h_j(\mathbf x_i).
  \end{align*}
  \end{proof}

\begin{lemma}\cite{lei2015multi}
  \label{lemma-3e}
  Let $g$ be $N(0,1)$ distributed. For any $p>0$, the $p$-th moment of $g$ can be bounded by
  \begin{align}
  \label{Eqp}
    \left[\mathbb{E}|g|^p\right]^{\frac{1}{p}}\leq (2p)^{\frac{1}{2}+\frac{1}{p}}.
  \end{align}
\end{lemma}
\begin{proof}
  %In the next, we will bound $\left[\mathbb{E}_{g_1}|g_1|^{q^\ast}\right]^{\frac{1}{q^\ast}}$.
  Let $\Gamma(n)=(n-1)!$ be the Gamma function.
  The $p$-th moment of a $N(0,1)$ distributed random variable can be exactly expressed via Gamma function \cite{winkelbauer2012moments}:
  \begin{align*}
    \left[\mathbb{E}_{g}|g|^{p}\right]^{\frac{1}{p}}&=\frac{2^{\frac{p}{2}}}{\sqrt{\pi}}\Gamma\left(\frac{p+1}{2}\right)
    %&\leq \frac{2^{\frac{p}{2}}}{\sqrt{\pi}}\Gamma\left(\left\lceil\frac{p+1}{2}\right\rceil\right)
    \leq \frac{2^{\frac{p}{2}}}{\sqrt{\pi}}\left\lceil\frac{p-1}{2}\right\rceil!.
    %&\overset{\text{\cite{robbins1955remark}}}{\leq}
%     \frac{2^{\frac{q^\ast}{2}}}{\sqrt{\pi}}\sqrt{2\pi}\left\lceil\frac{p-1}{2}\right\rceil^{\left\lceil\frac{p-1}{2}\right\rceil!+\frac{1}{2}}
  \end{align*}
  According to the Stirling's approximation in \cite{robbins1955remark},
  we can obtain that
  \begin{align*}
    \left[\mathbb{E}_{g}|g|^{p}\right]^{\frac{1}{p}}
    &\leq \frac{2^{\frac{p}{2}}}{\sqrt{\pi}}\left\lceil\frac{p-1}{2}\right\rceil!\\
    &\overset{\text{Stirling's approximation}}{\leq} \frac{2^{\frac{p}{2}}}{\sqrt{\pi}}\sqrt{2\pi}\left\lceil\frac{p-1}{2}\right\rceil^{\left\lceil\frac{p-1}{2}\right\rceil!+\frac{1}{2}}\\
    &\leq (2p)^{\frac{p}{2}+1}.
  \end{align*}
\end{proof}

\begin{proposition}\cite{lei2015multi}
  \label{Lemma-empRad}
Suppose that $\mathcal{L}$ is defined in equation \textrm{(2)} in the paper,
then we have
  \begin{align*}
    \hat{\mathcal{R}}(\mathcal{L})\leq \frac{\sqrt{\vartheta}}{\sqrt{n}}\times
      \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } p^\ast\geq 2\log K,\\
      &(2p^\ast)^{1+\frac{1}{p^\ast}}K^{\frac{1}{p^\ast}}, &&\text{otherwise.}
      \end{aligned}
      \right.
  \end{align*}
\end{proposition}
\begin{proof}
From Lemma \ref{hatRLlemma}, we know that
\begin{align}
\label{eq-middle-mi}
  \hat{\mathcal{R}}(\mathcal{L})\leq \frac{\sqrt{2\pi}}{n}\underbrace{\mathbb{E}_{\mathbf g}\sup_{h=(h_1,\ldots,h_K)
  \in\mathcal{H}_{p,\kappa}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}h_j(\mathbf x_i)}_{:=A_1},
\end{align}
  where $g_1,\ldots,g_{nK}$ are  independent $N(0,1)$ random variables.
  In the next, we will bound $A_1$.
  To this end,
 we denote by $\|\cdot\|_\ast$ dual norm of $\|\cdot\|$, i.e.,
$$\|w\|_\ast:=\sup_{\| w'\|\leq 1}\langle w, w'\rangle.$$
For a convex function $f$, we denote by $f^\ast$ its Fenchel
conjugate, i.e., $$f^\ast(\nu):=\sup_{w}\left[\langle w,\nu\rangle-f(w)\right].$$
  From Corollary 4 in \cite{Theodoros2000regularization}, we know that,
  if $f$ is $\eta$-strongly convex w.r.t $\|\cdot\|$ and $f^\ast(\mathbf 0)=0$, then for any sequence
  $\nu_1,\ldots,\nu_n$ and for any $\mu$ we have
  \begin{align*}
   % \label{lemmab2}
    \sum_{i=1}^n\langle \nu_i,\mu\rangle -f(\mu) \leq
    \sum_{i=1}^n\langle \nabla f^\ast(\nu_{i:i-1}),\nu\rangle +\frac{1}{2\eta}\sum_{i=1}^n\|\nu_i\|_\ast^2,
  \end{align*}
  where $\nu_{1:i}$ denotes the sum $\sum_{j=1}^i\nu_j$.
  Let $q$ be any number satisfying $p\leq q\leq 2$.
  Introduce the function $f_q(\mathbf w)=\frac{1}{2}\|\mathbf w\|_{2,q}^2$.
  It is easy to verify that
  \begin{align*}
    f_q(\mathbf w)=\frac{1}{2}\|\mathbf w\|_{2,q}^2\leq\frac{1}{2}\|\mathbf w\|_{2,p}^2\leq  \frac{1}{2}.
  \end{align*}
  Since  $f_q(\mathbf w)$ is $\frac{1}{q^\ast}$-strongly convex w.r.t  $\|\cdot\|_{2,q^\ast}$.
  Let
  \begin{align*}
    &\nu_i= (g_i\phi(\mathbf x_i),g_{n+i}\phi(\mathbf x_i),\ldots, g_{(K-1)n+i}\phi(\mathbf x_i)),\\
    &\mu=(\mathbf w_1,\ldots,\mathbf w_K),
  \end{align*}
  we can  obtain that
  \begin{align*}
    &\lambda\sup_{h\in \mathcal{H}_{q,\kappa}}\sum_{i=1}^n\sum_{j=1}^cg_{(j-1)n+i}\langle \mathbf w_j, \phi(\mathbf x_i)\rangle\\
    &=\sup_{h\in\mathcal{H}_{q,\kappa}}\sum_{i=1}^n\langle(\mathbf w_1,\ldots,\mathbf w_K),\lambda\nu_i\rangle\\
    &\leq \sup_{h\in\mathcal{H}_{q,\kappa}} f_q(\mathbf w)+\sum_{i=1}^n\langle f^\ast(\nu_{1:i-1}),\lambda\nu_i\rangle+\sum_{i=1}^n\frac{q^\ast\lambda^2}{2}\|\nu_i\|_{2,q^\ast}^2.
  \end{align*}
  Taking expectation on both sides w.r.t. $g_1,\ldots,g_{nK}$,
  we can obtain that
  \begin{align*}
    &\mathbb{E}_{\bm g}\sup_{h\in\mathcal{H}_{q,\kappa}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}\langle \mathbf w_j, \phi(\mathbf x_i)\rangle\\
    \nonumber
    &\leq\frac{1}{2\lambda}+\underbrace{\mathbb{E}_{\bm g}\sum_{i=1}^n\langle f^\ast(\nu_{1:i-1}),\nu_i\rangle}_{=0}+
    \sum_{i=1}^n\frac{q^\ast\lambda}{2}\|\nu_i\|_{2,q^\ast}^2\\
    &=\frac{1}{2\lambda}+\frac{q^\ast\lambda}{2}\sum_{i=1}^n\|\nu_i\|_{2,q^\ast}^2.
    %\frac{q^\ast\lambda}{2}\underbrace{\sum_{i=1}^n\mathbb{E}_{\bm g}\|(g_i\phi(\mathbf x_i),g_{n+i}\phi(\mathbf x_i),\ldots, g_{(K-1)n+i}\phi(\mathbf x_i))\|_{2,q^\ast}^2}_{:=B}.
  \end{align*}
  Choosing  $\lambda=\frac{1}{\sqrt{q^\ast\sum_{i=1}^n\|\nu_i\|_{2,q^\ast}^2}}$,
  the above inequality translates to
  \begin{align}
   \label{eq-mid-he}
  \nonumber
    &\mathbb{E}_{\bm g}\sup_{h\in\mathcal{H}_{q,\kappa}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}\langle \mathbf w_j, \phi(\mathbf x_i)\rangle\\
    \nonumber
    &\leq \sqrt{q^\ast\sum_{i=1}^n\|\nu_i\|_{2,q^\ast}^2}\\
    &\leq \sqrt{q^\ast\underbrace{\sum_{i=1}^n\mathbb{E}_{\bm g}\|(g_i\phi(\mathbf x_i),\ldots, g_{(K-1)n+i}\phi(\mathbf x_i))\|_{2,q^\ast}^2}_{:=A_2}}.
  \end{align}
  %Let $A_2:=\sum_{i=1}^n\mathbb{E}_{\bm g}\|(g_i\phi(\mathbf x_i),g_{n+i}\phi(\mathbf x_i),\ldots, g_{(K-1)n+i}\phi(\mathbf x_i))\|_{2,q^\ast}^2$.
  In the next, we bound $A_2$:
  \begin{align*}
    A_2&=\sum_{i=1}^n\mathbb{E}_{\bm g}\left[\sum_{j=1}^K\|g_{(j-1)n+i}\phi(\mathbf x_i)\|_2^{q^\ast}\right]^{\frac{2}{q^\ast}}\\
    &=\sum_{i=1}^n\mathbb{E}_{\bm g}\left[\sum_{j=1}^K|g_{(j-1)n+i}|^{q^\ast}\right]^{\frac{2}{q^\ast}}k(\mathbf x_i,\mathbf x_i)\\
    &\overset{\text{symmetry}}{=}\mathbb{E}_{\bm g}\left[\sum_{j=1}^K|g_{j}|^{q^\ast}\right]^{\frac{2}{q^\ast}}\sum_{i=1}^nk(\mathbf x_i,\mathbf x_i)\\
    &\overset{\text{Jensen's Inequality}}{\leq}
    \left[\mathbb{E}_{\bm g} \sum_{j=1}^K|g_{j}|^{q^\ast}\right]^{\frac{2}{q^\ast}}\sum_{i=1}^nk(\mathbf x_i,\mathbf x_i)\\
    &=\left[K\mathbb{E}_{g_1}|g_1|^{q^\ast}\right]^{\frac{2}{q^\ast}}\sum_{i=1}^nk(\mathbf x_i,\mathbf x_i)\\
    &\leq K^{\frac{2}{q^\ast}}\left[\mathbb{E}_{g_1}|g_1|^{q^\ast}\right]^{\frac{2}{q^\ast}}n\vartheta.
  \end{align*}
  Substituting the above inequality in \textrm{(10)} in the paper, we have
  \begin{align*}
    \mathbb{E}_{\bm g}\sup_{h\in \mathcal{H}_{q,\kappa}}\sum_{i=1}^n\sum_{j=1}^Kg_{(j-1)n+i}\langle \mathbf w_j,\lambda \phi(\mathbf x_i)\rangle
    \leq  K^{\frac{1}{q^\ast}}\sqrt{q^\ast}\left[\mathbb{E}_{g_1}|g_1|^{q^\ast}\right]^\frac{1}{q^\ast}\sqrt{n\vartheta}.
  \end{align*}
  From the trivial inequality $\|\mathbf w\|_{2,p}\geq \|\mathbf w\|_{2,q}$,
  we know that $\mathcal{H}_{p,\kappa}\subset \mathcal{H}_{q,\kappa}$.
  Combining the above  equation and \textrm{(9)} in the paper,
  we have
  \begin{align*}
    \hat{\mathcal{R}}(\mathcal{L})\leq\inf_{p\leq q\leq 2}\frac{\sqrt{2\vartheta\pi}}{\sqrt{n}} K^{\frac{1}{q^\ast}}\sqrt{q^\ast}
    \left[\mathbb{E}_{g_1}|g_1|^{q^\ast}\right]^\frac{1}{q^\ast}.
  \end{align*}
  It can be directly checked that the function $t\rightarrow \sqrt{t}c^{1/t}$ is decreasing along the interval $(0,2\log K)$
  and increasing along the interval $(2\log K,\infty)$.
  Therefore, if $p^\ast\geq 2\log K$,
  \begin{align}
  \label{eq-hai}
    \nonumber\hat{\mathcal{R}}(\mathcal{L})&\leq \frac{\sqrt{2\vartheta\pi}}{\sqrt{n}} K^{\frac{1}{2\log K }}\sqrt{2\log K}
     \left[\mathbb{E}_{g_1}|g_1|^{2\log K}\right]^\frac{1}{2\log K}\\
     \nonumber &\overset{\text{Lemma \ref{lemma-3e}}}\leq \frac{\sqrt{2\vartheta\pi}}{\sqrt{n}} K^{\frac{1}{2\log K }}\sqrt{2\log K}(4\log K)^{\frac{1}{2}+\frac{1}{\log K}}\\
     &\leq \frac{\sqrt{e\vartheta\pi}}{\sqrt{n}}(\log 4K)^{1+\frac{1}{2\log K}}.
  \end{align}
  If $0<p^\ast\leq 2\log K$,
  we have
  \begin{align}
  \label{eq-haihello}
    \nonumber \hat{\mathcal{R}}(\mathcal{L})&\leq \frac{\sqrt{2\vartheta\pi}}{\sqrt{n}} K^{\frac{1}{p^\ast}}\sqrt{p^\ast}
    \left[\mathbb{E}_{g_1}|g_1|^{p^\ast}\right]^\frac{1}{p^\ast}\\
    &\overset{\text{Lemma \ref{lemma-3e}}}\leq \frac{\sqrt{\vartheta\pi}}{\sqrt{n}} K^{\frac{1}{p^\ast}}(2p^\ast)^{1+\frac{1}{p^\ast}}.
  \end{align}
  Combining \eqref{eq-hai} and \eqref{eq-haihello} completes the proof.
\end{proof}

\begin{proof}[Proof of Theorem \ref{rademacherlocal}]
  According to the Lemma 3.6 of \cite{oneto2013improved},
  with probability $1-\delta$, we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r)\leq \hat{\mathcal{R}}(\mathcal{L}^r)+\sqrt{\frac{2\log(1/\delta)\mathcal{R}(\mathcal{L}^r)}{n}}.
  \end{align*}
  Note that $\forall a,b\geq 0, \sqrt{ab}\leq \frac{a}{2}+\frac{b}{2}$.
  Thus, we have
  \begin{align*}
    \mathcal{R}(\mathcal{L}^r)\leq \hat{\mathcal{R}}(\mathcal{L}^r)+\frac{\mathcal{R}(\mathcal{L}^r)}{2}+\frac{\log(1/\delta)}{n}.
  \end{align*}
  So, we can obtain that
  \begin{align}
  \label{eqRhatRnew}
    \mathcal{R}(\mathcal{L}^r)\leq 2\hat{\mathcal{R}}(\mathcal{L}^r)+\frac{2\log(1/\delta)}{n}
  \end{align}
  From the Lemma 2.2 of \cite{Srebro2010lrc},
  we know that if $\ell$ is a $\zeta$-smooth loss function,
  \begin{align}
   \label{RlocalRnew}
    \hat{\mathcal{R}}(\mathcal{L}^r)\leq c_d\sqrt{\zeta r}\log^{\frac{3}{2}}(n)\hat{\mathcal{R}}(\mathcal{L}),
  \end{align}
  where $c_d$ is a constant depends on $d$.
  Substituting  \eqref{RlocalRnew} into \eqref{eqRhatRnew},
  we have
  \begin{align}
  \label{eq-middle-result-hatR}
    \hat{\mathcal{R}}(\mathcal{L}^r)\leq 2c_d\sqrt{\zeta r}\log^{\frac{3}{2}}(n)\hat{\mathcal{R}}(\mathcal{L})
    +\frac{2\log(1/\delta)}{n}.
  \end{align}
  From Proposition  \ref{Lemma-empRad},
  we have
  \begin{align*}
    \hat{\mathcal{R}}(\mathcal{L})\leq \frac{1}{\sqrt{n}}\times
      \left\{
      \begin{aligned}
      &\sqrt{e\vartheta}(4\log K)^{1+\frac{1}{2\log K}}, &&\text{if } q\geq 2\log K,\\
      &\sqrt{\vartheta}(2q)^{1+\frac{1}{q}}c^{\frac{1}{q}}, &&\text{otherwise.}
      \end{aligned}
      \right.
  \end{align*}
  Substituting the above result into \eqref{eq-middle-result-hatR},
  the proof is over.
%  the proof is over.
\end{proof}
%\subsection{Appendix: Proof of Proposition \ref{Lemma-empRad}}


\section{Appendix B: Proof of Theorem \ref{theorem-finally}}
\begin{theorem}
\label{theorem-finally}
  If $\ell$ is a $\zeta$-smooth loss.
  Then, $\forall h\in\mathcal{H}_{p,\kappa}$ and $\forall k>\max(1,\frac{\sqrt{2}}{2d})$,  with probability
  $1-\delta$, we have
  \begin{align*}
   L(h)\leq \max\left\{
        \frac{k}{k-1}\hat{L}(\ell_h),
       \hat{L}(\ell_h)+\frac{c_{d,\vartheta,\zeta, k}\xi^2(K) \log^3 n}{n}+\frac{c_{\delta}}{n}
     \right\},
\end{align*}
where \begin{align*}
  \xi(K)=
  \left\{
      \begin{aligned}
      &\sqrt{e}(4\log K)^{1+\frac{1}{2\log K}}, &\text{if } q\geq 2\log K,\\
      &(2q)^{1+\frac{1}{q}}K^{\frac{1}{q}}, &\text{otherwise},
      \end{aligned}
      \right.
  \end{align*}
  $c_{d,\vartheta}$ is a constant depending on $d,\vartheta,\zeta, k$,
  and $c_{\delta}$ is a constant depending on $\delta$.
\end{theorem}

To prove Theorem \ref{theorem-finally}, we first introduce the following two lemmas.
\begin{lemma}
\label{the-ori-bound}
   Let $\bar{\mathcal{L}}$ be the normalized loss space
  \begin{align}
    \label{eq-ell-r}
    \bar{\mathcal{L}}=\left\{
        \frac{r}{\max(L(\ell_h^2),r)}\ell_h\Big| \ell_h\in \mathcal{L}
    \right\}.
  \end{align}
   Suppose that $\forall k>1$,
   \begin{align*}
     \hat{U}_n({\bar{\mathcal{L}}})
     :=\sup_{\bar{\ell}_h\in\bar{\mathcal{L}}}\left\{L(\bar{\ell}_h)-\hat{L}(\bar{\ell}_h)\right\}
     \leq \frac{r}{Mk}.
   \end{align*}
   Then, $\forall h\in\mathcal{H}_{p,\kappa}$, we have
   \begin{align*}
     L(\ell_h)&\leq \max\left\{
        \left(\frac{k}{k-1}\hat{L}(\ell_h)
        \right),
        \left(\hat{L}(\ell_h)+\frac{r}{Mk}
        \right)
     \right\}.
   \end{align*}
\end{lemma}
\begin{proof}
  Note that, $\forall \bar{\ell}_h\in{\bar{\mathcal{L}}}$:
  \begin{align}
    \label{eq-proof-lr-hatlr}
    L(\bar{\ell}_h)\leq \hat{L}_{n}(\bar{\ell}_h)
    +\hat{U}_n({\bar{\mathcal{L}}})\leq \hat{L}_n(\bar{\ell}_h)+\frac{r}{Mk}.
  \end{align}
  Let us consider the two cases:
  \begin{itemize}
    \item[1)] $L(\ell_h^2)\leq r$, $\ell_h\in\mathcal{L}$.
    \item[2)] $L(\ell_h^2)>r$, $\ell_h\in\mathcal{L}$.
  \end{itemize}
  In the first case $\bar{\ell}_h=\ell_h$,
  by \eqref{eq-proof-lr-hatlr},
  we have
  \begin{align}
    \label{eq-Lell-bound}
    L(\ell_h)=L(\bar{\ell}_h)\leq \hat{L}_n(\bar{\ell}_h)+\frac{r}{Mk}
    =\hat{L}(\ell_h)+\frac{r}{Mk}.
  \end{align}
  In the second case, $\bar{\ell}_h=\frac{r}{L(\ell_h^2)}\ell_h$,
  then
  \begin{align}
    \label{eq-Lell-plus-hatl}
    \begin{aligned}
    L(\ell_h)-\hat{L}(\ell_h)
    \leq \hat{U}_n(\mathcal{L})
    =\frac{L(\ell_h^2)}{r}\hat{U}_n(\bar{\mathcal{L}})
    \leq\frac{M\cdot L(\ell_h)}{r}\frac{r}{Mk}=\frac{L(\ell_h)}{k}.
    \end{aligned}
  \end{align}
  %Note that $L(\ell_h)\equiv L(\ell_f)$ and $\hat{L}(\ell_h)\equiv\hat{L}_n(\ell_f)$.
  By combining the results of Eqs \eqref{eq-Lell-bound} and \eqref{eq-Lell-plus-hatl},
  the proof is over.
\end{proof}

\begin{lemma}
\label{lem-lrs-subset-lr}
  Suppose that $\bar{\mathcal{L}}$ is defined in Equation \eqref{eq-ell-r} in the paper,
  then
  \begin{align*}
    \bar{\mathcal{L}}\subseteq \mathcal{L}^r.
  \end{align*}
\end{lemma}
\begin{proof}
  Let us consider $\mathcal{L}^r$ in the two cases:
  \begin{itemize}
    \item[1)] $L(\ell_h^2)\leq r$, $\ell_h\in\mathcal{L}$.
    \item[2)] $L(\ell_h^2)>r$, $\ell_h\in\mathcal{L}$.
  \end{itemize}
  In the first case, $\bar{\ell}_h=\ell_h$ and then:
  \begin{align*}
    L(\ell_h^2)=L(\bar{\ell}_h^2)\leq r.
  \end{align*}
  In the second case, $L(\ell_h^2)>r$, so we have that
  \begin{align*}
    \bar{\ell}_h=
    \left[\frac{r}{L(\ell_h^2)}\right]\ell_h, \frac{r}{L(\ell_h^2)}\leq 1,
  \end{align*}
  and the following bound holds:
  \begin{align*}
    L(\bar{\ell}_h^2)= \left[\frac{r}{L(\ell_h^2)}
    \right]^2L(\ell_h^2)\leq \left[\frac{r}{L(\ell_h^2)}
    \right]L(\ell_h^2)= r.
  \end{align*}
  Thus, the lemma is proved.
\end{proof}
\begin{lemma}
  \label{lem-sub-root-Rademacher}
  $
    \psi(r)=\mathcal{R}(\mathcal{L}^r)
  $
  is a sub-root function.
\end{lemma}
\label{lemma-sub}
\begin{proof}
  In order to prove this lemma, we can show the following:
    1) $\psi_n(r)$ is positive;
    2) $\psi_n(r)$ is non-decrasing;
    3) $\psi_n(r)/\sqrt{r}$ is non-increasing.

  By the definition of $\mathcal{R}(\mathcal{L}^r)$,
  it is easy to verity that $\mathcal{R}(\mathcal{L}^r)$ is positive.

  Concerning the second property, we have that, for $0\leq r_1\leq r_2$:
  $
    \mathcal{L}^{r_1}\subseteq \mathcal{L}^{r_2},
  $
  therefore
  \begin{align*}
    \psi(r_1)&=\mathbb{E}_{\mathcal{S},\bm \sigma}
      \left[\sup_{\ell_h\in\mathcal{L}^{r_1}}\frac{2}{n}
      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)
          \right]\\
    &\leq \mathbb{E}_{\mathcal{S},\bm \sigma}
      \left[\sup_{\ell_h\in\mathcal{L}^{r_2}}\frac{2}{n}
      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)
          \right]=\psi(r_2).
  \end{align*}
  Finally, concerning the third property,
  for $0\leq r_1\leq r_2$,
  let
  \begin{align*}
    \ell_{h^{r_2}}
    =\argsup_{\ell_h\in\mathcal{L}^{r_2}}
    \mathbb{E}_{\mathcal{S},\bm \sigma}
      \left[\sup_{\ell_h\in\mathcal{L}^{r_2}}\frac{2}{n}
      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)
          \right].
  \end{align*}
  Note that, since $\frac{r_1}{r_2}\leq 1$,
  we have that $\sqrt{\frac{r_1}{r_2}}\ell_{h^{r_2}}\in \mathcal{L}^{r_2}$.
  Consequently:
  $
    L\left[
    \left(\sqrt{\frac{r_1}{r_2}}\ell_{h^{r_2}}\right)\right]^2
    =\frac{r_1}{r_2}L\left[(\ell_{h^{r_2}})^2\right]\leq r_1.
  $
  Thus, we have that:
  \begin{align*}
    \psi(r_1)&=\mathbb{E}_{\mathcal{S},\bm \sigma}
      \left[\sup_{\ell_h\in\mathcal{L}^{r_1}}\frac{2}{n}
      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)
          \right]\\
    &\geq \mathbb{E}_{\mathcal{S},\bm \sigma}\left[
    \frac{2}{n}
      \sum_{i=1}^{n}
      \sigma_i\sqrt{\frac{r_1}{r_2}}\ell_{h^{r_2}}(z_i)\right]
  \\
    &=\sqrt{\frac{r_1}{r_2}}\mathbb{E}_{\mathcal{S},\bm \sigma}
      \left[\sup_{\ell_h\in\mathcal{L}^{r_2}}\frac{2}{n}
      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)
          \right]
    =\sqrt{\frac{r_1}{r_2}}\psi(r_2),
  \end{align*}
  which allows proving the claim since
  $
    \frac{\psi(r_2)}{\sqrt{r_2}}
    \leq \frac{\psi(r_1)}{\sqrt{r_1}}.
  $
\end{proof}
\begin{proposition}
\label{the-fixedpoint}
Let us consider a sub-root function $\psi(r)$, with fixed point $r^\ast$,
and suppose that $\forall r>r^\ast$,
  \begin{align}
  \label{Randpsi}
    \mathcal{R}(\mathcal{L}^r)\leq \psi(r).
  \end{align}
  Then, $\forall h\in\mathcal{H}_{p,\kappa}$ and $\forall k>\max(1,\frac{\sqrt{2}}{2M})$, with probability
  $1-\delta$, we have,
  \begin{align*}
   L(\ell_h)&\leq \max\left\{
        \frac{k}{k-1}\hat{L}(\ell_h),
       \hat{L}(\ell_h)+c_Mr^\ast+\frac{c_{\delta}}{n}
     \right\},
\end{align*}
where $c_M=18Mk, c_{\delta}=\frac{(12k+14)\log(1/\delta)}{3}$
\end{proposition}
\begin{proof}
  From the Theorem 2.1 of \cite{Bartlett2005lrc},
  we have
  \begin{align*}
    \hat{U}(\bar{\mathcal{L}})&=\sup_{\bar{\ell}_h\in\bar{\mathcal{L}}}\left\{L(\bar{\ell}_h)-\hat{L}(\bar{\ell}_h)\right\}\\
    &\leq \inf_{\alpha>0}\Big(2(1+\alpha)\mathcal{R}(\bar{\mathcal{L}})+
    \sqrt{\frac{2r\log(1/\delta)}{n}}\\&~~~~~+M\left(\frac{1}{3}+\frac{1}{\alpha}\right)\frac{\log(1/\delta)}{n}\Big)\\
    &\overset{\text{Lemma }\ref{lem-lrs-subset-lr}}{\leq}
    \inf_{\alpha>0}\Big(2(1+\alpha)\mathcal{R}(\mathcal{L}^r)+
    \sqrt{\frac{2r\log(1/\delta)}{n}}\\&~~~~~+M\left(\frac{1}{3}+\frac{1}{\alpha}\right)\frac{\log(1/\delta)}{n}\Big)\\
    &\overset{\eqref{Randpsi}, \text{ set $\alpha=\frac{1}{2}$}}{\leq} 3\psi(r)+\sqrt{\frac{2r\log(1/\delta)}{n}}+\frac{7M\log(1/\delta)}{3n}\\
    &\overset{\text{sub-root}}{\leq} 3\sqrt{rr^\ast}+\sqrt{\frac{2r\log(1/\delta)}{n}}+\frac{7M\log(1/\delta)}{3n}.
  \end{align*}
  The next step of the proof consists in showing that $r$ can be chosen,
  such that $\hat{U}(\bar{\mathcal{L}})\leq \frac{r}{Mk}$ and $r\geq r^\ast$,
  so that we can exploit Lemma \ref{the-ori-bound}
  and conclude the proof.
  For this purpose,
we set
\begin{align*}
  A=3\sqrt{r^\ast}+\sqrt{\frac{2\log(1/\delta)}{n}},
  B= \frac{7M\log(1/\delta)}{3n}.
\end{align*}
Thus, we have to find the solution of
\begin{align*}
  A\sqrt{r}+B=\frac{r}{Mk},
\end{align*}
which is
\begin{align}
\label{eq-solution-of-r}
  r=\frac{\left[
  \left(
    \frac{2B}{kM}+A^2
    \right)+
    \sqrt{\left(
    \frac{2B}{kM}+A^2
    \right)^2-\frac{4B^2}{M^2k^2}}
  \right]}
  {\frac{2}{M^2k^2}}.
\end{align}
Since $k\geq \max(1,\frac{\sqrt{2}}{2M})$,
$k^2M^2\geq \frac{1}{2}$.
Therefore, from \eqref{eq-solution-of-r}, we have
\begin{align*}
  r\geq A^2M^2k^2\geq \frac{A^2}{2}= r^\ast,
  r\leq A^2M^2k^2+2BMk.
\end{align*}
Thus, we have
\begin{align*}
 \frac{r}{Mk}&\leq A^2Mk+2B \\
 &=\Big(3\sqrt{r^\ast}+\sqrt{{2\log(1/\delta)}/{n}}\Big)^2Mk+\frac{14M\log(1/\delta)}{3n}.
\end{align*}
Note that, $\forall a,b>0$, $(a+b)^2\leq 2a^2+2b^2$,
so we have that
\begin{align*}
  \frac{r}{Mk}&\leq 18Mkr^\ast+\frac{(12k+14)\log(1/\delta)}{3n}
\end{align*}
By substituting the above inequality
 into Lemma \ref{the-ori-bound},
the proof is over.
  %Note that $\sqrt{ab}\leq \frac{a}{2}+\frac{b}{2}$,
%  we get
%  \begin{align*}
%    \hat{U}(\bar{\mathcal{L}})\leq
%  \end{align*}
\end{proof}

\begin{proof}[Proof of Theorem \ref{theorem-finally}]
From Proposition \ref{the-fixedpoint},
with probability $1-\delta$,
we have
\begin{align}
\label{eq-middle-leel}
   L(\ell_h)&\leq \max\left\{
        \frac{k}{k-1}\hat{L}(\ell_h),
       \hat{L}(\ell_h)+c_dr^\ast+\frac{c_\delta}{n}
     \right\},
\end{align}
where $r^\ast$ is a fixed point of $\mathcal{R}(\mathcal{L}^r)$.
%that is $r^\ast=\mathcal{R}(\mathcal{L}^{r^\ast})$.
From Lemma \ref{lemma-sub},
we know that the $\mathcal{R}(\mathcal{L}^r)$ is a sub-root function,
so the fixed point $r^\ast$ of $\mathcal{R}(\mathcal{L}^r)$ is uniquely exists.

According to  Theorem \ref{rademacherlocal}, we know that
\begin{align*}
    \mathcal{R}(\mathcal{L}^r) \leq \frac{c_{d,\vartheta}\xi(K)\sqrt{\zeta r}\log^{\frac{3}{2}}(n)}{\sqrt{n}}+\frac{4\log(1/\delta)}{n}.
  \end{align*}
Thus, if we set $A=\frac{c_{d,\vartheta}\xi(K)\sqrt{\zeta}\log^{\frac{3}{2}}(n)}{\sqrt{n}}$, $B=\frac{4\log(1/\delta)}{n}$,
the fixed point $r^\ast$ is smaller than the solution of
  $
     A\sqrt{r}+B=r,
  $
   which is
   \begin{align*}
     r^s&=\frac{2B+A^2+\sqrt{(2B+A^2)^2-4B^2}}{2}\\
     &\leq 2B+A^2
     =\frac{c_{d,\vartheta}^2\xi^2(K)\zeta\log^3(n)}{n}+\frac{4\log(1/\delta)}{n}.
   \end{align*}
   Substituting the above inequality into \eqref{eq-middle-leel} finishes the proof.
\end{proof}

\section{Appendix C: Proof of Theorem \ref{theorem-fenchel-dual}}
\begin{theorem}
\label{theorem-fenchel-dual}
Let
$
    \bm \nu=\Big[\|\pmb{\theta}_1\|-\beta r_1,\ldots,
    \|\pmb{\theta}_M\|-\beta r_M\Big],
$
then the component $m$-th of $\;\nabla\Omega^\ast(\pmb{\theta})$ is
\begin{align*}
\frac{\mathrm{sgn}(\nu_m) \pmb{\theta}_m}{\alpha\|\pmb{\theta}_m\|}
       \frac{|\nu_m|^{q-1}}{\|\bm \nu\|_q^{q-2}},
\end{align*}
where $\mathrm{sgn}(x)$ is defined as
$\mathrm{sgn}(x)=1$, if $x>0$, $\mathrm{sgn}(x)=-1$, if $x<0$,
$\mathrm{sgn}(x)\in[-1,+1]$, if $x=0$.
\end{theorem}
\begin{lemma}
  \label{inverse-mapping}
  Let $p \in (1,2]$ and $q=p/(p-1)$, and then the norms $\|\pmb{c}\|_p$ and $\|\pmb{c}^\ast\|_q$ are dual to each other. Define the mapping $f:\mathcal{M} \to \mathcal{M}$ with
  \begin{align*}
    c_i^\ast=f_i(\pmb{c})=\nabla_i\Big(\frac{1}{2}\|\pmb{c}\|_p^2\Big)=\frac{sng(c_i)|c_i|^{p-1}}{\|\pmb{c}\|_p^{p-2}}, i=1,\ldots,n,
  \end{align*}
and the inverse mapping $f^{-1}$ with
  \begin{align*}
    c_i=f^{-1}_i(\pmb{c}^\ast)=\nabla_i\Big(\frac{1}{2}\|\pmb{c}^\ast\|_q^2\Big)=\frac{sng(c_i^\ast)|c_i^\ast|^{q-1}}{\|\pmb{c}^\ast\|_q^{q-2}}, i=1,\ldots,n,
  \end{align*}
\end{lemma}
These mapping are often called {\em link functions} in machine learning (e.g., \cite{Gentile03a}).
\begin{lemma}
\label{lemma-lemma7}
For $\ell_1$-regularization, there is a scalar minimization problem
  \begin{align*}
    \min\limits_{\mathbf w \in \mathbf{R}} \eta_t\mathbf w+\lambda_t|\mathbf w|+\frac{\gamma_t}{2}\mathbf w^2,
  \end{align*}
And an optimal solution $\mathbf w^\ast$ can be summarized as
  \begin{align*}
    \mathbf w^\ast=\left\{
        \begin{aligned}
        &\mathbf{0}  &if |\eta_t| \leq \lambda_t,\\
        &-\frac{1}{\gamma_t}(\eta_t-\lambda_t sgn(\eta_t)) &\textrm{otherwise}.
        \end{aligned}
    \right.
  \end{align*}
\end{lemma}
\begin{proof}
The minimization problem is an unconstrained nonsmooth optimization problem. Its optimality condition \cite{rockafellar2015convex} states that $\mathbf{w}^\ast$ is an optimal solution if and only if there exists $\xi \in \partial|\mathbf{w}^\ast|$ such that
\begin{align*}
    \mathbf{\eta}_t+\mathbf{\lambda}_t\xi+\mathbf{\gamma}_t\mathbf{w}^\ast=0.
\end{align*}
There are more discussions in Appendix A of \cite{Xiao10}. Finally, we can get the closed-form for $\ell_1$-regularization.
\end{proof}


\begin{proof}[Proof of Theorem \ref{theorem-fenchel-dual}]
According to standard Legendre-Fenchel duality, we can get
\begin{align*}
\nabla\Omega^*(\pmb{\theta})&=\argmax\limits_{\mathbf w} \mathbf w \cdot \pmb{\theta} - \Omega(\mathbf w)\\
&=\argmax\limits_{\mathbf w} \mathbf w \cdot \pmb{\theta}
-\frac{\alpha}{2}\|\mathbf w\|_{2,p}^2
-\beta\pmb{\mu}\cdot\pmb{r}.
\end{align*}

To reach the above argmax, the derivative of argmax should be zero,
so $\mathbf w$ must be proportional to $\pmb{\theta}$.
As in UFO-MKL \cite{OrabonaL11}, we explicitly give a tricky link function
$\mathbf w_m=\mu_m\pmb{\theta}_m$ for different kernels.
By this explicit link function, the algorithm can update both $\mathbf w$ and $\pmb \mu$ by $\nabla\Omega^\ast(\pmb{\theta})$.
Then, for the convenience of computation, we focus on $c_m=\mu_m\|\pmb{\theta}_m\|$, rewriting the argmax:
\begin{align}
    \label{lp-regularization}
    \argmin \limits_{\pmb{c}} (\beta\mathbf{r} - \mathbf{a})\cdot\pmb{c}+\frac{\alpha}{2}\|\pmb{c}\|_p^2
\end{align}
where $\mathbf{a}=\Big[\|\pmb{\theta}_1\|,\ldots, \|\pmb{\theta}_M\|\Big]$.

The optimality condition of the above minimization problem \cite{rockafellar2015convex} states that $\pmb{c}^\ast$ is an optimal solution of \eqref{lp-regularization}. And we set the derivative of above argmin being zero
\begin{align}
    \label{l2-regularization-subgradient}
        \beta\mathbf{r}-\mathbf{a}+\alpha\pmb{c}^\ast=0
\end{align}
And then we can get $\pmb{c}^\ast=\frac{1}{\alpha}\Big(\mathbf{a}-\beta\mathbf{r}\Big)$.
Following similar arguments of Lemma \ref{inverse-mapping} and Lemma \ref{lemma-lemma7}, we find that it has a closed-form solution
\begin{align*}
    c_m=&f ^{-1}(c_m^\ast)=\nabla_m\Big(\frac{1}{2}\|{c}_m^\ast\|_q^2\Big)
    =\frac{\mathrm{sgn}({c}_m^\ast)|{c}_m^\ast|^{q-1}}{\alpha\|\pmb{c}^\ast\|_q^{q-2}}.
\end{align*}
Let
$
    \bm \nu=\Big[\|\pmb{\theta}_1\|-\beta\mathbf{r}_1,\ldots,
    \|\pmb{\theta}_M\|-\beta\mathbf{r}_M\Big],
$
and use $\mu_m=c_m/\|\pmb{\theta}_m\|$ and $\mathbf w_m=\mu_m\pmb{\theta}_m$, We can get
\begin{align*}
    &{\mu}_m=\frac{\mathrm{sgn}(\nu_m)|\nu_m|^{q-1}}{\alpha\|\pmb{\theta}_m\|\|\bm \nu\|_q^{q-2}},
    &\mathbf w_m=\frac{\mathrm{sgn}(\nu_m)\pmb{\theta}_m|\nu_m|^{q-1}}{\alpha\|\pmb{\theta}_m\|\|\bm \nu\|_q^{q-2}}.\\
\end{align*}
Similar argmax has been analysis in Section 7.2 of \cite{Xiao10}.
\end{proof}
\section{Appendix D: Convergence Analysis}
\subsection{Conv-MKL}
Convergence rate of the proposed \texttt{Conv-MKL} is decided by which $\ell_p$ MC-MKL algorithm it uses.
In experiments, we following implement \texttt{Conv-MKL} based on UFO-MKL \cite{OrabonaL11}.
Thus, convergence rate of \texttt{Conv-MKL} is same with UFO-MKL in Section 4.1 of \cite{OrabonaL11}.
\subsection{SMSD-MKL}
%In this subsection, we analysis convergence rate guarantee for SMSD-MKL.
%In multi-class classification, there is one hyperplane for each class
%and $\phi_m(\cdot, \cdot)$ induces the transformation in $m$-th class.
Denote by $z^t=\partial\ell(\mathbf w,\phi_{\bm \mu}(\mathbf{x}^t), y^t)$,
we now state the convergence therem for any loss function that satisfies the following hypothesis
\begin{align}
\label{loss_function_hypothesis}
    \|z_m\| \leq L\|\phi_m(\cdot)\|_2, \forall t =1,\ldots, M.
\end{align}
%The hinge loss for multi-class $\ell(\mathbf w,\phi_{\bm \mu}(\mathbf{x}_i), y_i)=
%\max\limits_{i \not= j}\left|1-\left(\langle\mathbf w_i, \phi_{\bm \mu}(\mathbf{x}_i)\rangle -\langle\mathbf w_j, \phi_{\bm \mu}(\mathbf{x}_j)\rangle\right)\right|_+$ satisfies the hypothesis with $L=\sqrt{2}$.

\begin{theorem}
Denote by $$f(\mathbf w)=\Omega(\mathbf w)+\frac{1}{n}\sum_{i=1}^n\ell(\mathbf w,\phi_{\bm \mu}(\mathbf{x}_i),y_i)$$
and by $\mathbf w^\ast$ the solution.
Suppose that $\|\phi_m(\cdot)\|_2 \leq 1$, and the loss function $\ell$ satisfies $\eqref{loss_function_hypothesis}$.
Let $\delta \in (0,1)$, then with probability at least $1 - \delta$ over the choices of the random samples
we have that after $T$ iterations of the SMSD-MKL algorithm
\begin{align*}
    f(\mathbf w^{T+1})-f(\mathbf w^\ast) \leq \frac{eL^2(1+\log T)\log M}{\alpha\delta T},
\end{align*}
where $e$ is the Euler's number.
\end{theorem}
\begin{proof}
Using \eqref{loss_function_hypothesis} and $\|\phi_m(\cdot)\|_2 \leq 1$, we have
\begin{align*}
    &\|\partial(\mathbf w^t,\phi_{\bm \mu}(\mathbf x^t), y^t)\|_{2,q} \\
    &\leq LM^{1/q} \max_{j=1, \ldots, M} \|\phi_m(\mathbf x^t)\|_2 \leq LF^{1/q}
\end{align*}
The function $\Omega(\mathbf w)=\frac{\alpha}{2}\|\mathbf w\|_{2,p}^2 + \beta\mathbf{\mu} \cdot \mathbf{r}$ is
 $\alpha$-strongly convex w.r.t. the norm $\|\cdot\|_{2,q}$. Hence, using according to Theorem 1 in \cite{Shalev-ShwartzSS07},
 using $\eta=1$ and $g=\Omega$, and using Markov inequality as in \cite{Shalev-ShwartzSS07} we prove the stated result.
\end{proof}



%\section{Appendix D}





%\subsection*{Proof of the Lemma 5}
%\begin{proof}
%  In order to prove the lemma, the following properties mush apply:
%  \begin{itemize}
%    \item[1)] $\psi_n(r)$ is positive
%    \item[2)] $\psi_n(r)$ is non-decrasing
%    \item[3)] $\psi_n(r)/\sqrt{r}$ is non-increasing
%  \end{itemize}
%  By the definition of $R_n(\mathcal{L}^r)$,
%  it is easy to verity that $R_n(\mathcal{L}^r)$ is positive.
%
%  Concerning the second property, we have that, for $0\leq r_1\leq r_2$:
%  $
%    \mathcal{L}^{r_1}\subseteq \mathcal{L}^{r_2},
%  $
%  therefore
%  \begin{align*}
%    \psi_n(r_1)&=\mathbb{E}_{\mathcal{S},\bm \sigma}
%      \left[\sup_{\ell_h\in\mathcal{L}^{r_1}}\left|\frac{2}{n}
%      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)\right|
%          \right]\\
%    &\leq \mathbb{E}_{\mathcal{S},\bm \sigma}
%      \left[\sup_{\ell_h\in\mathcal{L}^{r_2}}\left|\frac{2}{n}
%      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)\right|
%          \right]
%    \\&=\psi_n(r_2).
%  \end{align*}
%  Finally, concerning the third property,
%  for $0\leq r_1\leq r_2$,
%  let
%  \begin{align*}
%    \ell_{h^{r_2}}
%    =\argsup_{\ell_h\in\mathcal{L}^{r_2}}
%    \mathbb{E}_{S,\bm \sigma}
%      \left[\sup_{\ell_h\in\mathcal{L}^{r_2}}\left|\frac{2}{n}
%      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)\right|
%          \right].
%  \end{align*}
%  Note that, since $\frac{r_1}{r_2}\leq 1$,
%  we have that $\sqrt{\frac{r_1}{r_2}}\ell_{h^{r_2}}\in \mathcal{L}^{r_2}$.
%  Consequently:
%  \begin{align*}
%    L\left[
%    \left(\sqrt{\frac{r_1}{r_2}}\ell_{h^{r_2}}\right)\right]^2
%    =\frac{r_1}{r_2}L\left[(\ell_{h^{r_2}})^2\right]\leq r_1.
%  \end{align*}
%  Thus, we have that:
%  \begin{align*}
%    \psi_n(r_1)&=\mathbb{E}_{S,\bm \sigma}
%      \left[\sup_{\ell_h\in\mathcal{L}^{r_1}}\left|\frac{2}{n}
%      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)\right|
%          \right]\\
%    &\geq \mathbb{E}_{S,\bm \sigma}\left[\left|
%    \frac{2}{n}
%      \sum_{i=1}^{n}
%      \sigma_i\sqrt{\frac{r_1}{r_2}}\ell(h^{r_2},z_i,z_{n+i})\right|\right]
%  \\
%    &=\sqrt{\frac{r_1}{r_2}}\mathbb{E}_{S,\bm \sigma}
%      \left[\sup_{\ell_h\in\mathcal{L}^{r_2}}\left|\frac{2}{n}
%      \sum_{i=1}^{n}\sigma_i\ell_h(z_i)\right|
%          \right]\\
%    &=\sqrt{\frac{r_1}{r_2}}\psi_n(r_2),
%  \end{align*}
%  which allows proving the claim since
%  \begin{align*}
%    \frac{\psi_n(r_2)}{\sqrt{r_2}}
%    \leq \frac{\psi_n(r_1)}{\sqrt{r_1}}.
%  \end{align*}
%\end{proof}
%
%\section*{Appendix B}
%
%
%
%\begin{lemma}
%\label{RLrbound}
%  For a non-negative $L$-smooth loss $\ell$,
%  with probability $1-\delta$,
%  we have
%  \begin{align*}
%    \mathcal{R}(\mathcal{L}^r)\leq 42\sqrt{6Lr}\log^{\frac{3}{2}}(64n)\hat{\mathcal{R}}(\mathcal{L})+\frac{2\log(1/\delta)}{n}.
%  \end{align*}
%\end{lemma}
%\begin{proof}
%  According to the Lemma 3.6 of \cite{oneto2013improved},
%  with $1-\delta$, we get
%  \begin{align*}
%    \mathcal{R}(\mathcal{L}^r)\leq \hat{\mathcal{R}}(\mathcal{L}^r)+\sqrt{\frac{2\log(1/\delta)\mathcal{R}(\mathcal{L}^r)}{n}}.
%  \end{align*}
%  Note that $\sqrt{ab}\leq \frac{a}{2}+\frac{b}{2}$.
%  Thus, we can obtain that
%  \begin{align*}
%    \mathcal{R}(\mathcal{L}^r)\leq \hat{\mathcal{R}}(\mathcal{L}^r)+\frac{\mathcal{R}(\mathcal{L}^r)}{2}+\frac{\log(1/\delta)}{n}.
%  \end{align*}
%  So, we get
%  \begin{align}
%  \label{eqRhatR}
%    \mathcal{R}(\mathcal{L}^r)\leq 2\hat{\mathcal{R}}(\mathcal{L}^r)+\frac{2\log(1/\delta)}{n}
%  \end{align}
%  From the Lemma 2.2 of \cite{Srebro2010lrc},
%  if $\ell$ is a $H$-smooth loss function,
%  then we have
%  \begin{align}
%   \label{RlocalR}
%    \hat{\mathcal{R}}(\mathcal{L}^r)\leq 21\sqrt{6Hr}\log^{\frac{3}{2}}(64n)R(\mathcal{H}).
%  \end{align}
%  Substitute  the equation \eqref{RlocalR} into \eqref{eqRhatR},
%  the proof is over.
%\end{proof}
%
%
%
%
%
%\begin{lemma}
%  The fixed point $r^\ast$ of $\mathcal{R}(\mathcal{L}^r)$ is bound by
%  \begin{align*}
%    r^\ast\leq c\log^3(64n)\hat{\mathcal{R}}(\mathcal{L})^2+\frac{4\log(1/\delta)}{n},
%  \end{align*}
%  where $c=(42\sqrt{6L})^2$.
%\end{lemma}
%\begin{proof}
%  According to Lemma \ref{RLrbound},
%  we get
%  \begin{align*}
%     \mathcal{R}(\mathcal{L}^r)\leq 42\sqrt{6Lr}\log^{\frac{3}{2}}(64n)\hat{\mathcal{R}}(\mathcal{L})+\frac{2\log(1/\delta)}{n}.
%  \end{align*}
%   Let $c_1=42\sqrt{6L}\log^{\frac{3}{2}}(64n)$, $c_2=\frac{2\log(1/\delta)}{n}$.
%   Thus, the fixed point $r^\ast$ is the solution of
%   \begin{align*}
%     c_1\hat{\mathcal{R}}(\mathcal{L})\sqrt{r}+c_2=r,
%   \end{align*}
%   which is
%   \begin{align*}
%     r^\ast&=\frac{2c_2+c_1^2\hat{\mathcal{R}}(\mathcal{L})^2+\sqrt{(2c_2+c_1^2\hat{\mathcal{R}}(\mathcal{L})^2)^2-4c_2^2}}{2}\\
%     &\leq 2c_2+c_1^2\hat{\mathcal{R}}(\mathcal{L})^2=c_1^2\hat{\mathcal{R}}(\mathcal{L})^2+\frac{4\log(1/\delta)}{n}.
%   \end{align*}
%  %So the fixed point $r^\ast$ of $\mathcal{R}(\mathcal{L}^r)$ is smaller than the max root of
%%  \begin{align*}
%%    42\sqrt{6Lr}\log^{\frac{3}{2}}(64n)\hat{\mathcal{R}}(\mathcal{L})+\frac{2\log(1/\delta)}{n}=r.
%%  \end{align*}
%\end{proof}
%
%
%
\bibliographystyle{abbrv}
\bibliography{MC_MKL_LR}
\end{document}
